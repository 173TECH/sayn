{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00b6 SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow. Use Cases \u00b6 SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models. Key Features \u00b6 SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation . Design Principles \u00b6 SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start \u00b6 SAYN supports Python 3.6 to 3.10. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power! Release Updates \u00b6 If you want to receive update emails about SAYN releases, you can sign up here . Support \u00b6 If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License \u00b6 SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#_1","text":"SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow.","title":""},{"location":"#use_cases","text":"SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key_features","text":"SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design_principles","text":"SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick_start","text":"SAYN supports Python 3.6 to 3.10. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#release_updates","text":"If you want to receive update emails about SAYN releases, you can sign up here .","title":"Release Updates"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"cli/","text":"SAYN CLI \u00b6 About \u00b6 SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options. Available Commands \u00b6 sayn init \u00b6 Initialises a SAYN project in the current working directory with the SAYN tutorial . sayn run \u00b6 Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks. Filtering Tasks \u00b6 Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Multiple tasks can be included after the argument, accumulating their values. Note that both -t and -x can be specified multiple times, resulting in the same outcome. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 task2 : runs task1 and task2 only. sayn run -t task1 -t task2 : runs task1 and task2 only. (equivalent to the one above.) sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors. Incremental Tasks Options \u00b6 SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console. sayn compile \u00b6 Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply. sayn dag-image \u00b6 Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"CLI"},{"location":"cli/#sayn_cli","text":"","title":"SAYN CLI"},{"location":"cli/#about","text":"SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options.","title":"About"},{"location":"cli/#available_commands","text":"","title":"Available Commands"},{"location":"cli/#sayn_init","text":"Initialises a SAYN project in the current working directory with the SAYN tutorial .","title":"sayn init"},{"location":"cli/#sayn_run","text":"Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks.","title":"sayn run"},{"location":"cli/#filtering_tasks","text":"Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Multiple tasks can be included after the argument, accumulating their values. Note that both -t and -x can be specified multiple times, resulting in the same outcome. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 task2 : runs task1 and task2 only. sayn run -t task1 -t task2 : runs task1 and task2 only. (equivalent to the one above.) sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors.","title":"Filtering Tasks"},{"location":"cli/#incremental_tasks_options","text":"SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console.","title":"Incremental Tasks Options"},{"location":"cli/#sayn_compile","text":"Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply.","title":"sayn compile"},{"location":"cli/#sayn_dag-image","text":"Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"sayn dag-image"},{"location":"installation/","text":"Installation \u00b6 SAYN is available for installation using pip and is regularly tested in Python 3.6+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"installation/#installation","text":"SAYN is available for installation using pip and is regularly tested in Python 3.6+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"parameters/","text":"Parameters \u00b6 parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks. Project Parameters \u00b6 Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile. Task Parameters \u00b6 Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL. Using Parameters \u00b6 Using Parameters In tasks \u00b6 Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }} In SQL Queries \u00b6 For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt In python Tasks \u00b6 Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"Parameters"},{"location":"parameters/#parameters","text":"parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks.","title":"Parameters"},{"location":"parameters/#project_parameters","text":"Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile.","title":"Project Parameters"},{"location":"parameters/#task_parameters","text":"Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL.","title":"Task Parameters"},{"location":"parameters/#using_parameters","text":"","title":"Using Parameters"},{"location":"parameters/#using_parameters_in_tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }}","title":"Using Parameters In tasks"},{"location":"parameters/#in_sql_queries","text":"For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt","title":"In SQL Queries"},{"location":"parameters/#in_python_tasks","text":"Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"In python Tasks"},{"location":"presets/","text":"Presets \u00b6 Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition. Defining The preset \u00b6 Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder). Preset Inheritance \u00b6 Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Presets"},{"location":"presets/#presets","text":"Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition.","title":"Presets"},{"location":"presets/#defining_the_preset","text":"Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder).","title":"Defining The preset"},{"location":"presets/#preset_inheritance","text":"Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Preset Inheritance"},{"location":"project_structure/","text":"SAYN Project Structure \u00b6 SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Structure"},{"location":"project_structure/#sayn_project_structure","text":"SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"SAYN Project Structure"},{"location":"api/database/","text":"\u00b6 Database \u00b6 Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine. execute ( self , script ) \u00b6 Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required load_data ( self , table , data , schema = None , batch_size = None , replace = False , ** ddl ) \u00b6 Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks {} Returns: Type Description int Number of records loaded read_data ( self , query , ** params ) \u00b6 Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"Database"},{"location":"api/database/#sayn.database","text":"","title":"sayn.database"},{"location":"api/database/#sayn.database.Database","text":"Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine.","title":"Database"},{"location":"api/database/#sayn.database.Database.execute","text":"Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required","title":"execute()"},{"location":"api/database/#sayn.database.Database.load_data","text":"Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks {} Returns: Type Description int Number of records loaded","title":"load_data()"},{"location":"api/database/#sayn.database.Database.read_data","text":"Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"read_data()"},{"location":"api/python_task/","text":"\u00b6 Task \u00b6 Base class for tasks in SAYN. Attributes: Name Type Description name str Name of the task as defined in the task group. group str Name of the task group where the task was defined. run_arguments dict Dictionary containing the values for the arguments specified in the cli. task_parameters dict Provides access to the parameters specified in the task. project_parameters dict Provides access to the global parameters of the project. parameters dict Convinience property joining project and task parameters. connections dict Dictionary of connections specified for the project. tracker sayn.logging.TaskEventTracker Message tracker for the current task. jinja_env jinja2.Environment Jinja environment for this task. The environment comes pre-populated with the parameter values relevant to the task. add_run_steps ( self , steps ) \u00b6 Adds new steps to the list of run steps for the task, allowing the CLI to indicate task execution progress. compile_obj ( self , obj , ** params ) \u00b6 Compiles the object into a string using the task jinja environment. Parameters: Name Type Description Default obj str/Path/Template The object to compile. If the object is not a Jinja template object, self.get_template will be called first. required params dict An optional dictionary of additional values to use for compilation. Note: Project and task parameters values are already set in the environment, so there's no need to pass them on {} debug ( self , message , details = None ) \u00b6 Print a debug message when executing sayn in debug mode ( sayn run -d ) error ( self , message , details = None ) \u00b6 Prints an error message which will be persisted on the screen after the task concludes execution. Executing this method doesn't abort the task or changes the task status. Use return self.fail for that instead. Parameters: Name Type Description Default message str An optinal error message to print to the screen. required fail ( self , msg = None ) \u00b6 Returned on failure in any stage. finish_current_step ( self , result = Result . Ok : None ) \u00b6 Specifies the end of the current step get_template ( self , obj ) \u00b6 Returns a Jinja template object. Parameters: Name Type Description Default obj str/Path The object to transform into a template. If a pathlib.Path is specified, the template will be read from disk. required info ( self , message , details = None ) \u00b6 Prints an info message. ready ( self ) \u00b6 (Deprecated: use success instead) Returned on successful execution. set_run_steps ( self , steps ) \u00b6 Sets the run steps for the task, allowing the CLI to indicate task execution progress. start_step ( self , step ) \u00b6 Specifies the start of a task step step ( self , step ) \u00b6 Step context Usage: with self . step ( 'Generate Data' ): data = generate_data () Parameters: Name Type Description Default step str name of the step being executed. required success ( self ) \u00b6 Returned on successful execution. warning ( self , message , details = None ) \u00b6 Prints a warning message which will be persisted on the screen after the task concludes execution.","title":"PythonTask"},{"location":"api/python_task/#sayn.tasks","text":"","title":"sayn.tasks"},{"location":"api/python_task/#sayn.tasks.Task","text":"Base class for tasks in SAYN. Attributes: Name Type Description name str Name of the task as defined in the task group. group str Name of the task group where the task was defined. run_arguments dict Dictionary containing the values for the arguments specified in the cli. task_parameters dict Provides access to the parameters specified in the task. project_parameters dict Provides access to the global parameters of the project. parameters dict Convinience property joining project and task parameters. connections dict Dictionary of connections specified for the project. tracker sayn.logging.TaskEventTracker Message tracker for the current task. jinja_env jinja2.Environment Jinja environment for this task. The environment comes pre-populated with the parameter values relevant to the task.","title":"Task"},{"location":"api/python_task/#sayn.tasks.Task.add_run_steps","text":"Adds new steps to the list of run steps for the task, allowing the CLI to indicate task execution progress.","title":"add_run_steps()"},{"location":"api/python_task/#sayn.tasks.Task.compile_obj","text":"Compiles the object into a string using the task jinja environment. Parameters: Name Type Description Default obj str/Path/Template The object to compile. If the object is not a Jinja template object, self.get_template will be called first. required params dict An optional dictionary of additional values to use for compilation. Note: Project and task parameters values are already set in the environment, so there's no need to pass them on {}","title":"compile_obj()"},{"location":"api/python_task/#sayn.tasks.Task.debug","text":"Print a debug message when executing sayn in debug mode ( sayn run -d )","title":"debug()"},{"location":"api/python_task/#sayn.tasks.Task.error","text":"Prints an error message which will be persisted on the screen after the task concludes execution. Executing this method doesn't abort the task or changes the task status. Use return self.fail for that instead. Parameters: Name Type Description Default message str An optinal error message to print to the screen. required","title":"error()"},{"location":"api/python_task/#sayn.tasks.Task.fail","text":"Returned on failure in any stage.","title":"fail()"},{"location":"api/python_task/#sayn.tasks.Task.finish_current_step","text":"Specifies the end of the current step","title":"finish_current_step()"},{"location":"api/python_task/#sayn.tasks.Task.get_template","text":"Returns a Jinja template object. Parameters: Name Type Description Default obj str/Path The object to transform into a template. If a pathlib.Path is specified, the template will be read from disk. required","title":"get_template()"},{"location":"api/python_task/#sayn.tasks.Task.info","text":"Prints an info message.","title":"info()"},{"location":"api/python_task/#sayn.tasks.Task.ready","text":"(Deprecated: use success instead) Returned on successful execution.","title":"ready()"},{"location":"api/python_task/#sayn.tasks.Task.set_run_steps","text":"Sets the run steps for the task, allowing the CLI to indicate task execution progress.","title":"set_run_steps()"},{"location":"api/python_task/#sayn.tasks.Task.start_step","text":"Specifies the start of a task step","title":"start_step()"},{"location":"api/python_task/#sayn.tasks.Task.step","text":"Step context Usage: with self . step ( 'Generate Data' ): data = generate_data () Parameters: Name Type Description Default step str name of the step being executed. required","title":"step()"},{"location":"api/python_task/#sayn.tasks.Task.success","text":"Returned on successful execution.","title":"success()"},{"location":"api/python_task/#sayn.tasks.Task.warning","text":"Prints a warning message which will be persisted on the screen after the task concludes execution.","title":"warning()"},{"location":"databases/bigquery/","text":"BigQuery \u00b6 The BigQuery driver depends on pybigquery and can be installed with: pip install \"sayn[bigquery]\" Warning Currently the latest version of pybigquery will not install on Python 3.10. pip install pybigquery can be used to install an earlier version although some problems might arise, so we recommend using Python 3.9 instead. The Bigquery connector looks for the following parameters: Parameter Description Default project GCP project where the cluster is Required credentials_path Path relative to the project to the json for the service account to use Required location Default location for tables created Dataset default dataset Dataset to use when running queries. Can be specified in sql For advanced configurations, SAYN will pass other parameters to create_engine , so check the pybigquery dialect for extra parameters. Bigquery Specific DDL \u00b6 Partitioning \u00b6 SAYN supports specifying the partitioning model for tables created with autosql and copy tasks. To do this we specify partition in the ddl field. The value is a string matching a BigQuery partition expression . tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : partition : DATE(_PARTITIONTIME) Clustering \u00b6 We can also specify the clustering for the table with the cluster property in autosql and copy tasks. The value in this case is a list of columns. If the ddl for the task includes the list of columns, the columns specified in the cluster should be present in the column list. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : cluster : - arena_name","title":"BigQuery"},{"location":"databases/bigquery/#bigquery","text":"The BigQuery driver depends on pybigquery and can be installed with: pip install \"sayn[bigquery]\" Warning Currently the latest version of pybigquery will not install on Python 3.10. pip install pybigquery can be used to install an earlier version although some problems might arise, so we recommend using Python 3.9 instead. The Bigquery connector looks for the following parameters: Parameter Description Default project GCP project where the cluster is Required credentials_path Path relative to the project to the json for the service account to use Required location Default location for tables created Dataset default dataset Dataset to use when running queries. Can be specified in sql For advanced configurations, SAYN will pass other parameters to create_engine , so check the pybigquery dialect for extra parameters.","title":"BigQuery"},{"location":"databases/bigquery/#bigquery_specific_ddl","text":"","title":"Bigquery Specific DDL"},{"location":"databases/bigquery/#partitioning","text":"SAYN supports specifying the partitioning model for tables created with autosql and copy tasks. To do this we specify partition in the ddl field. The value is a string matching a BigQuery partition expression . tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : partition : DATE(_PARTITIONTIME)","title":"Partitioning"},{"location":"databases/bigquery/#clustering","text":"We can also specify the clustering for the table with the cluster property in autosql and copy tasks. The value in this case is a list of columns. If the ddl for the task includes the list of columns, the columns specified in the cluster should be present in the column list. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : cluster : - arena_name","title":"Clustering"},{"location":"databases/mysql/","text":"MySQL \u00b6 The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/mysql/#mysql","text":"The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/overview/","text":"Databases \u00b6 About \u00b6 SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: BigQuery MySQL PostgreSQL Redshift Snowflake SQLite Usage \u00b6 Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200 Using Databases In python Tasks \u00b6 Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Overview"},{"location":"databases/overview/#databases","text":"","title":"Databases"},{"location":"databases/overview/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: BigQuery MySQL PostgreSQL Redshift Snowflake SQLite","title":"About"},{"location":"databases/overview/#usage","text":"Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200","title":"Usage"},{"location":"databases/overview/#using_databases_in_python_tasks","text":"Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Using Databases In python Tasks"},{"location":"databases/postgresql/","text":"PostgreSQL \u00b6 The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/postgresql/#postgresql","text":"The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/redshift/","text":"Redshift \u00b6 The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters. Connection Types \u00b6 SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based. Standard User/Password Connection \u00b6 If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Connecting With IAM \u00b6 With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters . Redshift Specific DDL \u00b6 Indexes \u00b6 Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition. Sorting \u00b6 Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY . Distribution \u00b6 We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Redshift"},{"location":"databases/redshift/#redshift","text":"The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters.","title":"Redshift"},{"location":"databases/redshift/#connection_types","text":"SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based.","title":"Connection Types"},{"location":"databases/redshift/#standard_userpassword_connection","text":"If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models","title":"Standard User/Password Connection"},{"location":"databases/redshift/#connecting_with_iam","text":"With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters .","title":"Connecting With IAM"},{"location":"databases/redshift/#redshift_specific_ddl","text":"","title":"Redshift Specific DDL"},{"location":"databases/redshift/#indexes","text":"Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition.","title":"Indexes"},{"location":"databases/redshift/#sorting","text":"Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY .","title":"Sorting"},{"location":"databases/redshift/#distribution","text":"We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Distribution"},{"location":"databases/snowflake/","text":"Snowflake \u00b6 The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/snowflake/#snowflake","text":"The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/sqlite/","text":"SQLite \u00b6 SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"databases/sqlite/#sqlite","text":"SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"project_examples/bbc_news_nlp/","text":"SAYN Project Example: BBC News NLP \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from BBC RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Implementation Details \u00b6 Step 1: Extract Task Group \u00b6 Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data Task Details ( load_data ) \u00b6 First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_bbc_feeds links : - http://feeds.bbci.co.uk/news/england/rss.xml - http://feeds.bbci.co.uk/news/wales/rss.xml - http://feeds.bbci.co.uk/news/scotland/rss.xml - http://feeds.bbci.co.uk/news/northern_ireland/rss.xml - http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml - http://feeds.bbci.co.uk/news/world/middle_east/rss.xml - http://feeds.bbci.co.uk/news/world/latin_america/rss.xml - http://feeds.bbci.co.uk/news/world/europe/rss.xml - http://feeds.bbci.co.uk/news/world/asia/rss.xml - http://feeds.bbci.co.uk/news/world/africa/rss.xml Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending BBC data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method LoadData Class \u00b6 Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_bbc_data : fetches data from the BBC RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_bbc_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. Utility Method ( fetch_bbc_data ) \u00b6 The fetch_bbc_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. Lastly, the function assigns a unique_id to each article which is based on its article id and the source it originates from. This is because the same article may be published in multiple sources with the same id, which means our original ids are not unique and could be misleading. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_bbc_data ( self , link ): \"\"\"Parse and label RSS BBC News data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # remove incompatible columns data . drop ( [ \"title_detail\" , \"summary_detail\" , \"links\" , \"published_parsed\" ], axis = 1 , inplace = True , ) # get the source (this only works for BBC RSS feeds) data [ \"source\" ] = link [ 29 : - 8 ] . replace ( \"/\" , \"_\" ) # generating ids to be unique, since same story ids can be published in different sources data [ \"unique_id\" ] = data [ \"id\" ] + data [ \"source\" ] return data def setup ( self ): self . set_run_steps ([ \"Appending BBC data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending BBC data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_bbc_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml Step 2: Modelling Group \u00b6 Quick Summary: Create the SQL query dim_bbc_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml Task Details ( dim_bbc_feeds ) \u00b6 Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_bbc_feeds.sql sql/dim_bbc_feeds.sql SELECT DISTINCT unique_id , id , title , summary , link , guidislink , published , source FROM {{ user_prefix }} logs_bbc_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_bbc_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data Step 3: Data Science Group \u00b6 Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_bbc_feeds_nlp_stats to calculate aggregate statistics grouped by source Group Overview \u00b6 Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_bbc_feeds table, therefore we will need to set their their table parameters to dim_bbc_feeds . Since both of these tasks are children of the dim_bbc_feeds task, we will also need to set their parents attributes to dim_bbc_feeds . The nlp task has a text parameter, this parameter specifies which columns have text for processing. The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords (e.g. \"say\" and its variations seem to be very common in summaries, however they are not very informative). tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds text : - title - summary wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds stopwords : - say - said - says - will - country - US - England - Scotland - Wales - NI - Ireland - Europe - BBC - yn Task Details ( wordcloud ) \u00b6 The wordcloud task will have the following steps: Grouping texts : aggregates article summaries and groups them by source (summaries are used instead of titles since they tend to be longer) Generating clouds : generates a wordcloud for each source, as well as the full dataset RenderCloud Class \u00b6 Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"firebrick\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) except : mask = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . summary ) sources = df . groupby ( \"source\" ) grouped_texts = sources . summary . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating bbc_wordcloud.png\" ) self . word_cloud ( \"bbc\" , full_text , stopwords , b_colour = \"white\" , c_colour = \"black\" ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords ) return self . success () Task Details ( nlp ) \u00b6 The nlp task will have the following steps: Processing texts : loops through text_fields, generates text statistics on each entry Updating database : similar to LoadData step, has additional debugging information LanguageProcessing Class \u00b6 Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] text_fields = self . parameters [ \"text\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) for t in text_fields : self . info ( f \"Processing texts for { t } field\" ) self . desc_text ( df , t , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success () Task Details ( dim_bbc_feeds_nlp_stats ) \u00b6 Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_bbc_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_bbc_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_tl , AVG ( title_words ) AS average_tw , AVG ( title_sentences ) AS average_ts , AVG ( summary_letters ) AS average_sl , AVG ( summary_words ) AS average_sw , AVG ( summary_sentences ) AS average_ss FROM {{ user_prefix }} dim_bbc_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_bbc_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data dim_bbc_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp Step 4: Run the project \u00b6 All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"SAYN Project Example: BBC News NLP"},{"location":"project_examples/bbc_news_nlp/#sayn_project_example_bbc_news_nlp","text":"","title":"SAYN Project Example: BBC News NLP"},{"location":"project_examples/bbc_news_nlp/#project_description","text":"","title":"Project Description"},{"location":"project_examples/bbc_news_nlp/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from BBC RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data","title":"Overview"},{"location":"project_examples/bbc_news_nlp/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow","title":"Features Used"},{"location":"project_examples/bbc_news_nlp/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/bbc_news_nlp/#implementation_details","text":"","title":"Implementation Details"},{"location":"project_examples/bbc_news_nlp/#step_1_extract_task_group","text":"Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data","title":"Step 1: Extract Task Group"},{"location":"project_examples/bbc_news_nlp/#task_details_load_data","text":"First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_bbc_feeds links : - http://feeds.bbci.co.uk/news/england/rss.xml - http://feeds.bbci.co.uk/news/wales/rss.xml - http://feeds.bbci.co.uk/news/scotland/rss.xml - http://feeds.bbci.co.uk/news/northern_ireland/rss.xml - http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml - http://feeds.bbci.co.uk/news/world/middle_east/rss.xml - http://feeds.bbci.co.uk/news/world/latin_america/rss.xml - http://feeds.bbci.co.uk/news/world/europe/rss.xml - http://feeds.bbci.co.uk/news/world/asia/rss.xml - http://feeds.bbci.co.uk/news/world/africa/rss.xml Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending BBC data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method","title":"Task Details (load_data)"},{"location":"project_examples/bbc_news_nlp/#loaddata_class","text":"Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_bbc_data : fetches data from the BBC RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_bbc_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run.","title":"LoadData Class"},{"location":"project_examples/bbc_news_nlp/#utility_method_fetch_bbc_data","text":"The fetch_bbc_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. Lastly, the function assigns a unique_id to each article which is based on its article id and the source it originates from. This is because the same article may be published in multiple sources with the same id, which means our original ids are not unique and could be misleading. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_bbc_data ( self , link ): \"\"\"Parse and label RSS BBC News data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # remove incompatible columns data . drop ( [ \"title_detail\" , \"summary_detail\" , \"links\" , \"published_parsed\" ], axis = 1 , inplace = True , ) # get the source (this only works for BBC RSS feeds) data [ \"source\" ] = link [ 29 : - 8 ] . replace ( \"/\" , \"_\" ) # generating ids to be unique, since same story ids can be published in different sources data [ \"unique_id\" ] = data [ \"id\" ] + data [ \"source\" ] return data def setup ( self ): self . set_run_steps ([ \"Appending BBC data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending BBC data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_bbc_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml","title":"Utility Method (fetch_bbc_data)"},{"location":"project_examples/bbc_news_nlp/#step_2_modelling_group","text":"Quick Summary: Create the SQL query dim_bbc_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml","title":"Step 2: Modelling Group"},{"location":"project_examples/bbc_news_nlp/#task_details_dim_bbc_feeds","text":"Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_bbc_feeds.sql sql/dim_bbc_feeds.sql SELECT DISTINCT unique_id , id , title , summary , link , guidislink , published , source FROM {{ user_prefix }} logs_bbc_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_bbc_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data","title":"Task Details (dim_bbc_feeds)"},{"location":"project_examples/bbc_news_nlp/#step_3_data_science_group","text":"Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_bbc_feeds_nlp_stats to calculate aggregate statistics grouped by source","title":"Step 3: Data Science Group"},{"location":"project_examples/bbc_news_nlp/#group_overview","text":"Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_bbc_feeds table, therefore we will need to set their their table parameters to dim_bbc_feeds . Since both of these tasks are children of the dim_bbc_feeds task, we will also need to set their parents attributes to dim_bbc_feeds . The nlp task has a text parameter, this parameter specifies which columns have text for processing. The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords (e.g. \"say\" and its variations seem to be very common in summaries, however they are not very informative). tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds text : - title - summary wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds stopwords : - say - said - says - will - country - US - England - Scotland - Wales - NI - Ireland - Europe - BBC - yn","title":"Group Overview"},{"location":"project_examples/bbc_news_nlp/#task_details_wordcloud","text":"The wordcloud task will have the following steps: Grouping texts : aggregates article summaries and groups them by source (summaries are used instead of titles since they tend to be longer) Generating clouds : generates a wordcloud for each source, as well as the full dataset","title":"Task Details (wordcloud)"},{"location":"project_examples/bbc_news_nlp/#rendercloud_class","text":"Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"firebrick\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) except : mask = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . summary ) sources = df . groupby ( \"source\" ) grouped_texts = sources . summary . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating bbc_wordcloud.png\" ) self . word_cloud ( \"bbc\" , full_text , stopwords , b_colour = \"white\" , c_colour = \"black\" ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords ) return self . success ()","title":"RenderCloud Class"},{"location":"project_examples/bbc_news_nlp/#task_details_nlp","text":"The nlp task will have the following steps: Processing texts : loops through text_fields, generates text statistics on each entry Updating database : similar to LoadData step, has additional debugging information","title":"Task Details (nlp)"},{"location":"project_examples/bbc_news_nlp/#languageprocessing_class","text":"Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] text_fields = self . parameters [ \"text\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) for t in text_fields : self . info ( f \"Processing texts for { t } field\" ) self . desc_text ( df , t , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success ()","title":"LanguageProcessing Class"},{"location":"project_examples/bbc_news_nlp/#task_details_dim_bbc_feeds_nlp_stats","text":"Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_bbc_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_bbc_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_tl , AVG ( title_words ) AS average_tw , AVG ( title_sentences ) AS average_ts , AVG ( summary_letters ) AS average_sl , AVG ( summary_words ) AS average_sw , AVG ( summary_sentences ) AS average_ss FROM {{ user_prefix }} dim_bbc_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_bbc_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data dim_bbc_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp","title":"Task Details (dim_bbc_feeds_nlp_stats)"},{"location":"project_examples/bbc_news_nlp/#step_4_run_the_project","text":"All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Step 4: Run the project"},{"location":"project_examples/facebook_data_project/","text":"SAYN Project Example: Facebook Data Project \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts Facebook Messenger data Loads it into a SQLite database Cleans the extracted data Calculates reply times for chat data Performs some basic text and sentiment analysis on the transformed data Generates wordcloud timelapse GIFs for each conversation Generates a bar chart race GIF for most shared sites in chat data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: Data processing: numpy , pandas , nltk , vaderSentiment Visualisations: matplotlib , wordcloud , pillow , bar_chart_race By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. You can also connect this database to your preferred visualisation tool. Running The Project \u00b6 To run the project, you will need to: clone the repository with git clone https://github.com/173TECH/facebook_data_project.git . rename the sample_settings.yaml file to settings.yaml . install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. install ImageMagick , details here: https://imagemagick.org/ use sayn run from the root of the project folder to run all SAYN commands. Attention This project comes with a sample dataset, you should use this dataset to test run the project. After a successful run you should see 3 new files in python/img , these should be the following: sample_Goku_timelapse.gif sample_Vegeta_timelapse.gif chart_race.gif Adding Your Facebook Messenger Data \u00b6 For this you will need your Facebook Messenger data in JSON format, you can get request it by doing the following: Sign in to Facebook Go to Settings & Privacy > Settings > Your Facebook Information > Download Your Information Change format to JSON and click Create File (this can take a while depending on your date range and media quality) Once you have the data, you can find the chat data in messages/inbox (you should see a collection of folders corresponding to each of your chats): Copy and paste the chat folders you are interested into the data folder in this project. In tasks/data_science.yaml , change the facebook_name parameter to your full name on Facebook Note: If you use a large amount of chat data you will experience longer load times for certain tasks Note If you use a large amount of chat data you will experience longer load times for certain tasks","title":"Facebook Data Project"},{"location":"project_examples/facebook_data_project/#sayn_project_example_facebook_data_project","text":"","title":"SAYN Project Example: Facebook Data Project"},{"location":"project_examples/facebook_data_project/#project_description","text":"","title":"Project Description"},{"location":"project_examples/facebook_data_project/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts Facebook Messenger data Loads it into a SQLite database Cleans the extracted data Calculates reply times for chat data Performs some basic text and sentiment analysis on the transformed data Generates wordcloud timelapse GIFs for each conversation Generates a bar chart race GIF for most shared sites in chat data","title":"Overview"},{"location":"project_examples/facebook_data_project/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: Data processing: numpy , pandas , nltk , vaderSentiment Visualisations: matplotlib , wordcloud , pillow , bar_chart_race By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. You can also connect this database to your preferred visualisation tool.","title":"Features Used"},{"location":"project_examples/facebook_data_project/#running_the_project","text":"To run the project, you will need to: clone the repository with git clone https://github.com/173TECH/facebook_data_project.git . rename the sample_settings.yaml file to settings.yaml . install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. install ImageMagick , details here: https://imagemagick.org/ use sayn run from the root of the project folder to run all SAYN commands. Attention This project comes with a sample dataset, you should use this dataset to test run the project. After a successful run you should see 3 new files in python/img , these should be the following: sample_Goku_timelapse.gif sample_Vegeta_timelapse.gif chart_race.gif","title":"Running The Project"},{"location":"project_examples/facebook_data_project/#adding_your_facebook_messenger_data","text":"For this you will need your Facebook Messenger data in JSON format, you can get request it by doing the following: Sign in to Facebook Go to Settings & Privacy > Settings > Your Facebook Information > Download Your Information Change format to JSON and click Create File (this can take a while depending on your date range and media quality) Once you have the data, you can find the chat data in messages/inbox (you should see a collection of folders corresponding to each of your chats): Copy and paste the chat folders you are interested into the data folder in this project. In tasks/data_science.yaml , change the facebook_name parameter to your full name on Facebook Note: If you use a large amount of chat data you will experience longer load times for certain tasks Note If you use a large amount of chat data you will experience longer load times for certain tasks","title":"Adding Your Facebook Messenger Data"},{"location":"project_examples/reddit_news_nlp/","text":"SAYN Project Example: Reddit News NLP \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from Reddit RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Implementation Details \u00b6 Step 1: Extract Task Group \u00b6 Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data Task Details ( load_data ) \u00b6 First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_reddit_feeds links : - https://www.reddit.com/r/USnews/new/.rss - https://www.reddit.com/r/UKnews/new/.rss - https://www.reddit.com/r/EUnews/new/.rss Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending Reddit data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method LoadData Class \u00b6 Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_reddit_data : fetches data from the Reddit RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_reddit_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. Utility Method ( fetch_reddit_data ) \u00b6 The fetch_reddit_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_reddit_data ( self , link ): \"\"\"Parse and label RSS Reddit data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # select columns of interest data = data . loc [:, [ \"id\" , \"link\" , \"updated\" , \"published\" , \"title\" ]] # get the source, only works for Reddit RSS feeds source_elements = link . split ( \"/\" ) data [ \"source\" ] = source_elements [ 4 ] + \"_\" + source_elements [ 5 ] return data def setup ( self ): self . set_run_steps ([ \"Appending Reddit data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending Reddit data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_reddit_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml Step 2: Modelling Group \u00b6 Quick Summary: Create the SQL query dim_reddit_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml Task Details ( dim_reddit_feeds ) \u00b6 Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_reddit_feeds.sql sql/dim_reddit_feeds.sql SELECT DISTINCT id , title , published , updated , link , source FROM {{ user_prefix }} logs_reddit_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_reddit_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data Step 3: Data Science Group \u00b6 Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_reddit_feeds_nlp_stats to calculate aggregate statistics grouped by source Group Overview \u00b6 Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_reddit_feeds table, therefore we will need to set their their table parameters to dim_reddit_feeds . Since both of these tasks are children of the dim_reddit_feeds task, we will also need to set their parents attributes to dim_reddit_feeds . The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords. tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds stopwords : - Reddit Task Details ( wordcloud ) \u00b6 The wordcloud task will have the following steps: Grouping texts : aggregates article titles and groups them by source Generating clouds : generates a wordcloud for each source, as well as the full dataset RenderCloud Class \u00b6 Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"black\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) image_colours = ImageColorGenerator ( mask ) except : mask = None image_colours = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , color_func = image_colours , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . title ) sources = df . groupby ( \"source\" ) grouped_texts = sources . title . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating reddit_wordcloud.png\" ) self . word_cloud ( \"reddit\" , full_text , stopwords ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords , b_colour = \"black\" , c_colour = \"white\" ) return self . success () Task Details ( nlp ) \u00b6 The nlp task will have the following steps: Processing texts : generates text statistics for each title Updating database : similar to LoadData step, has additional debugging information LanguageProcessing Class \u00b6 Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) self . info ( f \"Processing texts for title field\" ) self . desc_text ( df , \"title\" , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success () Task Details ( dim_reddit_feeds_nlp_stats ) \u00b6 Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_reddit_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_reddit_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_letters , AVG ( title_words ) AS average_words , AVG ( title_sentences ) AS average_sentences FROM {{ user_prefix }} dim_reddit_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_reddit_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data dim_reddit_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp Step 4: Run the project \u00b6 All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Reddit News NLP"},{"location":"project_examples/reddit_news_nlp/#sayn_project_example_reddit_news_nlp","text":"","title":"SAYN Project Example: Reddit News NLP"},{"location":"project_examples/reddit_news_nlp/#project_description","text":"","title":"Project Description"},{"location":"project_examples/reddit_news_nlp/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from Reddit RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data","title":"Overview"},{"location":"project_examples/reddit_news_nlp/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow","title":"Features Used"},{"location":"project_examples/reddit_news_nlp/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/reddit_news_nlp/#implementation_details","text":"","title":"Implementation Details"},{"location":"project_examples/reddit_news_nlp/#step_1_extract_task_group","text":"Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data","title":"Step 1: Extract Task Group"},{"location":"project_examples/reddit_news_nlp/#task_details_load_data","text":"First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_reddit_feeds links : - https://www.reddit.com/r/USnews/new/.rss - https://www.reddit.com/r/UKnews/new/.rss - https://www.reddit.com/r/EUnews/new/.rss Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending Reddit data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method","title":"Task Details (load_data)"},{"location":"project_examples/reddit_news_nlp/#loaddata_class","text":"Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_reddit_data : fetches data from the Reddit RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_reddit_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run.","title":"LoadData Class"},{"location":"project_examples/reddit_news_nlp/#utility_method_fetch_reddit_data","text":"The fetch_reddit_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_reddit_data ( self , link ): \"\"\"Parse and label RSS Reddit data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # select columns of interest data = data . loc [:, [ \"id\" , \"link\" , \"updated\" , \"published\" , \"title\" ]] # get the source, only works for Reddit RSS feeds source_elements = link . split ( \"/\" ) data [ \"source\" ] = source_elements [ 4 ] + \"_\" + source_elements [ 5 ] return data def setup ( self ): self . set_run_steps ([ \"Appending Reddit data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending Reddit data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_reddit_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml","title":"Utility Method (fetch_reddit_data)"},{"location":"project_examples/reddit_news_nlp/#step_2_modelling_group","text":"Quick Summary: Create the SQL query dim_reddit_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml","title":"Step 2: Modelling Group"},{"location":"project_examples/reddit_news_nlp/#task_details_dim_reddit_feeds","text":"Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_reddit_feeds.sql sql/dim_reddit_feeds.sql SELECT DISTINCT id , title , published , updated , link , source FROM {{ user_prefix }} logs_reddit_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_reddit_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data","title":"Task Details (dim_reddit_feeds)"},{"location":"project_examples/reddit_news_nlp/#step_3_data_science_group","text":"Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_reddit_feeds_nlp_stats to calculate aggregate statistics grouped by source","title":"Step 3: Data Science Group"},{"location":"project_examples/reddit_news_nlp/#group_overview","text":"Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_reddit_feeds table, therefore we will need to set their their table parameters to dim_reddit_feeds . Since both of these tasks are children of the dim_reddit_feeds task, we will also need to set their parents attributes to dim_reddit_feeds . The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords. tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds stopwords : - Reddit","title":"Group Overview"},{"location":"project_examples/reddit_news_nlp/#task_details_wordcloud","text":"The wordcloud task will have the following steps: Grouping texts : aggregates article titles and groups them by source Generating clouds : generates a wordcloud for each source, as well as the full dataset","title":"Task Details (wordcloud)"},{"location":"project_examples/reddit_news_nlp/#rendercloud_class","text":"Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"black\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) image_colours = ImageColorGenerator ( mask ) except : mask = None image_colours = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , color_func = image_colours , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . title ) sources = df . groupby ( \"source\" ) grouped_texts = sources . title . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating reddit_wordcloud.png\" ) self . word_cloud ( \"reddit\" , full_text , stopwords ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords , b_colour = \"black\" , c_colour = \"white\" ) return self . success ()","title":"RenderCloud Class"},{"location":"project_examples/reddit_news_nlp/#task_details_nlp","text":"The nlp task will have the following steps: Processing texts : generates text statistics for each title Updating database : similar to LoadData step, has additional debugging information","title":"Task Details (nlp)"},{"location":"project_examples/reddit_news_nlp/#languageprocessing_class","text":"Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) self . info ( f \"Processing texts for title field\" ) self . desc_text ( df , \"title\" , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success ()","title":"LanguageProcessing Class"},{"location":"project_examples/reddit_news_nlp/#task_details_dim_reddit_feeds_nlp_stats","text":"Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_reddit_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_reddit_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_letters , AVG ( title_words ) AS average_words , AVG ( title_sentences ) AS average_sentences FROM {{ user_prefix }} dim_reddit_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_reddit_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data dim_reddit_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp","title":"Task Details (dim_reddit_feeds_nlp_stats)"},{"location":"project_examples/reddit_news_nlp/#step_4_run_the_project","text":"All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Step 4: Run the project"},{"location":"project_examples/simple_etl/","text":"SAYN Project Example: A Simple ETL \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to implement a simple ETL with the framework. You can find the GitHub repository here . This ETL extracts jokes from an API, translates them into Yodish (the language of Yoda, this is) with another API and then runs some SQL transformations on the extracted data. Both APIs are public and do not require an API key. However, they both have limited quotas (especially the Yodish translation API) so you should avoid re-running the extraction part of the project multiple times in a row (you can use the command sayn run -x tag:extract after the first sayn run ). Features Used \u00b6 Python task to extract data with APIs. Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_simple_etl.git . Rename the settings_sample.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Running The Project With PostgreSQL \u00b6 If desired, you can also run the project using a PostgreSQL database. For this, you simply need to: Change the warehouse credential to use a PostgreSQL database connection. Install psycopg2 as a package.","title":"A Simple ETL"},{"location":"project_examples/simple_etl/#sayn_project_example_a_simple_etl","text":"","title":"SAYN Project Example: A Simple ETL"},{"location":"project_examples/simple_etl/#project_description","text":"","title":"Project Description"},{"location":"project_examples/simple_etl/#overview","text":"This is an example SAYN project which shows how to implement a simple ETL with the framework. You can find the GitHub repository here . This ETL extracts jokes from an API, translates them into Yodish (the language of Yoda, this is) with another API and then runs some SQL transformations on the extracted data. Both APIs are public and do not require an API key. However, they both have limited quotas (especially the Yodish translation API) so you should avoid re-running the extraction part of the project multiple times in a row (you can use the command sayn run -x tag:extract after the first sayn run ).","title":"Overview"},{"location":"project_examples/simple_etl/#features_used","text":"Python task to extract data with APIs. Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily.","title":"Features Used"},{"location":"project_examples/simple_etl/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_simple_etl.git . Rename the settings_sample.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/simple_etl/#running_the_project_with_postgresql","text":"If desired, you can also run the project using a PostgreSQL database. For this, you simply need to: Change the warehouse credential to use a PostgreSQL database connection. Install psycopg2 as a package.","title":"Running The Project With PostgreSQL"},{"location":"settings/project_yaml/","text":"Settings: project.yaml \u00b6 The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"project.yaml"},{"location":"settings/project_yaml/#settings_projectyaml","text":"The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"Settings: project.yaml"},{"location":"settings/settings_yaml/","text":"Settings: settings.yaml \u00b6 The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production. Defining Credentials \u00b6 Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects. Using Environment Variables \u00b6 Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are either basic types (ie: strings), json or yaml encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh # JSON encoded credential export SAYN_CREDENTIAL_warehouse = '{\"type\": \"snowflake\", \"account\": ...}' # YAML encoded credential export SAYN_CREDENTIAL_backend = \" type: postgresql host: host.address.com user: ... \" # Project parameters as strings export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" # Project parameters allow complex types JSON or YAML encoded export SAYN_PARAMETER_dict_param = \" key1: value1 key2: value2 \" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence.","title":"settings.yaml"},{"location":"settings/settings_yaml/#settings_settingsyaml","text":"The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production.","title":"Settings: settings.yaml"},{"location":"settings/settings_yaml/#defining_credentials","text":"Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects.","title":"Defining Credentials"},{"location":"settings/settings_yaml/#using_environment_variables","text":"Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are either basic types (ie: strings), json or yaml encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh # JSON encoded credential export SAYN_CREDENTIAL_warehouse = '{\"type\": \"snowflake\", \"account\": ...}' # YAML encoded credential export SAYN_CREDENTIAL_backend = \" type: postgresql host: host.address.com user: ... \" # Project parameters as strings export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" # Project parameters allow complex types JSON or YAML encoded export SAYN_PARAMETER_dict_param = \" key1: value1 key2: value2 \" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence.","title":"Using Environment Variables"},{"location":"tasks/autosql/","text":"autosql Task \u00b6 About \u00b6 The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks \u00b6 An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. schema : the (optional) destination schema where the object will be created. table : is the name of the object that will be created. db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . Using autosql In incremental Mode \u00b6 autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2 Defining DDLs \u00b6 Autosql tasks support the definition of optional DDL. Each DDL entry is independent to others (you can define only DDLs which are relevant to you). Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages. ALTER TABLE DDLs \u00b6 The following DDLs will be issued by SAYN with ALTER TABLE statements: indexes: the indexes to add on the table. primary_key: this should be added in the indexes section using the primary_key name for the index. permissions: the permissions you want to give to each role. You should map each role to the rights you want to grant separated by commas (e.g. SELECT, DELETE). autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 permissions : role_name : SELECT ... CREATE TABLE DDLs \u00b6 SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. columns can define the following attributes: name: the column name. type: the column type. primary: set to True if the column is part of the primary key. unique: set to True to enforce a unique constraint on the column. not_null: set to True to enforce a non null constraint on the column. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"AutoSQL"},{"location":"tasks/autosql/#autosql_task","text":"","title":"autosql Task"},{"location":"tasks/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/autosql/#defining_autosql_tasks","text":"An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. schema : the (optional) destination schema where the object will be created. table : is the name of the object that will be created. db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"Defining autosql Tasks"},{"location":"tasks/autosql/#using_autosql_in_incremental_mode","text":"autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2","title":"Using autosql In incremental Mode"},{"location":"tasks/autosql/#defining_ddls","text":"Autosql tasks support the definition of optional DDL. Each DDL entry is independent to others (you can define only DDLs which are relevant to you). Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages.","title":"Defining DDLs"},{"location":"tasks/autosql/#alter_table_ddls","text":"The following DDLs will be issued by SAYN with ALTER TABLE statements: indexes: the indexes to add on the table. primary_key: this should be added in the indexes section using the primary_key name for the index. permissions: the permissions you want to give to each role. You should map each role to the rights you want to grant separated by commas (e.g. SELECT, DELETE). autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 permissions : role_name : SELECT ...","title":"ALTER TABLE DDLs"},{"location":"tasks/autosql/#create_table_ddls","text":"SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. columns can define the following attributes: name: the column name. type: the column type. primary: set to True if the column is part of the primary key. unique: set to True to enforce a unique constraint on the column. not_null: set to True to enforce a non null constraint on the column. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"CREATE TABLE DDLs"},{"location":"tasks/copy/","text":"copy Task \u00b6 About \u00b6 The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Defining copy Tasks \u00b6 A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . By default, tables will be copied in full every time SAYN runs replacing the table with the newly pulled data. This behaviour can be altered with the following: incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior or equal to the maximum found in the destination, or with a NULL value. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. append : a boolean flag indicating if data should be replaced in the destination. This means that in full load mode ( incremental_key not specified) records will be appended rather than the table being recreated every time; and in incremental mode records will not be removed, so delete_key shouldn't be specified. Additionally an extra column _sayn_load_ts will be added to the destination table to help with de-duplication. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at append : True In this other example, whenever the task runs it checks the latest value of updated_at and appends to the destination table every record in the source with an updated_at greater than or equal to the maximum value present in the destination. While the task is running, SAYN will get records from the source database and load into a temporary table, and will merge into the destination table once all records have been loaded. The frequency of loading into this table is determined by the value of max_batch_rows as defined in the credentials for the destination database, which defaults to 50000. However this behaviour can be changed with 2 properties: max_batch_rows : this allows you to overwrite the value specified in the credential for this task only. max_merge_rows : this value changes the behaviour so that instead of merging into the destination table once all rows have been loaded, instead SAYN will merge after this number of records have been loaded and then it will repeat the whole process. The advantage of using this parameter is that for copies that take a long time, an error (ie: loosing the connection with the source) wouldn't result in the process having to be started again from the beginning. Warning When using max_merge_rows SAYN will loop through the merge load and merge process until the number of records loaded is lower than the value of max_merge_rows . In order to avoid infinite loops, the process will also stop after a maximum of 100 iteration. To avoid issues, it should be set to a very large value (larger than max_batch_rows ). Data types and DDL \u00b6 copy tasks accept a ddl field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id ddl : columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP An additional property dst_name in columns is also supported. Specifying this property will change the name of the column in the destination table. When using this property, delete_key and incremental_key need to reference this new name. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_ts delete_key : id ddl : columns : - id - name : updated_at dst_name : updated_ts In this example, the updated_at column at source will be called updated_ts on the target. Note the name in incremental_key uses the name on the target. Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Copy"},{"location":"tasks/copy/#copy_task","text":"","title":"copy Task"},{"location":"tasks/copy/#about","text":"The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse.","title":"About"},{"location":"tasks/copy/#defining_copy_tasks","text":"A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . By default, tables will be copied in full every time SAYN runs replacing the table with the newly pulled data. This behaviour can be altered with the following: incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior or equal to the maximum found in the destination, or with a NULL value. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. append : a boolean flag indicating if data should be replaced in the destination. This means that in full load mode ( incremental_key not specified) records will be appended rather than the table being recreated every time; and in incremental mode records will not be removed, so delete_key shouldn't be specified. Additionally an extra column _sayn_load_ts will be added to the destination table to help with de-duplication. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at append : True In this other example, whenever the task runs it checks the latest value of updated_at and appends to the destination table every record in the source with an updated_at greater than or equal to the maximum value present in the destination. While the task is running, SAYN will get records from the source database and load into a temporary table, and will merge into the destination table once all records have been loaded. The frequency of loading into this table is determined by the value of max_batch_rows as defined in the credentials for the destination database, which defaults to 50000. However this behaviour can be changed with 2 properties: max_batch_rows : this allows you to overwrite the value specified in the credential for this task only. max_merge_rows : this value changes the behaviour so that instead of merging into the destination table once all rows have been loaded, instead SAYN will merge after this number of records have been loaded and then it will repeat the whole process. The advantage of using this parameter is that for copies that take a long time, an error (ie: loosing the connection with the source) wouldn't result in the process having to be started again from the beginning. Warning When using max_merge_rows SAYN will loop through the merge load and merge process until the number of records loaded is lower than the value of max_merge_rows . In order to avoid infinite loops, the process will also stop after a maximum of 100 iteration. To avoid issues, it should be set to a very large value (larger than max_batch_rows ).","title":"Defining copy Tasks"},{"location":"tasks/copy/#data_types_and_ddl","text":"copy tasks accept a ddl field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id ddl : columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP An additional property dst_name in columns is also supported. Specifying this property will change the name of the column in the destination table. When using this property, delete_key and incremental_key need to reference this new name. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_ts delete_key : id ddl : columns : - id - name : updated_at dst_name : updated_ts In this example, the updated_at column at source will be called updated_ts on the target. Note the name in incremental_key uses the name on the target. Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Data types and DDL"},{"location":"tasks/dummy/","text":"dummy Task \u00b6 About \u00b6 The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups. Defining dummy Tasks \u00b6 A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy Usage \u00b6 dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Dummy"},{"location":"tasks/dummy/#dummy_task","text":"","title":"dummy Task"},{"location":"tasks/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups.","title":"About"},{"location":"tasks/dummy/#defining_dummy_tasks","text":"A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy","title":"Defining dummy Tasks"},{"location":"tasks/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Usage"},{"location":"tasks/overview/","text":"Tasks \u00b6 About \u00b6 Tasks are the backbone of your SAYN project. They are used by SAYN to create a DAG (Directed Acyclic Graph). Info A Directed Acyclic Graph is a concept which enables to conveniently model tasks and dependencies. It uses the following key principles graph : a specific data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined with the parents list. To relate back to the DAG concept, this implies each task in SAYN represents a node and edges are defined by the parents attribute of each task. For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Task Types \u00b6 Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file. Defining Tasks \u00b6 Tasks are defined in YAML files located under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list on_fail Defines the behaviour when the task fails . Optional one of: skip or no_skip Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it. Task Groups \u00b6 Task groups are a convenient way to segment and organise your data processes in your SAYN project. Each YAML file in the tasks folder represents a task group. Tip When growing a SAYN project, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. Each task group file defines tasks (required) and presets (optional). tasks/base.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data Property Description Required tasks The set of tasks that compose the task group. For more details on tasks , please see the Tasks section. Yes presets Defines preset task structures shared by several tasks. Presets defined within task group files can inherit from presets defined at the project level in project.yaml . See the Presets section for more details. Optional Task Attributes \u00b6 Task attributes can be used when defining tasks in a dynamic way. The following example shows how to use the task name and task group dynamically when defining a task: tasks/base.yaml tasks : sql_task : type : sql file_name : '{{task.group}}/{{task.name}}.sql' This will effectively tell the task to look for a file located at base/sql_task.sql in the sql folder. Task failure behaviour \u00b6 When a task fails during an execution, all descendent tasks will be skipped as expected. However sometimes it can be useful to execute descending tasks even if a parent fails, for example when an API can frequently throw errors and we want to continue the execution just with as much data as it was possible to pull from it. In this case we make use of the on_fail task property to specify that we do not want to skip descending tasks. tasks/base.yaml tasks : could_fail_task : type : python class : could_fail.CouldFailTask on_fail : no_skip child_task : type : sql file_name : query_using_could_fail_data.sql parents : - failing_task In the above case, if could_fail_task fails, child_task will not be skipped.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"Tasks are the backbone of your SAYN project. They are used by SAYN to create a DAG (Directed Acyclic Graph). Info A Directed Acyclic Graph is a concept which enables to conveniently model tasks and dependencies. It uses the following key principles graph : a specific data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined with the parents list. To relate back to the DAG concept, this implies each task in SAYN represents a node and edges are defined by the parents attribute of each task. For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers!","title":"About"},{"location":"tasks/overview/#task_types","text":"Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Task Types"},{"location":"tasks/overview/#defining_tasks","text":"Tasks are defined in YAML files located under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list on_fail Defines the behaviour when the task fails . Optional one of: skip or no_skip Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it.","title":"Defining Tasks"},{"location":"tasks/overview/#task_groups","text":"Task groups are a convenient way to segment and organise your data processes in your SAYN project. Each YAML file in the tasks folder represents a task group. Tip When growing a SAYN project, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. Each task group file defines tasks (required) and presets (optional). tasks/base.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data Property Description Required tasks The set of tasks that compose the task group. For more details on tasks , please see the Tasks section. Yes presets Defines preset task structures shared by several tasks. Presets defined within task group files can inherit from presets defined at the project level in project.yaml . See the Presets section for more details. Optional","title":"Task Groups"},{"location":"tasks/overview/#task_attributes","text":"Task attributes can be used when defining tasks in a dynamic way. The following example shows how to use the task name and task group dynamically when defining a task: tasks/base.yaml tasks : sql_task : type : sql file_name : '{{task.group}}/{{task.name}}.sql' This will effectively tell the task to look for a file located at base/sql_task.sql in the sql folder.","title":"Task Attributes"},{"location":"tasks/overview/#task_failure_behaviour","text":"When a task fails during an execution, all descendent tasks will be skipped as expected. However sometimes it can be useful to execute descending tasks even if a parent fails, for example when an API can frequently throw errors and we want to continue the execution just with as much data as it was possible to pull from it. In this case we make use of the on_fail task property to specify that we do not want to skip descending tasks. tasks/base.yaml tasks : could_fail_task : type : python class : could_fail.CouldFailTask on_fail : no_skip child_task : type : sql file_name : query_using_could_fail_data.sql parents : - failing_task In the above case, if could_fail_task fails, child_task will not be skipped.","title":"Task failure behaviour"},{"location":"tasks/python/","text":"python Task \u00b6 About \u00b6 The python task allows you run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. Defining python Tasks \u00b6 A python task is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder. Writing A python Task \u00b6 Basics \u00b6 The basic code to construct a python task is: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def setup ( self ): # Do some checked return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Please note that python tasks need to return either self.success() or self.fail() in order to run. Using the SAYN API \u00b6 When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. Here's a list of properties available: self.parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. self.run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). self.connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. self.default_db : provides access to the default_db database object specified in the project.yaml file. Tip You can use self.default_db to easily perform some operations on the default database such as reading or loading data. See the methods available on the Database API. We all love pandas ! If you want to load a pandas dataframe you can use one of these options: with the pandas.DataFrame.to_sql method: df.to_sql(self.default_db.engine, 'table') . with the self.default_db.load_data method: self.default_db.load_data('table', df.to_dict('records')) . Logging For python Tasks With The SAYN API \u00b6 The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py self . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with self . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Python"},{"location":"tasks/python/#python_task","text":"","title":"python Task"},{"location":"tasks/python/#about","text":"The python task allows you run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models.","title":"About"},{"location":"tasks/python/#defining_python_tasks","text":"A python task is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder.","title":"Defining python Tasks"},{"location":"tasks/python/#writing_a_python_task","text":"","title":"Writing A python Task"},{"location":"tasks/python/#basics","text":"The basic code to construct a python task is: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def setup ( self ): # Do some checked return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Please note that python tasks need to return either self.success() or self.fail() in order to run.","title":"Basics"},{"location":"tasks/python/#using_the_sayn_api","text":"When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. Here's a list of properties available: self.parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. self.run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). self.connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. self.default_db : provides access to the default_db database object specified in the project.yaml file. Tip You can use self.default_db to easily perform some operations on the default database such as reading or loading data. See the methods available on the Database API. We all love pandas ! If you want to load a pandas dataframe you can use one of these options: with the pandas.DataFrame.to_sql method: df.to_sql(self.default_db.engine, 'table') . with the self.default_db.load_data method: self.default_db.load_data('table', df.to_dict('records')) .","title":"Using the SAYN API"},{"location":"tasks/python/#logging_for_python_tasks_with_the_sayn_api","text":"The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py self . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with self . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Logging For python Tasks With The SAYN API"},{"location":"tasks/sql/","text":"sql Task \u00b6 About \u00b6 The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql . Defining sql Tasks \u00b6 A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : path to a file under the sql folder containing the SQL script to execute. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"SQL"},{"location":"tasks/sql/#sql_task","text":"","title":"sql Task"},{"location":"tasks/sql/#about","text":"The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql .","title":"About"},{"location":"tasks/sql/#defining_sql_tasks","text":"A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : path to a file under the sql folder containing the SQL script to execute. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"Defining sql Tasks"},{"location":"tutorials/tutorial_part1/","text":"Tutorial: Part 1 \u00b6 This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section . This project generates some random data with a python task and performs some modelling on it with autosql tasks. Running SAYN \u00b6 To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You can open dev.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background. Project Overview \u00b6 The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution. Implementing Your Project \u00b6 Now let's see how the tutorial project would be created from scratch. Step 1: Define The Project In project.yaml \u00b6 The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped. Step 2: Define Your Individual Settings With settings.yaml \u00b6 The settings.yaml file at the root level of your directory and contains: settings.yaml profiles : dev : credentials : warehouse : dev_db prod : credentials : warehouse : prod_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db prod_db : type : sqlite database : prod.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and credentials defined below. In this case we define 2 profiles dev and prod. default_profile : the profile used by default at execution time. It can be overriden using sayn run -p prod . credentials : here we define the credentials. In this case we have two for dev and prod, that are used as warehouse on each profile. Step 3: Define Your Tasks \u00b6 In SAYN, tasks are defined in yaml files within the tasks folder. Each file is considered a task group . Our project contains only one task group: base.yaml : tasks/base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : table : dim_tournaments parents : - load_data # ... The tasks entry contains a map of tasks definitions. In this case we're using two types of tasks: python : lets you define a task written in Python. Python tasks are useful to complete your extraction and load layers if you're using an ELT tool or for data science models defined in Python. autosql : lets you write a SELECT statement while SAYN manages the table or view creation automatically for you. Our example has multiple autosql tasks which create models based on the logs. Tip Although this tutorial only has one file in the tasks folder, you can separate tasks in multiple files. SAYN automatically includes any file from the tasks folder with a .yaml extension when creating the DAG. Each file is considered a task group . load_data Task \u00b6 In our example project the only python task is load_data which creates some synthetic logs and loads them to our database. The code can be found in the class LoadData in python/load_data.py . Let's have a look at the main elements of a python task: python/load_data.py # ... from sayn import PythonTask class LoadData ( PythonTask ): def run ( self ): # Your code here The above is the beginning of the python task. When the execution of sayn run hits the load_data task the code in the run method will execute. A task in SAYN can be split into multiple steps, which is useful for debugging when errors occur. In this case, we first generate all data and then we load each dataset one by one. We can define the steps a task will follow with the self.set_run_steps method. python/load_data.py def run ( self ): # ... self . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) # ... To indicate SAYN what step is executing, we can use the following construct: python/load_data.py def run ( self ): # ... with self . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( self . fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( self . arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( self . tournaments ) ] Here our \"Generate Dimensions\" step simply generates the dimension variables with an id. The final core element is accessing databases. In our project we defined a single credential called warehouse and we made this the default_db . To access this we just need to use self.default_db . python/load_data.py self . default_db . execute ( q_create ) The main method in Database objects is execute which accepts a sql script via parameter and executes it in a transaction. Another method used in this tutorial is load_data which loads a dataset into the database automatically creating a table for it first. For more information about how to build python tasks, visit the python tasks section . autosql Tasks \u00b6 Let's have a look at one of the autosql tasks ( dim_tournaments ). As you can see in tasks/base.yaml above, we specify a file_name which contains: sql/dim_tournaments.yaml SELECT l . tournament_id , l . tournament_name FROM logs_tournaments l This is a simple SELECT statement that SAYN will use when creating a table called dim_tournaments as defined in the destination field in the base.yaml file. For more information about setting up autosql tasks, visit the autosql tasks section . Running Your Project \u00b6 So far we've used sayn run to execute our project, however SAYN provides more options: sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : allows the filtering the tasks to run. More options are available to run specific components of your SAYN project. All details can be found in the SAYN cli section. What Next? \u00b6 You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"Tutorial (Part 1)"},{"location":"tutorials/tutorial_part1/#tutorial_part_1","text":"This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section . This project generates some random data with a python task and performs some modelling on it with autosql tasks.","title":"Tutorial: Part 1"},{"location":"tutorials/tutorial_part1/#running_sayn","text":"To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You can open dev.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background.","title":"Running SAYN"},{"location":"tutorials/tutorial_part1/#project_overview","text":"The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution.","title":"Project Overview"},{"location":"tutorials/tutorial_part1/#implementing_your_project","text":"Now let's see how the tutorial project would be created from scratch.","title":"Implementing Your Project"},{"location":"tutorials/tutorial_part1/#step_1_define_the_project_in_projectyaml","text":"The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped.","title":"Step 1: Define The Project In project.yaml"},{"location":"tutorials/tutorial_part1/#step_2_define_your_individual_settings_with_settingsyaml","text":"The settings.yaml file at the root level of your directory and contains: settings.yaml profiles : dev : credentials : warehouse : dev_db prod : credentials : warehouse : prod_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db prod_db : type : sqlite database : prod.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and credentials defined below. In this case we define 2 profiles dev and prod. default_profile : the profile used by default at execution time. It can be overriden using sayn run -p prod . credentials : here we define the credentials. In this case we have two for dev and prod, that are used as warehouse on each profile.","title":"Step 2: Define Your Individual Settings With settings.yaml"},{"location":"tutorials/tutorial_part1/#step_3_define_your_tasks","text":"In SAYN, tasks are defined in yaml files within the tasks folder. Each file is considered a task group . Our project contains only one task group: base.yaml : tasks/base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : table : dim_tournaments parents : - load_data # ... The tasks entry contains a map of tasks definitions. In this case we're using two types of tasks: python : lets you define a task written in Python. Python tasks are useful to complete your extraction and load layers if you're using an ELT tool or for data science models defined in Python. autosql : lets you write a SELECT statement while SAYN manages the table or view creation automatically for you. Our example has multiple autosql tasks which create models based on the logs. Tip Although this tutorial only has one file in the tasks folder, you can separate tasks in multiple files. SAYN automatically includes any file from the tasks folder with a .yaml extension when creating the DAG. Each file is considered a task group .","title":"Step 3: Define Your Tasks"},{"location":"tutorials/tutorial_part1/#load_data_task","text":"In our example project the only python task is load_data which creates some synthetic logs and loads them to our database. The code can be found in the class LoadData in python/load_data.py . Let's have a look at the main elements of a python task: python/load_data.py # ... from sayn import PythonTask class LoadData ( PythonTask ): def run ( self ): # Your code here The above is the beginning of the python task. When the execution of sayn run hits the load_data task the code in the run method will execute. A task in SAYN can be split into multiple steps, which is useful for debugging when errors occur. In this case, we first generate all data and then we load each dataset one by one. We can define the steps a task will follow with the self.set_run_steps method. python/load_data.py def run ( self ): # ... self . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) # ... To indicate SAYN what step is executing, we can use the following construct: python/load_data.py def run ( self ): # ... with self . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( self . fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( self . arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( self . tournaments ) ] Here our \"Generate Dimensions\" step simply generates the dimension variables with an id. The final core element is accessing databases. In our project we defined a single credential called warehouse and we made this the default_db . To access this we just need to use self.default_db . python/load_data.py self . default_db . execute ( q_create ) The main method in Database objects is execute which accepts a sql script via parameter and executes it in a transaction. Another method used in this tutorial is load_data which loads a dataset into the database automatically creating a table for it first. For more information about how to build python tasks, visit the python tasks section .","title":"load_data Task"},{"location":"tutorials/tutorial_part1/#autosql_tasks","text":"Let's have a look at one of the autosql tasks ( dim_tournaments ). As you can see in tasks/base.yaml above, we specify a file_name which contains: sql/dim_tournaments.yaml SELECT l . tournament_id , l . tournament_name FROM logs_tournaments l This is a simple SELECT statement that SAYN will use when creating a table called dim_tournaments as defined in the destination field in the base.yaml file. For more information about setting up autosql tasks, visit the autosql tasks section .","title":"autosql Tasks"},{"location":"tutorials/tutorial_part1/#running_your_project","text":"So far we've used sayn run to execute our project, however SAYN provides more options: sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : allows the filtering the tasks to run. More options are available to run specific components of your SAYN project. All details can be found in the SAYN cli section.","title":"Running Your Project"},{"location":"tutorials/tutorial_part1/#what_next","text":"You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"What Next?"},{"location":"tutorials/tutorial_part2/","text":"Tutorial: Part 2 \u00b6 This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. In Tutorial: Part 1 we implemented our first ETL process with SAYN. We will now expand on that by adding parameters and presets . A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work. Step 1: Define The Project parameters \u00b6 You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters at the end of project.yaml . This is a YAML map which defines the default value. project.yaml # ... parameters : user_prefix : '' #no prefix for prod In this case we defined a parameter user_prefix that we will use to name tables. This is useful when multiple users are testing a project as it allows the final table name to be different between collaborators. Now we can define the value of the parameter on each profile. We do this in the settings.yaml . settings.yaml profiles : dev : credentials : warehouse : dev_db parameters : user_prefix : up_ prod : credentials : warehouse : prod_db Note how we don't redefine the parameter in our prod profile as the default value is more appropriate. Step 2: Making Tasks Dynamic With parameters \u00b6 Now that our parameters are setup, we can use those to make our tasks' code dynamic. In python Tasks \u00b6 For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils.py to see how we use the user_prefix parameter to change the table names. python/load_data.py # ... def run ( self ): user_prefix = self . parameters [ 'user_prefix' ] # ... q_create = get_create_table ( log_type , user_prefix ) # ... In autosql Tasks \u00b6 The files in the sql folder are always interpreted as Jinja templates. This means that in order to access parameters all we have to do is enclose it in {{ }} Jinja blocks. For example, in order to reference the tables created by load_data the dim_arenas task can be changed like this: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l Now sayn run will transform the above into valid SQL creating compile/base/dim_arenas.sql with it. The file path following the rule compile/task_group_name/autosql_task_name.sql : compile/base/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM up_logs_arenas l SAYN provides a sayn compile command that works like sayn run except that it won't execute the code. What it does though, is generate the compiled files that SAYN would run with the sayn run command. Step 3: Making Task Definitions Dynamic With parameters \u00b6 Now that our python task generates tables with the user_prefix in the name and our autosql tasks will select data from it. What we also need to do is change the table names our autosql tasks are generating. For that, let's take dim_arenas and modify it so that it generates a table called up_dim_arenas (or other user_prefix defined in settings.yaml ): tasks/base.yaml tasks : # ... dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data # ... Note the value of destination.table is now some Jinja code that will compile to the value of user_prefix followed by the name of the task. Step 4: Using presets To Standardise Task Definitions \u00b6 Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets allow you to define common properties shared by several tasks. tasks/base.yaml presets : modelling : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data tasks : # ... dim_arenas : preset : modelling # ... Now the modelling preset has to dynamic properties: * table : defined like we did in the previous so that the create table contains the user_prefix in the name. * file_name : that uses the task name to point at the correct file in the sql folder. In addition, modelling is defined so that tasks referencing it: * are autosql tasks. * Materialise as tables. * Have load_data as a parent task, so that models always run after our log generator. When a task references a preset, we're not restricted to the values defined in the preset. A task can override those values. Take f_rankings for example: tasks/base.yaml tasks : # ... f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Here we're overloading 2 properties: * materialisation which will make f_rankings a view rather than a table. * parents which will make f_ranking depend on f_fighter_results as well as load_data as defined in the preset. Running Our New Project \u00b6 You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a dev.db database prefix all tables with up_ and read from up_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables What Next? \u00b6 This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"Tutorial (Part 2)"},{"location":"tutorials/tutorial_part2/#tutorial_part_2","text":"This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. In Tutorial: Part 1 we implemented our first ETL process with SAYN. We will now expand on that by adding parameters and presets . A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work.","title":"Tutorial: Part 2"},{"location":"tutorials/tutorial_part2/#step_1_define_the_project_parameters","text":"You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters at the end of project.yaml . This is a YAML map which defines the default value. project.yaml # ... parameters : user_prefix : '' #no prefix for prod In this case we defined a parameter user_prefix that we will use to name tables. This is useful when multiple users are testing a project as it allows the final table name to be different between collaborators. Now we can define the value of the parameter on each profile. We do this in the settings.yaml . settings.yaml profiles : dev : credentials : warehouse : dev_db parameters : user_prefix : up_ prod : credentials : warehouse : prod_db Note how we don't redefine the parameter in our prod profile as the default value is more appropriate.","title":"Step 1: Define The Project parameters"},{"location":"tutorials/tutorial_part2/#step_2_making_tasks_dynamic_with_parameters","text":"Now that our parameters are setup, we can use those to make our tasks' code dynamic.","title":"Step 2: Making Tasks Dynamic With parameters"},{"location":"tutorials/tutorial_part2/#in_python_tasks","text":"For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils.py to see how we use the user_prefix parameter to change the table names. python/load_data.py # ... def run ( self ): user_prefix = self . parameters [ 'user_prefix' ] # ... q_create = get_create_table ( log_type , user_prefix ) # ...","title":"In python Tasks"},{"location":"tutorials/tutorial_part2/#in_autosql_tasks","text":"The files in the sql folder are always interpreted as Jinja templates. This means that in order to access parameters all we have to do is enclose it in {{ }} Jinja blocks. For example, in order to reference the tables created by load_data the dim_arenas task can be changed like this: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l Now sayn run will transform the above into valid SQL creating compile/base/dim_arenas.sql with it. The file path following the rule compile/task_group_name/autosql_task_name.sql : compile/base/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM up_logs_arenas l SAYN provides a sayn compile command that works like sayn run except that it won't execute the code. What it does though, is generate the compiled files that SAYN would run with the sayn run command.","title":"In autosql Tasks"},{"location":"tutorials/tutorial_part2/#step_3_making_task_definitions_dynamic_with_parameters","text":"Now that our python task generates tables with the user_prefix in the name and our autosql tasks will select data from it. What we also need to do is change the table names our autosql tasks are generating. For that, let's take dim_arenas and modify it so that it generates a table called up_dim_arenas (or other user_prefix defined in settings.yaml ): tasks/base.yaml tasks : # ... dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data # ... Note the value of destination.table is now some Jinja code that will compile to the value of user_prefix followed by the name of the task.","title":"Step 3: Making Task Definitions Dynamic With parameters"},{"location":"tutorials/tutorial_part2/#step_4_using_presets_to_standardise_task_definitions","text":"Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets allow you to define common properties shared by several tasks. tasks/base.yaml presets : modelling : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data tasks : # ... dim_arenas : preset : modelling # ... Now the modelling preset has to dynamic properties: * table : defined like we did in the previous so that the create table contains the user_prefix in the name. * file_name : that uses the task name to point at the correct file in the sql folder. In addition, modelling is defined so that tasks referencing it: * are autosql tasks. * Materialise as tables. * Have load_data as a parent task, so that models always run after our log generator. When a task references a preset, we're not restricted to the values defined in the preset. A task can override those values. Take f_rankings for example: tasks/base.yaml tasks : # ... f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Here we're overloading 2 properties: * materialisation which will make f_rankings a view rather than a table. * parents which will make f_ranking depend on f_fighter_results as well as load_data as defined in the preset.","title":"Step 4: Using presets To Standardise Task Definitions"},{"location":"tutorials/tutorial_part2/#running_our_new_project","text":"You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a dev.db database prefix all tables with up_ and read from up_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables","title":"Running Our New Project"},{"location":"tutorials/tutorial_part2/#what_next","text":"This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"What Next?"}]}