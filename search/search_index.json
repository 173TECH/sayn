{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SAYN SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected. Use Cases SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Stitch or Fivetran with customised extraction processes. Date modelling: transform raw data in your data warehouse. Date science: integrate and execute data science models. Key Features SAYN has the following key features: SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Create a Direct Acyclic Graph by simply declaring task dependencies. Multiple databases supported. and much more... See the Documentation . Design Principles SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start $ pip install git+https://github.com/173TECH/sayn.git $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the SAYN Tutorial which will give you a good overview of SAYN's true power! Support If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#sayn","text":"SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected.","title":"SAYN"},{"location":"#use-cases","text":"SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Stitch or Fivetran with customised extraction processes. Date modelling: transform raw data in your data warehouse. Date science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key-features","text":"SAYN has the following key features: SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Create a Direct Acyclic Graph by simply declaring task dependencies. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design-principles","text":"SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick-start","text":"$ pip install git+https://github.com/173TECH/sayn.git $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the SAYN Tutorial which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"commands/","text":"Commands About SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands. Commands Detail init : initialises a SAYN project in the current working directory. run : runs all tasks if no parameter is specified. This command has the following flags. -t : run specific task(s). It should be used as sayn run -t task_1 . You can run the task and all its parents by prefixing the task name with + such as sayn run -t +task_1 . The same applies for all children with sayn run -t task_1+ . -m : run specific model. This can be used when having multiple model files: sayn run -m marketing . -p : select profile to use for run -f : do a full load. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level. compile : compiles the SAYN code. The code is available in a compile folder at the project's root. same parameters apply than the run command. dag-image : generates a visualisation of the DAG. This requires graphviz (both the software and the Python package).","title":"Commands"},{"location":"commands/#commands","text":"","title":"Commands"},{"location":"commands/#about","text":"SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands.","title":"About"},{"location":"commands/#commands-detail","text":"init : initialises a SAYN project in the current working directory. run : runs all tasks if no parameter is specified. This command has the following flags. -t : run specific task(s). It should be used as sayn run -t task_1 . You can run the task and all its parents by prefixing the task name with + such as sayn run -t +task_1 . The same applies for all children with sayn run -t task_1+ . -m : run specific model. This can be used when having multiple model files: sayn run -m marketing . -p : select profile to use for run -f : do a full load. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level. compile : compiles the SAYN code. The code is available in a compile folder at the project's root. same parameters apply than the run command. dag-image : generates a visualisation of the DAG. This requires graphviz (both the software and the Python package).","title":"Commands Detail"},{"location":"databases/","text":"Databases About SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Snowflake Redshift PostgreSQL MySQL SQLite Usage In order to connect to databases, the connection credentials need to be added into the credentials sections of the settings.yaml file. Please see below examples for how to add credentials of each database type: Snowfake credentials: snowflake-conn: type: snowflake connect_args: account: [account] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database] schema: [schema] warehouse: [warehouse] role: [role] Redshift credentials: redshift-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name] PostgreSQL credentials: postgresql-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name] MySQL credentials: mysql-conn: type: mysql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database] SQLite credentials: sqlite-conn: type: sqlite database: [path_to_database]","title":"Databases"},{"location":"databases/#databases","text":"","title":"Databases"},{"location":"databases/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Snowflake Redshift PostgreSQL MySQL SQLite","title":"About"},{"location":"databases/#usage","text":"In order to connect to databases, the connection credentials need to be added into the credentials sections of the settings.yaml file. Please see below examples for how to add credentials of each database type:","title":"Usage"},{"location":"databases/#snowfake","text":"credentials: snowflake-conn: type: snowflake connect_args: account: [account] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database] schema: [schema] warehouse: [warehouse] role: [role]","title":"Snowfake"},{"location":"databases/#redshift","text":"credentials: redshift-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name]","title":"Redshift"},{"location":"databases/#postgresql","text":"credentials: postgresql-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name]","title":"PostgreSQL"},{"location":"databases/#mysql","text":"credentials: mysql-conn: type: mysql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database]","title":"MySQL"},{"location":"databases/#sqlite","text":"credentials: sqlite-conn: type: sqlite database: [path_to_database]","title":"SQLite"},{"location":"getting_started/","text":"Getting Started With SAYN Installing SAYN In order to install SAYN, please run pip install git+https://github.com/173TECH/sayn.git . Creating your first project In order to initialise your SAYN project, open your command line and then navigate to the directory in which you want to create your SAYN project. Then, run the command sayn init [project-name] where [project-name] is a name of your choice. This will create the SAYN repository which will have the following structure: project-name # The name you chose. logs/ # The folder where SAYN logs are stored. python/ # The folder where Python tasks modules should be stored. sql/ # The folder where SQL tasks queries should be stored. models.yaml # The backbone of a SAYN project enabling to define and orchestrate tasks. settings.yaml # The settings of an individual SAYN user. .gitignore # To ignore settings.yaml and other files. readme.md # Some instructions to get going. Have a look at the folder structure, then you can proceed with your first SAYN run! Your first SAYN run The SAYN folder initialised with sayn init comes with an example project which you can use to get familiar with SAYN. In order to run the project, go to the root of the project folder (where models.yaml is) and run the command sayn run . You should see the detail of tasks being executed. That's it, you have run your first SAYN project :) You can now continue with the Tutorial which goes through the project example you got with sayn init and introduces some core SAYN concepts. Enjoy!","title":"Getting Started"},{"location":"getting_started/#getting-started-with-sayn","text":"","title":"Getting Started With SAYN"},{"location":"getting_started/#installing-sayn","text":"In order to install SAYN, please run pip install git+https://github.com/173TECH/sayn.git .","title":"Installing SAYN"},{"location":"getting_started/#creating-your-first-project","text":"In order to initialise your SAYN project, open your command line and then navigate to the directory in which you want to create your SAYN project. Then, run the command sayn init [project-name] where [project-name] is a name of your choice. This will create the SAYN repository which will have the following structure: project-name # The name you chose. logs/ # The folder where SAYN logs are stored. python/ # The folder where Python tasks modules should be stored. sql/ # The folder where SQL tasks queries should be stored. models.yaml # The backbone of a SAYN project enabling to define and orchestrate tasks. settings.yaml # The settings of an individual SAYN user. .gitignore # To ignore settings.yaml and other files. readme.md # Some instructions to get going. Have a look at the folder structure, then you can proceed with your first SAYN run!","title":"Creating your first project"},{"location":"getting_started/#your-first-sayn-run","text":"The SAYN folder initialised with sayn init comes with an example project which you can use to get familiar with SAYN. In order to run the project, go to the root of the project folder (where models.yaml is) and run the command sayn run . You should see the detail of tasks being executed. That's it, you have run your first SAYN project :) You can now continue with the Tutorial which goes through the project example you got with sayn init and introduces some core SAYN concepts. Enjoy!","title":"Your first SAYN run"},{"location":"groups/","text":"Groups About groups are defined in models.yaml and enable to define some shared parameters across tasks. If a task in models.yaml refers to a group it will inherit all the properties of the group. groups are a great way to avoid repetition. Usage A task refers to a group with the group attribute, such as follows: task_group: #other task properties group: my_group For example, for our data modelling, we could define a modelling group as follows: models.yaml #your models settings groups: modelling: type: autosql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: {{task.name}} tasks: #some tasks model_1: file_name: model_1.sql group: modelling model_2: file_name: model_2.sql group: modelling #more tasks belonging to the modelling group This will imply that the tasks model_1 and model_2 will be autosql tasks, materialise as tables and share all the other attributes of the modelling group. Note: in the modelling group, the table parameter is set as {{task.name}} . This is a dynamic parameter which is discussed in the Parameters section.","title":"Groups"},{"location":"groups/#groups","text":"","title":"Groups"},{"location":"groups/#about","text":"groups are defined in models.yaml and enable to define some shared parameters across tasks. If a task in models.yaml refers to a group it will inherit all the properties of the group. groups are a great way to avoid repetition.","title":"About"},{"location":"groups/#usage","text":"A task refers to a group with the group attribute, such as follows: task_group: #other task properties group: my_group For example, for our data modelling, we could define a modelling group as follows: models.yaml #your models settings groups: modelling: type: autosql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: {{task.name}} tasks: #some tasks model_1: file_name: model_1.sql group: modelling model_2: file_name: model_2.sql group: modelling #more tasks belonging to the modelling group This will imply that the tasks model_1 and model_2 will be autosql tasks, materialise as tables and share all the other attributes of the modelling group. Note: in the modelling group, the table parameter is set as {{task.name}} . This is a dynamic parameter which is discussed in the Parameters section.","title":"Usage"},{"location":"models/","text":"Models: models.yaml Role models.yaml is the backbone of a SAYN project where tasks and their parentage are defined. This is what SAYN will use to create and run the DAG. This file is shared across all users of the SAYN project. Content models.yaml sayn_project_name: [company]_sayn_etl default_db: warehouse required_credentials: - warehouse parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tasks: task_1: file_name: task_1.sql type: sql task_2: file_name: task_2.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' task_3: file_name: task_3.sql group: modelling task_4: file_name: task_4.sql group: modelling materialisation: view parents: - task_3 task_5: module: task_5 type: python class: TaskFive parents: - task_1 - task_4 The models.yaml has the following mandatory parameters defined: sayn_project_name : the name of your SAYN project. default_db : this is the database that will be used by tasks writing to the analytics warehouse (e.g. sql , autosql tasks). The default_db that is set should be part of the required_credentials . required_credentials : the list of database and API credentials which are required to run the SAYN project. The credential details are then specified in settings.yaml . tasks : the list of tasks and their definitions. Those will be used by SAYN to create and run the DAG. The models.yaml has the following optional parameters defined: parameters : parameters are used to customise tasks. SQL queries are effectively Jinja templates so those parameters can be used in queries. They can also be accessed in Python tasks via the task object attributes. parameters can be overridden with settings.yaml which means that user can set specific parameters for testing or separate development environments. groups : groups enable to define some attributes which will be used by all tasks referring to the group. models : models enable to add additional models to your DAG. Those additional model files should be YAML files stored in a models folder at the root level of the project. For example, all marketing tasks could be in models/marketing.yaml .","title":"Models"},{"location":"models/#models-modelsyaml","text":"","title":"Models: models.yaml"},{"location":"models/#role","text":"models.yaml is the backbone of a SAYN project where tasks and their parentage are defined. This is what SAYN will use to create and run the DAG. This file is shared across all users of the SAYN project.","title":"Role"},{"location":"models/#content","text":"models.yaml sayn_project_name: [company]_sayn_etl default_db: warehouse required_credentials: - warehouse parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tasks: task_1: file_name: task_1.sql type: sql task_2: file_name: task_2.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' task_3: file_name: task_3.sql group: modelling task_4: file_name: task_4.sql group: modelling materialisation: view parents: - task_3 task_5: module: task_5 type: python class: TaskFive parents: - task_1 - task_4 The models.yaml has the following mandatory parameters defined: sayn_project_name : the name of your SAYN project. default_db : this is the database that will be used by tasks writing to the analytics warehouse (e.g. sql , autosql tasks). The default_db that is set should be part of the required_credentials . required_credentials : the list of database and API credentials which are required to run the SAYN project. The credential details are then specified in settings.yaml . tasks : the list of tasks and their definitions. Those will be used by SAYN to create and run the DAG. The models.yaml has the following optional parameters defined: parameters : parameters are used to customise tasks. SQL queries are effectively Jinja templates so those parameters can be used in queries. They can also be accessed in Python tasks via the task object attributes. parameters can be overridden with settings.yaml which means that user can set specific parameters for testing or separate development environments. groups : groups enable to define some attributes which will be used by all tasks referring to the group. models : models enable to add additional models to your DAG. Those additional model files should be YAML files stored in a models folder at the root level of the project. For example, all marketing tasks could be in models/marketing.yaml .","title":"Content"},{"location":"parameters/","text":"Parameters About parameters are a really powerful tool. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to differentiation between development production environments. Under the hood, SAYN uses Jinja templating for both the SQL queries and the models.yaml file. parameters can also be accessed in python tasks. Defining Parameters parameters are defined at three levels: models.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from models.yaml . tasks : each task can set its own parameters with the parameters attribute. Accessing Parameters For the below section, we consider a project with the following setup: models.yaml # ... parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models # ... settings.yaml # ... default_profile: dev profiles: dev: # ... parameters: table_prefix: 'songoku_' schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: # ... parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models In tasks Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: {{schema_staging}} schema: {{schema_models}} table: {{table_prefix}}{{task.name}} Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: analytics_adhoc schema: analytics_adhoc table: songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: task_autosql_param In groups parameters can be accessed in groups in the same way than they are accessed in tasks . For example, you could have the following modelling group definition: groups: modelling: type: autosql materialisation: table to: staging_schema: {{schema_staging}} schema: {{schema_models}} table: {{table_prefix}}{{task.name}} The interpretation of this group will work as in the above section, using parameters from the relevant profile at execution time. In SQL Queries For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter-name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt.* FROM {{schema_models}}.{{table_prefix}}my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt.* FROM analytics_adhoc.songoku_my_table AS mt In Python Tasks parameters can be accessed in python tasks via the SAYN API. The parameters are stored on sayn_config attribute of the Task object and therefore be accesses with self.sayn_config.parameters . For example, you could have the following python task code to access parameters: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False sayn_params = self.sayn_config.parameters #code you want to run if err: return self.failed() else: return self.finished() In this code, the sayn_params variable will contain a dictionary with all parameters from both the project and the task itself.","title":"Parameters"},{"location":"parameters/#parameters","text":"","title":"Parameters"},{"location":"parameters/#about","text":"parameters are a really powerful tool. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to differentiation between development production environments. Under the hood, SAYN uses Jinja templating for both the SQL queries and the models.yaml file. parameters can also be accessed in python tasks.","title":"About"},{"location":"parameters/#defining-parameters","text":"parameters are defined at three levels: models.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from models.yaml . tasks : each task can set its own parameters with the parameters attribute.","title":"Defining Parameters"},{"location":"parameters/#accessing-parameters","text":"For the below section, we consider a project with the following setup: models.yaml # ... parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models # ... settings.yaml # ... default_profile: dev profiles: dev: # ... parameters: table_prefix: 'songoku_' schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: # ... parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models","title":"Accessing Parameters"},{"location":"parameters/#in-tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: {{schema_staging}} schema: {{schema_models}} table: {{table_prefix}}{{task.name}} Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: analytics_adhoc schema: analytics_adhoc table: songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: task_autosql_param","title":"In tasks"},{"location":"parameters/#in-groups","text":"parameters can be accessed in groups in the same way than they are accessed in tasks . For example, you could have the following modelling group definition: groups: modelling: type: autosql materialisation: table to: staging_schema: {{schema_staging}} schema: {{schema_models}} table: {{table_prefix}}{{task.name}} The interpretation of this group will work as in the above section, using parameters from the relevant profile at execution time.","title":"In groups"},{"location":"parameters/#in-sql-queries","text":"For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter-name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt.* FROM {{schema_models}}.{{table_prefix}}my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt.* FROM analytics_adhoc.songoku_my_table AS mt","title":"In SQL Queries"},{"location":"parameters/#in-python-tasks","text":"parameters can be accessed in python tasks via the SAYN API. The parameters are stored on sayn_config attribute of the Task object and therefore be accesses with self.sayn_config.parameters . For example, you could have the following python task code to access parameters: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False sayn_params = self.sayn_config.parameters #code you want to run if err: return self.failed() else: return self.finished() In this code, the sayn_params variable will contain a dictionary with all parameters from both the project and the task itself.","title":"In Python Tasks"},{"location":"project_structure/","text":"SAYN Project Structure Initial Project Structure A usual SAYN project structure will be as follows: project-name # The name you chose. compile/ # The folder where compiled SQL queries are stored. logs/ # The folder where SAYN logs are stored. models/ # The folder where additional models can be added. python/ # The folder where Python tasks modules should be stored. sql/ # The folder where SQL tasks queries should be stored. models.yaml # The backbone of a SAYN project enabling to define and orchestrate tasks. settings.yaml # The settings of an individual SAYN user. .gitignore # To ignore settings.yaml and other files. The two core files of a SAYN project are settings.yaml (unique settings for each individual) and models.yaml where the tasks of the project are defined (this is shared by all users on the project). Please note that sayn init [project-name] initialises your SAYN project with an example and and a sample SQLite database. This is only so you can have an overview of SAYN and go through the Tutorial and can be deleted once you start working on your own project. We will now cover the detail of both files, what they contain and what they are used for.","title":"Project Structure"},{"location":"project_structure/#sayn-project-structure","text":"","title":"SAYN Project Structure"},{"location":"project_structure/#initial-project-structure","text":"A usual SAYN project structure will be as follows: project-name # The name you chose. compile/ # The folder where compiled SQL queries are stored. logs/ # The folder where SAYN logs are stored. models/ # The folder where additional models can be added. python/ # The folder where Python tasks modules should be stored. sql/ # The folder where SQL tasks queries should be stored. models.yaml # The backbone of a SAYN project enabling to define and orchestrate tasks. settings.yaml # The settings of an individual SAYN user. .gitignore # To ignore settings.yaml and other files. The two core files of a SAYN project are settings.yaml (unique settings for each individual) and models.yaml where the tasks of the project are defined (this is shared by all users on the project). Please note that sayn init [project-name] initialises your SAYN project with an example and and a sample SQLite database. This is only so you can have an overview of SAYN and go through the Tutorial and can be deleted once you start working on your own project. We will now cover the detail of both files, what they contain and what they are used for.","title":"Initial Project Structure"},{"location":"settings/","text":"Settings: settings.yaml Role The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . It enables the SAYN user to set profiles for testing / prod and overwrite the default project parameters. The profile used can be controlled by the user when running SAYN. Content Overview settings.yaml default_profile: dev profiles: dev: credentials: warehouse: snowflake-songoku parameters: table_prefix: songoku_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: credentials: warehouse: snowflake-prod # no need for prod parameters as those are read from models.yaml credentials: snowflake-songoku: type: snowflake account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] snowflake-prod: type: snowflake account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] All parameters in settings.yaml are mandatory except default_profile which is mandatory only if two or more profiles are defined. Please see below details on the parameters: default_profile : the profile that will be used by default when running SAYN. The profile specified needs to be defined in the profiles . profiles : the list of profiles with detail about which credential and parameter they should use. credentials : the details of credentials for the SAYN project. Here, the user has details for the Snowflake warehouse connection. In the case of this settings file: The warehouse credential for the test profile should be snowflake-tu (the individual test user's credentials) and it should be snowflake-prod for the prod profile. The test profile would use analytics_adhoc for the parameter schema_models . The prod profile would use analytics_models . This enables to easily switch between development and production environments which is best practice. More details about this can be found in the Parameters section. API Credentials The credentials section of the settings.yaml file is used to store both databases and API credentials. Database credentials are covered above. For API credentials, those can be added using the type api and any parameter required for the API connection. Please see an example below: # ... credentials: # ... credential_name: type: api api_key: 'api_key' Those API credentials can then be accessed in python tasks through the task object.","title":"Settings"},{"location":"settings/#settings-settingsyaml","text":"","title":"Settings: settings.yaml"},{"location":"settings/#role","text":"The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . It enables the SAYN user to set profiles for testing / prod and overwrite the default project parameters. The profile used can be controlled by the user when running SAYN.","title":"Role"},{"location":"settings/#content","text":"","title":"Content"},{"location":"settings/#overview","text":"settings.yaml default_profile: dev profiles: dev: credentials: warehouse: snowflake-songoku parameters: table_prefix: songoku_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: credentials: warehouse: snowflake-prod # no need for prod parameters as those are read from models.yaml credentials: snowflake-songoku: type: snowflake account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] snowflake-prod: type: snowflake account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] All parameters in settings.yaml are mandatory except default_profile which is mandatory only if two or more profiles are defined. Please see below details on the parameters: default_profile : the profile that will be used by default when running SAYN. The profile specified needs to be defined in the profiles . profiles : the list of profiles with detail about which credential and parameter they should use. credentials : the details of credentials for the SAYN project. Here, the user has details for the Snowflake warehouse connection. In the case of this settings file: The warehouse credential for the test profile should be snowflake-tu (the individual test user's credentials) and it should be snowflake-prod for the prod profile. The test profile would use analytics_adhoc for the parameter schema_models . The prod profile would use analytics_models . This enables to easily switch between development and production environments which is best practice. More details about this can be found in the Parameters section.","title":"Overview"},{"location":"settings/#api-credentials","text":"The credentials section of the settings.yaml file is used to store both databases and API credentials. Database credentials are covered above. For API credentials, those can be added using the type api and any parameter required for the API connection. Please see an example below: # ... credentials: # ... credential_name: type: api api_key: 'api_key' Those API credentials can then be accessed in python tasks through the task object.","title":"API Credentials"},{"location":"tutorial/","text":"Tutorial Your first SAYN project Make sure you have followed the steps from the Getting Started section in order to install the SAYN Python package and create your first project folder. The project you installed with sayn init contains and example SAYN project. We will use this project through this tutorial to get you going with SAYN and explain core concepts. If you have not done it yet, join our public Slack channel in order to get help with SAYN whenever needed (the developers and maintainers of SAYN are in this channel). Project structure overview A SAYN project is composed of two key files: models.yaml : this is the backbone of a SAYN project where tasks and their parentage are defined. This is what SAYN will use to create and run the DAG. This file is shared across all users of the SAYN project. settings.yaml : this file is used for individual settings to run the SAYN project. This file is unique to each SAYN user on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . It enables SAYN user to set profiles for testing and overwrite the default project parameters. Please see below example files for the above: models.yaml sayn_project_name: sayn_chinook default_db: warehouse required_credentials: - warehouse parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tasks: track_details_sql: file_name: track_details_sql.sql type: sql track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tracks_per_album: file_name: tracks_per_album.sql group: modelling tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered The models.yaml has the following parameters defined, which are all mandatory: sayn_project_name : the name of your SAYN project. default_db : this is the database that will be used by tasks writing to the analytics warehouse (e.g. sql , autosql tasks). The default_db that is set should be part of the required_credentials . required_credentials : the list of database and API credentials which are required to run the SAYN project. The credential details are then specified in settings.yaml . parameters : parameters are used to customise tasks. SQL queries are effectively Jinja templates so those parameters can be used in queries. They can also be access in Python tasks via the task object attributes. parameters can be overridden with settings.yaml which means that user can set specific parameters for testing or separate development environments. groups : groups enable to define some attributes which will be used by all tasks referring to the group. tasks : the list of tasks and their definitions. Those will be used by SAYN to create and run the DAG. Note: SAYN has a range of tasks that can be used to implement and automate data processes. This tutorial will only cover the sql , autosql and python tasks but more task types are available. For more information about tasks please see the Tasks section of the documentation. settings.yaml default_profile: test profiles: test: credentials: warehouse: chinook parameters: table_prefix: tu_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc credentials: chinook: type: sqlite database: chinook.db The settings.yaml has the following parameters defined, which are all mandatory: default_profile : the profile that will be used by default when running SAYN. The profile specified needs to be defined in the profiles . profiles : the list of profiles with credential and parameter details. Each credential specified should be defined in the credentials . credentials : the list of credentials for the SAYN project. The SAYN project also has the following folders: logs : the folder in which SAYN logs are stored by default. sql : the folder in which SQL task queries should be stored python : the folder in which Python task modules should be stored You should now have a good understanding of the core files and folders in the SAYN project. We will now go over the SAYN example project installed by sayn init and do our first SAYN run. Implementing your project We will now go through the example project and process our first run with SAYN (the project uses a SQLite database). Although the example project has everything defined for you and should run already, we will go through the steps as if we were implementing the project. Step 1: Define your individual settings with settings.yaml We will define our individual settings for the SAYN project. Those will go into the settings.yaml file which is unique to each user across the project. Step 1.1: Set the profile(s) First we will define our profiles. Our profiles define credentials (used to connect to databases and APIs) and parameters (used to customise tasks) for each profile. We define the profiles as follows: profiles: test: credentials: warehouse: chinook parameters: table_prefix: tu_ schema_logs: main schema_staging: main schema_models: main Tip: Although not done in the example, it is usually a good idea to define a test (for development) and a prod (for production) profile and use the test profile as a default. This enables you to switch easily between profile whenever necessary. Note: the schemas are all defined as main. This is because SQLite does not have schemas and we refer to the main database. Step 1.2: Set the default profile Now that we have defined profiles, we will set the default profile we want to use when running SAYN. We will default to using the test profile which is the only profile defined in the example project. At the top of settings.yaml , we add the following: default_profile: test Step 1.3: Set the credentials The last part is of settings.yaml is setting the credentials which will be used by the profiles. We add the following at the end of the file. credentials: chinook: type: sqlite database: chinook.db Note: different databases require different connection parameters. For specific detail regarding connecting to different databases please see the Databases section of the documentation. This is it for settings.yaml , the SAYN project will now know which profile, parameters and credentials to use at run time. Step 2: Define the project tasks with models.yaml We will now define the backbone of the SAYN project. This will be done with the models.yaml file which is shared by all users across the project. Step 2.1: Set the project name At the top of the file, we will add the following: sayn_project_name: sayn_chinook Note: it is good practice to give your project a name that is meaningful, such as [company]_sayn_etl. Step 2.2: Set the default database We will now set the default database that will be used by SAYN when running. This database will be used when running SQL tasks and can be accessed in Python tasks via the task's attributes (more on this below). Add the following to models.yaml . default_db: warehouse Step 2.3: Set the required credentials Those are the credentials which are required by the SAYN project to run. Any credential listed here should be defined in the credentials section of the settings.yaml file. Our example project only requires the SQLite database credentials, so we add the following to our models.yaml file: required_credentials: - warehouse Step 2.4: Set the project's default parameters As mentioned before, the parameters are used to customise SAYN tasks. Those parameters will be used by default when running SAYN, unless the parameters are overridden in the profile used at run time. Add the following to models.yaml : parameters: table_prefix: '' schema_logs: main #this is specific to SQLite (this should be the database name which is main) schema_staging: main #this is specific to SQLite (this should be the database name which is main) schema_models: main #this is specific to SQLite (this should be the database name which is main) We have defined a table_prefix which is useful for development, as each analyst can use their initials to prefix tables and views. This prevents conflicts in the event users are doing some testing that requires using the same tables and views. We also have defined schema_logs , schema_staging and schema_models . Although this is not really useful in our example as SQLite does not support schemas, this becomes really useful on databases used for analytics (i.e. Redshift, Snowflake, etc.) as this enables test profiles to use test schemas and production profiles to use the production schemas. Step 2.5: Set the group(s) Groups can be used in order to define attributes which can be used by multiple tasks. Tasks, which we will define in the next step, can refer to groups. When doing so, the task will inherit the attributes of the group it refers to. We will define one group, for modelling tasks. Add the following to models.yaml : groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' The attributes defined in the modelling group will become clearer when we cover tasks in the next section. In this instance, the modelling group mentions that all tasks of this group are of the type autosql (defined in the next section) and will all use the other defined parameters. Step 2.6: Set the tasks We will now define the tasks that will compose the SAYN project. Tasks all have a type (we will only cover the sql , autosql and python types in this tutorial). Tasks then define specific parameters based on the task type and can specify parents to create relationships between tasks. Let's create our tasks, add the following code to the end of models.yaml . tasks: track_details_sql: file_name: track_details_sql.sql type: sql track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tracks_per_album: file_name: tracks_per_album.sql group: modelling tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered Let's cover each task one by one. The first task, track_details_sql is a sql task: track_details_sql: file_name: track_details_sql.sql type: sql sql tasks enable you to run a SQL query. The query should be saved at the location indicated by the file_name attribute within the sql folder. Here is the SQL query referred by the track_details_sql task: sql/track_details_sql.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ DROP TABLE IF EXISTS {{schema_models}}.{{table_prefix}}track_details_sql ; CREATE TABLE {{schema_models}}.{{table_prefix}}track_details_sql AS SELECT t.trackid , t.name , al.title album_name , ar.name artist_name FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid INNER JOIN {{schema_logs}}.artists ar ON al.artistid = ar.artistid ; You can see here that this SQL query is referring to our parameters. SAYN uses Jinja templating which enables you to use your parameters when writing queries. For example, if using the test profile, the table_prefix tu_ would be used. Otherwise the empty prefix '' would be used as defined in the default parameters in models.yaml . You could obviously hardcode everything, but as we mentioned earlier, this is not a good idea as you ideally want to separate development and production environments. The second task is track_details_autosql which is an autosql task: track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' autosql tasks enable you to simply write a SELECT statement and SAYN will automatically create the table or view for you. See the Tasks section of the documentation for more information on this task type. autosql only requires you to define the materialisation type and the process parameters: staging_schema (for the temporary table created in the process), schema (the destination schema where the object will be created), table_name (the name of the object created). In addition, you can see the attributes of the track_details_autosql task refer to our parameters and also the task.name which is track_details_autosql . For example, if you had a prod profile with schema_models analytics_models and a test profile with schema_models analytics_adhoc , then this task would create the table in the analytics_models schema when using the prod profile and it would write it in the analytics_adhoc schema when using the test profile. Here is the SQL query referred by the track_details_autosql task. As you can observe, this query now does not have the DROP and CREATE statements. It only has the SELECT statement, everything is handled for you by SAYN. sql/track_details_autosql.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT t.trackid , t.name , al.title album_name , ar.name artist_name FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid INNER JOIN {{schema_logs}}.artists ar ON al.artistid = ar.artistid The third task is tracks_per_album which is using a group for definition: tracks_per_album: file_name: tracks_per_album.sql group: modelling Because this task refers to the modelling group, it inherits all the settings from this group, which are: groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' Therefore, the tracks_per_album task is an autosql task, materialises as a table and has the other attributes of the modelling group. When running this task with the test profile, it will therefore use main as the staging_schema and schema , and the table will be name tu_tracks_per_album . Here is the SQL query referred by the track_per_album task. sql/tracks_per_album.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT al.title album_name , COUNT(DISTINCT t.trackid) n_tracks FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid GROUP BY 1 The fourth task is tracks_per_album_ordered : tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album As you can see, it inherits from the modelling group. However, it overrides the materialisation to be a view. Finally it sets parents to be the tracks_per_album task. This means this task will always be executed after the tracks_per_album task is successfully finished. If the tracks_per_album fails, the tracks_per_album_ordered task will be skipped. Note: a task can have as many parents as desired. Here is the SQL query referred by the track_per_album_ordered task. sql/tracks_per_album_ordered.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT tpa.* FROM {{schema_models}}.{{table_prefix}}tracks_per_album tpa --here we prepend the table name with {{table_prefix}} which enables to separate when testing. If ran from prod, there is no prefix. If a test user has prefix specified, then the prefix will be added. ORDER BY 2 DESC The fifth and final task is print_top_10_albums_by_tracks and is a python task: print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered A python task enables you to use Python for your task. This means you could do anything, from extracting data from an API to running a data science model. python tasks require a module (this is the name of python file in the python folder. SAYN automatically looks there fore Python tasks) and a class (this should be the name of the class in the Python module.) Here is the Python codee referred by the print_top_10_albums_by_tracks task. python/print_top_10_albums_by_tracks.py #IMPORTANT: for python tasks to be able to execute, you neeed to have an __init__.py file into the python folder so it is treated as a package #here we define a python task #python tasks inherit from the sayn PythonTask from sayn import PythonTask #a python task needs to implement two functions #setup() wich operates necessary setup and returns self.ready() to indicate the task is ready to be ran #run() which executes the task and returns self.finished() to indicate the task has finished successfully class TopAlbumsPrinter(PythonTask): #here we are not doing anything for setup, just displaying the usage of self.failed() #in order to inform sayn that the task has failed, you would return self.failed() #note that self.failed() can also be used for run() #please note that setup() needs to follow the method's signature. Therefore it needs to be set as setup(self). def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() #here we define the code that will be executed at run time #please note that run() needs to follow the method's signature. Therefore it needs to be set as run(self). def run(self): #we can access the project parameters via the sayn_config attribute sayn_params = self.sayn_config.parameters #we use the config parameters to make the query dynamic #the query will therefore use parameters of the profile used at run time q = ''' SELECT tpao.* FROM {schema_models}.{table_prefix}tracks_per_album_ordered tpao LIMIT 10 ; '''.format( schema_models=sayn_params['schema_models'], table_prefix=sayn_params['table_prefix'] ) #the python task has the project's default_db connection object as an attribute. #this attribute has an number of method including select() to run a query and return results. #please see the documentation for more details on the API r = self.default_db.select(q) print('Printing top 10 albums by number of tracks:') for i in range(10): print('#{rank}: {album}, {n} tracks.'.format(rank=i+1, album=r[i]['album_name'], n=r[i]['n_tracks'])) return self.finished() Please note that for the Python tasks to run properly, you need to have an __init__.py file into the python folder so it is considered as a package . The TopAlbumsPrinter Python class implements two methods: setup and run . The setup method will be run when the DAG sets up all the tasks, and the run method will be run at execution time. Both methods need to have the signatures of the SAYN PythonTask class. As a result, you should only pass self as a parameter to those methods. A few things to note regarding python tasks: If you want to have the task fail (e.g. to control for errors), return self.failed() in the methods. As you can see in the run method, we are accessing a few useful attributes of the task. self.sayn_config.parameters contains the parameters used. self.default_db contains the default database. self.debault_db.select() effectively runs a select query on the default database. For more details on the python tasks, please refer to the Tasks documentation. This it, we now have set all our tasks and our project is ready to be ran :) Running your project Now that the settings.yaml have our individual settings and that models.yaml have the tasks defined, we can run the sayn project! Run the following command in order to run the whole project (make sure you are at the root level of your SAYN project directory): sayn run . sayn run will run the whole project using the profile set in default_profile . A SAYN run is composed of setting up the DAG (when all tasks are setup) and running the DAG (when the DAG is executed). You should see the detail of the process being logged to your command line (those logs are also saved in the log folder). Tip: after running sayn run , you will also see a compile folder appear. This is where all SQL queries will be compiled by SAYN so you can check there the output after the templates are filled. The sayn run command has a few option parameters: - -t in order to run a specific task. For example sayn run -t track_details_sql would only run the track_details_sql task. - -d can be used in order to print debug logs to the command line. Tip: you can run a task and all its parents or children by prefixing or suffixing the task name after the -t flag with + . For example, running sayn run -t +print_top_10_albums_by_tracks would run the print_top_10_albums_by_tracks and all its parents in their logical order. What Next? This is it, you have now completed the SAYN tutorial, congratulations! You now know how to implement and run a SAYN project and are able to use SAYN to increase your analytics workflow efficiencies. You should now have a look into the further details of the documentation, as SAYN has much more to offer. In specific, you can have a look at the following sections: S1 S2 Enjoy SAYN!","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"tutorial/#your-first-sayn-project","text":"Make sure you have followed the steps from the Getting Started section in order to install the SAYN Python package and create your first project folder. The project you installed with sayn init contains and example SAYN project. We will use this project through this tutorial to get you going with SAYN and explain core concepts. If you have not done it yet, join our public Slack channel in order to get help with SAYN whenever needed (the developers and maintainers of SAYN are in this channel).","title":"Your first SAYN project"},{"location":"tutorial/#project-structure-overview","text":"A SAYN project is composed of two key files: models.yaml : this is the backbone of a SAYN project where tasks and their parentage are defined. This is what SAYN will use to create and run the DAG. This file is shared across all users of the SAYN project. settings.yaml : this file is used for individual settings to run the SAYN project. This file is unique to each SAYN user on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . It enables SAYN user to set profiles for testing and overwrite the default project parameters. Please see below example files for the above: models.yaml sayn_project_name: sayn_chinook default_db: warehouse required_credentials: - warehouse parameters: table_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tasks: track_details_sql: file_name: track_details_sql.sql type: sql track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tracks_per_album: file_name: tracks_per_album.sql group: modelling tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered The models.yaml has the following parameters defined, which are all mandatory: sayn_project_name : the name of your SAYN project. default_db : this is the database that will be used by tasks writing to the analytics warehouse (e.g. sql , autosql tasks). The default_db that is set should be part of the required_credentials . required_credentials : the list of database and API credentials which are required to run the SAYN project. The credential details are then specified in settings.yaml . parameters : parameters are used to customise tasks. SQL queries are effectively Jinja templates so those parameters can be used in queries. They can also be access in Python tasks via the task object attributes. parameters can be overridden with settings.yaml which means that user can set specific parameters for testing or separate development environments. groups : groups enable to define some attributes which will be used by all tasks referring to the group. tasks : the list of tasks and their definitions. Those will be used by SAYN to create and run the DAG. Note: SAYN has a range of tasks that can be used to implement and automate data processes. This tutorial will only cover the sql , autosql and python tasks but more task types are available. For more information about tasks please see the Tasks section of the documentation. settings.yaml default_profile: test profiles: test: credentials: warehouse: chinook parameters: table_prefix: tu_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc credentials: chinook: type: sqlite database: chinook.db The settings.yaml has the following parameters defined, which are all mandatory: default_profile : the profile that will be used by default when running SAYN. The profile specified needs to be defined in the profiles . profiles : the list of profiles with credential and parameter details. Each credential specified should be defined in the credentials . credentials : the list of credentials for the SAYN project. The SAYN project also has the following folders: logs : the folder in which SAYN logs are stored by default. sql : the folder in which SQL task queries should be stored python : the folder in which Python task modules should be stored You should now have a good understanding of the core files and folders in the SAYN project. We will now go over the SAYN example project installed by sayn init and do our first SAYN run.","title":"Project structure overview"},{"location":"tutorial/#implementing-your-project","text":"We will now go through the example project and process our first run with SAYN (the project uses a SQLite database). Although the example project has everything defined for you and should run already, we will go through the steps as if we were implementing the project.","title":"Implementing your project"},{"location":"tutorial/#step-1-define-your-individual-settings-with-settingsyaml","text":"We will define our individual settings for the SAYN project. Those will go into the settings.yaml file which is unique to each user across the project.","title":"Step 1: Define your individual settings with settings.yaml"},{"location":"tutorial/#step-11-set-the-profiles","text":"First we will define our profiles. Our profiles define credentials (used to connect to databases and APIs) and parameters (used to customise tasks) for each profile. We define the profiles as follows: profiles: test: credentials: warehouse: chinook parameters: table_prefix: tu_ schema_logs: main schema_staging: main schema_models: main Tip: Although not done in the example, it is usually a good idea to define a test (for development) and a prod (for production) profile and use the test profile as a default. This enables you to switch easily between profile whenever necessary. Note: the schemas are all defined as main. This is because SQLite does not have schemas and we refer to the main database.","title":"Step 1.1: Set the profile(s)"},{"location":"tutorial/#step-12-set-the-default-profile","text":"Now that we have defined profiles, we will set the default profile we want to use when running SAYN. We will default to using the test profile which is the only profile defined in the example project. At the top of settings.yaml , we add the following: default_profile: test","title":"Step 1.2: Set the default profile"},{"location":"tutorial/#step-13-set-the-credentials","text":"The last part is of settings.yaml is setting the credentials which will be used by the profiles. We add the following at the end of the file. credentials: chinook: type: sqlite database: chinook.db Note: different databases require different connection parameters. For specific detail regarding connecting to different databases please see the Databases section of the documentation. This is it for settings.yaml , the SAYN project will now know which profile, parameters and credentials to use at run time.","title":"Step 1.3: Set the credentials"},{"location":"tutorial/#step-2-define-the-project-tasks-with-modelsyaml","text":"We will now define the backbone of the SAYN project. This will be done with the models.yaml file which is shared by all users across the project.","title":"Step 2: Define the project tasks with models.yaml"},{"location":"tutorial/#step-21-set-the-project-name","text":"At the top of the file, we will add the following: sayn_project_name: sayn_chinook Note: it is good practice to give your project a name that is meaningful, such as [company]_sayn_etl.","title":"Step 2.1: Set the project name"},{"location":"tutorial/#step-22-set-the-default-database","text":"We will now set the default database that will be used by SAYN when running. This database will be used when running SQL tasks and can be accessed in Python tasks via the task's attributes (more on this below). Add the following to models.yaml . default_db: warehouse","title":"Step 2.2: Set the default database"},{"location":"tutorial/#step-23-set-the-required-credentials","text":"Those are the credentials which are required by the SAYN project to run. Any credential listed here should be defined in the credentials section of the settings.yaml file. Our example project only requires the SQLite database credentials, so we add the following to our models.yaml file: required_credentials: - warehouse","title":"Step 2.3: Set the required credentials"},{"location":"tutorial/#step-24-set-the-projects-default-parameters","text":"As mentioned before, the parameters are used to customise SAYN tasks. Those parameters will be used by default when running SAYN, unless the parameters are overridden in the profile used at run time. Add the following to models.yaml : parameters: table_prefix: '' schema_logs: main #this is specific to SQLite (this should be the database name which is main) schema_staging: main #this is specific to SQLite (this should be the database name which is main) schema_models: main #this is specific to SQLite (this should be the database name which is main) We have defined a table_prefix which is useful for development, as each analyst can use their initials to prefix tables and views. This prevents conflicts in the event users are doing some testing that requires using the same tables and views. We also have defined schema_logs , schema_staging and schema_models . Although this is not really useful in our example as SQLite does not support schemas, this becomes really useful on databases used for analytics (i.e. Redshift, Snowflake, etc.) as this enables test profiles to use test schemas and production profiles to use the production schemas.","title":"Step 2.4: Set the project's default parameters"},{"location":"tutorial/#step-25-set-the-groups","text":"Groups can be used in order to define attributes which can be used by multiple tasks. Tasks, which we will define in the next step, can refer to groups. When doing so, the task will inherit the attributes of the group it refers to. We will define one group, for modelling tasks. Add the following to models.yaml : groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' The attributes defined in the modelling group will become clearer when we cover tasks in the next section. In this instance, the modelling group mentions that all tasks of this group are of the type autosql (defined in the next section) and will all use the other defined parameters.","title":"Step 2.5: Set the group(s)"},{"location":"tutorial/#step-26-set-the-tasks","text":"We will now define the tasks that will compose the SAYN project. Tasks all have a type (we will only cover the sql , autosql and python types in this tutorial). Tasks then define specific parameters based on the task type and can specify parents to create relationships between tasks. Let's create our tasks, add the following code to the end of models.yaml . tasks: track_details_sql: file_name: track_details_sql.sql type: sql track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' tracks_per_album: file_name: tracks_per_album.sql group: modelling tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered Let's cover each task one by one. The first task, track_details_sql is a sql task: track_details_sql: file_name: track_details_sql.sql type: sql sql tasks enable you to run a SQL query. The query should be saved at the location indicated by the file_name attribute within the sql folder. Here is the SQL query referred by the track_details_sql task: sql/track_details_sql.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ DROP TABLE IF EXISTS {{schema_models}}.{{table_prefix}}track_details_sql ; CREATE TABLE {{schema_models}}.{{table_prefix}}track_details_sql AS SELECT t.trackid , t.name , al.title album_name , ar.name artist_name FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid INNER JOIN {{schema_logs}}.artists ar ON al.artistid = ar.artistid ; You can see here that this SQL query is referring to our parameters. SAYN uses Jinja templating which enables you to use your parameters when writing queries. For example, if using the test profile, the table_prefix tu_ would be used. Otherwise the empty prefix '' would be used as defined in the default parameters in models.yaml . You could obviously hardcode everything, but as we mentioned earlier, this is not a good idea as you ideally want to separate development and production environments. The second task is track_details_autosql which is an autosql task: track_details_autosql: file_name: track_details_autosql.sql type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' autosql tasks enable you to simply write a SELECT statement and SAYN will automatically create the table or view for you. See the Tasks section of the documentation for more information on this task type. autosql only requires you to define the materialisation type and the process parameters: staging_schema (for the temporary table created in the process), schema (the destination schema where the object will be created), table_name (the name of the object created). In addition, you can see the attributes of the track_details_autosql task refer to our parameters and also the task.name which is track_details_autosql . For example, if you had a prod profile with schema_models analytics_models and a test profile with schema_models analytics_adhoc , then this task would create the table in the analytics_models schema when using the prod profile and it would write it in the analytics_adhoc schema when using the test profile. Here is the SQL query referred by the track_details_autosql task. As you can observe, this query now does not have the DROP and CREATE statements. It only has the SELECT statement, everything is handled for you by SAYN. sql/track_details_autosql.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT t.trackid , t.name , al.title album_name , ar.name artist_name FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid INNER JOIN {{schema_logs}}.artists ar ON al.artistid = ar.artistid The third task is tracks_per_album which is using a group for definition: tracks_per_album: file_name: tracks_per_album.sql group: modelling Because this task refers to the modelling group, it inherits all the settings from this group, which are: groups: modelling: type: autosql materialisation: table to: staging_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{table_prefix}}{{task.name}}' Therefore, the tracks_per_album task is an autosql task, materialises as a table and has the other attributes of the modelling group. When running this task with the test profile, it will therefore use main as the staging_schema and schema , and the table will be name tu_tracks_per_album . Here is the SQL query referred by the track_per_album task. sql/tracks_per_album.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT al.title album_name , COUNT(DISTINCT t.trackid) n_tracks FROM {{schema_logs}}.tracks t INNER JOIN {{schema_logs}}.albums al ON t.albumid = al.albumid GROUP BY 1 The fourth task is tracks_per_album_ordered : tracks_per_album_ordered: file_name: tracks_per_album_ordered.sql group: modelling materialisation: view parents: - tracks_per_album As you can see, it inherits from the modelling group. However, it overrides the materialisation to be a view. Finally it sets parents to be the tracks_per_album task. This means this task will always be executed after the tracks_per_album task is successfully finished. If the tracks_per_album fails, the tracks_per_album_ordered task will be skipped. Note: a task can have as many parents as desired. Here is the SQL query referred by the track_per_album_ordered task. sql/tracks_per_album_ordered.sql /* In this sql query, you can see the usage of parameters. This is based on Jinja templating. Parameters are passed from the profile used at run time (profiles are defined in the settings) After a query is compiled, they will appear in the compile folder */ SELECT tpa.* FROM {{schema_models}}.{{table_prefix}}tracks_per_album tpa --here we prepend the table name with {{table_prefix}} which enables to separate when testing. If ran from prod, there is no prefix. If a test user has prefix specified, then the prefix will be added. ORDER BY 2 DESC The fifth and final task is print_top_10_albums_by_tracks and is a python task: print_top_10_albums_by_tracks: module: print_top_10_albums_by_tracks type: python class: TopAlbumsPrinter parents: - tracks_per_album_ordered A python task enables you to use Python for your task. This means you could do anything, from extracting data from an API to running a data science model. python tasks require a module (this is the name of python file in the python folder. SAYN automatically looks there fore Python tasks) and a class (this should be the name of the class in the Python module.) Here is the Python codee referred by the print_top_10_albums_by_tracks task. python/print_top_10_albums_by_tracks.py #IMPORTANT: for python tasks to be able to execute, you neeed to have an __init__.py file into the python folder so it is treated as a package #here we define a python task #python tasks inherit from the sayn PythonTask from sayn import PythonTask #a python task needs to implement two functions #setup() wich operates necessary setup and returns self.ready() to indicate the task is ready to be ran #run() which executes the task and returns self.finished() to indicate the task has finished successfully class TopAlbumsPrinter(PythonTask): #here we are not doing anything for setup, just displaying the usage of self.failed() #in order to inform sayn that the task has failed, you would return self.failed() #note that self.failed() can also be used for run() #please note that setup() needs to follow the method's signature. Therefore it needs to be set as setup(self). def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() #here we define the code that will be executed at run time #please note that run() needs to follow the method's signature. Therefore it needs to be set as run(self). def run(self): #we can access the project parameters via the sayn_config attribute sayn_params = self.sayn_config.parameters #we use the config parameters to make the query dynamic #the query will therefore use parameters of the profile used at run time q = ''' SELECT tpao.* FROM {schema_models}.{table_prefix}tracks_per_album_ordered tpao LIMIT 10 ; '''.format( schema_models=sayn_params['schema_models'], table_prefix=sayn_params['table_prefix'] ) #the python task has the project's default_db connection object as an attribute. #this attribute has an number of method including select() to run a query and return results. #please see the documentation for more details on the API r = self.default_db.select(q) print('Printing top 10 albums by number of tracks:') for i in range(10): print('#{rank}: {album}, {n} tracks.'.format(rank=i+1, album=r[i]['album_name'], n=r[i]['n_tracks'])) return self.finished() Please note that for the Python tasks to run properly, you need to have an __init__.py file into the python folder so it is considered as a package . The TopAlbumsPrinter Python class implements two methods: setup and run . The setup method will be run when the DAG sets up all the tasks, and the run method will be run at execution time. Both methods need to have the signatures of the SAYN PythonTask class. As a result, you should only pass self as a parameter to those methods. A few things to note regarding python tasks: If you want to have the task fail (e.g. to control for errors), return self.failed() in the methods. As you can see in the run method, we are accessing a few useful attributes of the task. self.sayn_config.parameters contains the parameters used. self.default_db contains the default database. self.debault_db.select() effectively runs a select query on the default database. For more details on the python tasks, please refer to the Tasks documentation. This it, we now have set all our tasks and our project is ready to be ran :)","title":"Step 2.6: Set the tasks"},{"location":"tutorial/#running-your-project","text":"Now that the settings.yaml have our individual settings and that models.yaml have the tasks defined, we can run the sayn project! Run the following command in order to run the whole project (make sure you are at the root level of your SAYN project directory): sayn run . sayn run will run the whole project using the profile set in default_profile . A SAYN run is composed of setting up the DAG (when all tasks are setup) and running the DAG (when the DAG is executed). You should see the detail of the process being logged to your command line (those logs are also saved in the log folder). Tip: after running sayn run , you will also see a compile folder appear. This is where all SQL queries will be compiled by SAYN so you can check there the output after the templates are filled. The sayn run command has a few option parameters: - -t in order to run a specific task. For example sayn run -t track_details_sql would only run the track_details_sql task. - -d can be used in order to print debug logs to the command line. Tip: you can run a task and all its parents or children by prefixing or suffixing the task name after the -t flag with + . For example, running sayn run -t +print_top_10_albums_by_tracks would run the print_top_10_albums_by_tracks and all its parents in their logical order.","title":"Running your project"},{"location":"tutorial/#what-next","text":"This is it, you have now completed the SAYN tutorial, congratulations! You now know how to implement and run a SAYN project and are able to use SAYN to increase your analytics workflow efficiencies. You should now have a look into the further details of the documentation, as SAYN has much more to offer. In specific, you can have a look at the following sections: S1 S2 Enjoy SAYN!","title":"What Next?"},{"location":"tasks/overview/","text":"Tasks About Tasks are the core components of your SAYN project's DAG (directed acyclic graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on their parentage which you define within models.yaml . Core Tasks Please see below the core SAYN tasks: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to use Python. Can be used for a wide range of cases (basically anything Python lets you do), from data extraction to data science models. Extension Tasks Please see below the other tasks SAYN has available: copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file. Defining Tasks In models.yaml A task is defined by its type ( type is the only attribute shared by all tasks). Below is a simple example of a SAYN task definition: task_1: file_name: task_1.sql type: sql If a task is dependent upon another task, it can define parents as follows (a task can have as many parent(s) as desired): task_2: file_name: task_2.sql type: sql parents: - task_1 Finally, the different types of tasks have different attributes that need to be defined. Please see the additional detail in each specific task type's documentation.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"Tasks are the core components of your SAYN project's DAG (directed acyclic graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on their parentage which you define within models.yaml .","title":"About"},{"location":"tasks/overview/#core-tasks","text":"Please see below the core SAYN tasks: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to use Python. Can be used for a wide range of cases (basically anything Python lets you do), from data extraction to data science models.","title":"Core Tasks"},{"location":"tasks/overview/#extension-tasks","text":"Please see below the other tasks SAYN has available: copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Extension Tasks"},{"location":"tasks/overview/#defining-tasks-in-modelsyaml","text":"A task is defined by its type ( type is the only attribute shared by all tasks). Below is a simple example of a SAYN task definition: task_1: file_name: task_1.sql type: sql If a task is dependent upon another task, it can define parents as follows (a task can have as many parent(s) as desired): task_2: file_name: task_2.sql type: sql parents: - task_1 Finally, the different types of tasks have different attributes that need to be defined. Please see the additional detail in each specific task type's documentation.","title":"Defining Tasks In models.yaml"},{"location":"tasks/core/autosql/","text":"autosql Task About The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks In models.yaml An autosql task is defined as follows: task_autosql: type: autosql file_name: task_autosql.sql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: task_autosql It has a task name ( task_autosql here), and then defines the following mandatory attributes: type : the task type, this needs to be one the the task types supported by SAYN. file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). to : this sets the details of the data processing. staging_schema : specifies the schema which will be used to store any necessary temporary object created in the process. schema : is the destination schema where the object will be created. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Note: staging_schema and schema are actually optional. Not specifying those will use the database connection's default schema. We do recommend to set those however to prevent mistakes. In addition, autosql has the following optional attribute: ddl : can be used to control the DDLs used during the process execution. ddl is a mapping. Here are the lists of available ddl options: permissions : automatically grants permissions on the created object to specified roles and users. primary_key : sets the primary key on the table. indexes : sets an index on the table. columns : enables to specify the column types. Please see below an example with all ddl parameters set for an autosql task materialising into a table : task_autosql_ddl: type: autosql file_name: test_sql_incremental.sql ddl: primary_key: - listing_id columns: listing_id: type: integer listing_name: type: varchar indexes: - listing_id Using autosql In incremental Mode If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require at least a delete_key (a list of fields). Please see below an example: task_autosql_incremental: file_name: task_autosql_incremental.sql type: autosql materialisation: incremental to: staging_schema: analytics_staging schema: analytics_models table: task_autosql delete_key: - dt When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table.","title":"autosql"},{"location":"tasks/core/autosql/#autosql-task","text":"","title":"autosql Task"},{"location":"tasks/core/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/core/autosql/#defining-autosql-tasks-in-modelsyaml","text":"An autosql task is defined as follows: task_autosql: type: autosql file_name: task_autosql.sql materialisation: table to: staging_schema: analytics_staging schema: analytics_models table: task_autosql It has a task name ( task_autosql here), and then defines the following mandatory attributes: type : the task type, this needs to be one the the task types supported by SAYN. file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). to : this sets the details of the data processing. staging_schema : specifies the schema which will be used to store any necessary temporary object created in the process. schema : is the destination schema where the object will be created. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Note: staging_schema and schema are actually optional. Not specifying those will use the database connection's default schema. We do recommend to set those however to prevent mistakes. In addition, autosql has the following optional attribute: ddl : can be used to control the DDLs used during the process execution. ddl is a mapping. Here are the lists of available ddl options: permissions : automatically grants permissions on the created object to specified roles and users. primary_key : sets the primary key on the table. indexes : sets an index on the table. columns : enables to specify the column types. Please see below an example with all ddl parameters set for an autosql task materialising into a table : task_autosql_ddl: type: autosql file_name: test_sql_incremental.sql ddl: primary_key: - listing_id columns: listing_id: type: integer listing_name: type: varchar indexes: - listing_id","title":"Defining autosql Tasks In models.yaml"},{"location":"tasks/core/autosql/#using-autosql-in-incremental-mode","text":"If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require at least a delete_key (a list of fields). Please see below an example: task_autosql_incremental: file_name: task_autosql_incremental.sql type: autosql materialisation: incremental to: staging_schema: analytics_staging schema: analytics_models table: task_autosql delete_key: - dt When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table.","title":"Using autosql In incremental Mode"},{"location":"tasks/core/python/","text":"python Task About The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. Defining python Tasks In models.yaml A python task is defined as follows: task_python: type: python module: task_python class: TaskPython It has a task name ( task_python here), and then defines the following mandatory attributes: type : the task type, this needs to be one the the task types supported by SAYN. module : the name of the file within the python folder of the project's root. Please make sure you do not add the .py extension as the module will be imported . class : the name of the class in the module which should be ran. Writing A python Task Basics python tasks are used in the following way: import the PythonTask class from sayn . define the class you will want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.finished() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.finished() . Important: for python tasks you need to make sure the python folder in which you store your Python tasks' code contains an __init__.py file. Please see below some example code for a python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False #code you want to run if err: return self.failed() else: return self.finished() Using the SAYN API In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API. A lot of useful information is stored on the task in the sayn_config attribute: self.sayn_config.parameters : accesses project config parameters ( models.yaml , settings.yaml ). For more details on parameters , see the parameters section. self.parameters : accesses the task's parameters. self.sayn_config.credentials : accesses the credentials available for the profile used at run time. For more information on credentials please see the settings section. self.default_db : accesses the default_db specified in the models.yaml . Those are extremely useful in order to tailor your python tasks' code in order to separate between development and production environments.","title":"python"},{"location":"tasks/core/python/#python-task","text":"","title":"python Task"},{"location":"tasks/core/python/#about","text":"The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models.","title":"About"},{"location":"tasks/core/python/#defining-python-tasks-in-modelsyaml","text":"A python task is defined as follows: task_python: type: python module: task_python class: TaskPython It has a task name ( task_python here), and then defines the following mandatory attributes: type : the task type, this needs to be one the the task types supported by SAYN. module : the name of the file within the python folder of the project's root. Please make sure you do not add the .py extension as the module will be imported . class : the name of the class in the module which should be ran.","title":"Defining python Tasks In models.yaml"},{"location":"tasks/core/python/#writing-a-python-task","text":"","title":"Writing A python Task"},{"location":"tasks/core/python/#basics","text":"python tasks are used in the following way: import the PythonTask class from sayn . define the class you will want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.finished() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.finished() . Important: for python tasks you need to make sure the python folder in which you store your Python tasks' code contains an __init__.py file. Please see below some example code for a python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False #code you want to run if err: return self.failed() else: return self.finished()","title":"Basics"},{"location":"tasks/core/python/#using-the-sayn-api","text":"In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API. A lot of useful information is stored on the task in the sayn_config attribute: self.sayn_config.parameters : accesses project config parameters ( models.yaml , settings.yaml ). For more details on parameters , see the parameters section. self.parameters : accesses the task's parameters. self.sayn_config.credentials : accesses the credentials available for the profile used at run time. For more information on credentials please see the settings section. self.default_db : accesses the default_db specified in the models.yaml . Those are extremely useful in order to tailor your python tasks' code in order to separate between development and production environments.","title":"Using the SAYN API"},{"location":"tasks/extensions/copy/","text":"copy Task About The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL, MySQL, etc.) to your analytics warehouse. Defining copy Tasks In models.yaml A copy task is defined as follows: task_copy: type: copy from: db: from_db schema: from_schema table: from_table to: staging_schema: staging_schema schema: schema table: table_name ddl: columns: - name: column incremental_key: column delete_key: column copy tasks have the following parameters that need to be set: from : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. to : the destination details. The destination database is the default_db set in models.yaml . staging_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"copy"},{"location":"tasks/extensions/copy/#copy-task","text":"","title":"copy Task"},{"location":"tasks/extensions/copy/#about","text":"The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL, MySQL, etc.) to your analytics warehouse.","title":"About"},{"location":"tasks/extensions/copy/#defining-copy-tasks-in-modelsyaml","text":"A copy task is defined as follows: task_copy: type: copy from: db: from_db schema: from_schema table: from_table to: staging_schema: staging_schema schema: schema table: table_name ddl: columns: - name: column incremental_key: column delete_key: column copy tasks have the following parameters that need to be set: from : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. to : the destination details. The destination database is the default_db set in models.yaml . staging_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"Defining copy Tasks In models.yaml"},{"location":"tasks/extensions/dummy/","text":"dummy Task About The dummy is a task that does not do anything. It is mostly used as a connector between tasks. Defining dummy Tasks In models.yaml A dummy task is defined as follows: task_dummy: type: dummy This task does not require any other setting than its type . Usage dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in models.yaml : #your models code tasks: #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3: #task definition parents: - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. #your models code tasks: #some tasks dummy_task: type: dummy parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1: #task definition parents: - dummy_task task_mlt_parents_2: #task definition parents: - dummy_task task_mlt_parents_3: #task definition parents: - dummy_task","title":"dummy"},{"location":"tasks/extensions/dummy/#dummy-task","text":"","title":"dummy Task"},{"location":"tasks/extensions/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a connector between tasks.","title":"About"},{"location":"tasks/extensions/dummy/#defining-dummy-tasks-in-modelsyaml","text":"A dummy task is defined as follows: task_dummy: type: dummy This task does not require any other setting than its type .","title":"Defining dummy Tasks In models.yaml"},{"location":"tasks/extensions/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in models.yaml : #your models code tasks: #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3: #task definition parents: - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. #your models code tasks: #some tasks dummy_task: type: dummy parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1: #task definition parents: - dummy_task task_mlt_parents_2: #task definition parents: - dummy_task task_mlt_parents_3: #task definition parents: - dummy_task","title":"Usage"},{"location":"tasks/extensions/sql/","text":"sql Task About The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file. Defining sql Tasks In models.yaml A sql task is defined as follows: task_sql: type: sql file_name: sql_task.sql sql tasks only have one parameter that needs to be set: file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"sql"},{"location":"tasks/extensions/sql/#sql-task","text":"","title":"sql Task"},{"location":"tasks/extensions/sql/#about","text":"The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file.","title":"About"},{"location":"tasks/extensions/sql/#defining-sql-tasks-in-modelsyaml","text":"A sql task is defined as follows: task_sql: type: sql file_name: sql_task.sql sql tasks only have one parameter that needs to be set: file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"Defining sql Tasks In models.yaml"}]}