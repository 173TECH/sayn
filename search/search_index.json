{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected. Use Cases SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse. Data science: integrate and execute data science models. Key Features SAYN has the following key features: YAML based creation of DAGs (Direct Acyclic Graph). This means all analysts, including non Python proficient ones, can contribute to building ETL processes. SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation . Design Principles SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power! Support If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#_1","text":"SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected.","title":""},{"location":"#use-cases","text":"SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse. Data science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key-features","text":"SAYN has the following key features: YAML based creation of DAGs (Direct Acyclic Graph). This means all analysts, including non Python proficient ones, can contribute to building ETL processes. SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design-principles","text":"SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick-start","text":"$ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"commands/","text":"Commands About SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands. Commands Detail sayn init Initialises a SAYN project in the current working directory. sayn run Runs the whole SAYN project. This command should be run from the project's root. It has the following optional flags which can be cumulated as desired: -t : run specific tasks. -x : exclude specific tasks. -p : select profile to use for run. -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level. sayn run -t You can run specific tasks with the following commands: sayn run -t task_name : run task_name sayn run -t +task_name : run task_name and all its parents. sayn run -t task_name+ : run task_name and all its children. sayn run -t dag:dag_name : run all tasks from the dag dag_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x You can exclude specific tasks from a run with the -x flag. It can be used as follows: sayn run -x task_name : run all tasks except task_name . sayn run -t dag:marketing -x task_name : run all tasks in the marketing DAG except task_name . sayn run -p Runs SAYN using a specific profile. It is used with the same logic than for the -t flag. Please see below some examples: sayn run -p profile_name : runs SAYN using the settings of profile_name . sayn compile Compiles the SAYN code that would be executed. This command should be run from the project's root. The same optional flags than for sayn run apply. dag-image Generates a visualisation of the whole SAYN process. This command should be run from the project's root. This requires graphviz - both the software and the Python package .","title":"Commands"},{"location":"commands/#commands","text":"","title":"Commands"},{"location":"commands/#about","text":"SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands.","title":"About"},{"location":"commands/#commands-detail","text":"","title":"Commands Detail"},{"location":"commands/#sayn-init","text":"Initialises a SAYN project in the current working directory.","title":"sayn init"},{"location":"commands/#sayn-run","text":"Runs the whole SAYN project. This command should be run from the project's root. It has the following optional flags which can be cumulated as desired: -t : run specific tasks. -x : exclude specific tasks. -p : select profile to use for run. -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level.","title":"sayn run"},{"location":"commands/#sayn-run-t","text":"You can run specific tasks with the following commands: sayn run -t task_name : run task_name sayn run -t +task_name : run task_name and all its parents. sayn run -t task_name+ : run task_name and all its children. sayn run -t dag:dag_name : run all tasks from the dag dag_name . sayn run -t tag:tag_name run all tasks tagged with tag_name .","title":"sayn run -t"},{"location":"commands/#sayn-run-x","text":"You can exclude specific tasks from a run with the -x flag. It can be used as follows: sayn run -x task_name : run all tasks except task_name . sayn run -t dag:marketing -x task_name : run all tasks in the marketing DAG except task_name .","title":"sayn run -x"},{"location":"commands/#sayn-run-p","text":"Runs SAYN using a specific profile. It is used with the same logic than for the -t flag. Please see below some examples: sayn run -p profile_name : runs SAYN using the settings of profile_name .","title":"sayn run -p"},{"location":"commands/#sayn-compile","text":"Compiles the SAYN code that would be executed. This command should be run from the project's root. The same optional flags than for sayn run apply.","title":"sayn compile"},{"location":"commands/#dag-image","text":"Generates a visualisation of the whole SAYN process. This command should be run from the project's root. This requires graphviz - both the software and the Python package .","title":"dag-image"},{"location":"dags/","text":"DAGs About DAGs (Directed Acyclic Graph) are the essential processes being run by SAYN. They are defined by yaml files stored in the dags folder. Importing a DAG For a DAG to be able to run, it needs to be imported into the dags section of the project.yaml file as follows: project.yaml #... dags: - base #... Please note that you should not add the .yaml extension when importing a DAG within the project.yaml file. Creating a DAG Please see below an example DAG file: dag.yaml presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' tasks: load_data: type: python class: load_data.LoadData #task defined without preset dim_tournaments: type: autosql file_name: dim_tournaments.sql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: dim_tournaments parents: - load_data #task defined using a preset dim_arenas: preset: modelling file_name: dim_arenas.sql parents: - load_data # ... Each DAG file requires the following to be defined: tasks : the set of tasks that compose the DAG. For more details on tasks , please see the Tasks section. In addition, DAG files can define the following: presets : defines preset task structures so task can inherit attributes from those presets directly. presets defined within DAG files can inherit from preset defined at the project level in project.yaml . See the Presets section for more details.","title":"DAGs"},{"location":"dags/#dags","text":"","title":"DAGs"},{"location":"dags/#about","text":"DAGs (Directed Acyclic Graph) are the essential processes being run by SAYN. They are defined by yaml files stored in the dags folder.","title":"About"},{"location":"dags/#importing-a-dag","text":"For a DAG to be able to run, it needs to be imported into the dags section of the project.yaml file as follows: project.yaml #... dags: - base #... Please note that you should not add the .yaml extension when importing a DAG within the project.yaml file.","title":"Importing a DAG"},{"location":"dags/#creating-a-dag","text":"Please see below an example DAG file: dag.yaml presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' tasks: load_data: type: python class: load_data.LoadData #task defined without preset dim_tournaments: type: autosql file_name: dim_tournaments.sql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: dim_tournaments parents: - load_data #task defined using a preset dim_arenas: preset: modelling file_name: dim_arenas.sql parents: - load_data # ... Each DAG file requires the following to be defined: tasks : the set of tasks that compose the DAG. For more details on tasks , please see the Tasks section. In addition, DAG files can define the following: presets : defines preset task structures so task can inherit attributes from those presets directly. presets defined within DAG files can inherit from preset defined at the project level in project.yaml . See the Presets section for more details.","title":"Creating a DAG"},{"location":"parameters/","text":"Parameters About parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and some YAML files. parameters can also be accessed in python tasks. Defining Parameters parameters are defined at several levels: project.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from project.yaml . presets : a preset can set a parameters attribute so all tasks within that preset inherit those. tasks : a task can set a parameters attribute. Accessing Parameters For the below section, we consider a project with the following setup: project.yaml # ... parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models # ... settings.yaml # ... default_profile: dev profiles: dev: # ... parameters: user_prefix: songoku_ schema_logs: analytics_adhoc schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: # ... parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models In tasks Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{user_prefix}}{{task.name}}' Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: analytics_adhoc schema: analytics_adhoc table: songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql_param In presets parameters can be accessed in presets in the same way than they are accessed in tasks . For example, you could have the following modelling preset definition: presets: modelling: type: autosql materialisation: table destination: tmp_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{user_prefix}}{{task.name}}' The interpretation of this preset will work as in the above section, using parameters from the relevant profile at execution time. In SQL Queries For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter_name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt.* FROM {{schema_models}}.{{user_prefix}}my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt.* FROM analytics_adhoc.songoku_my_table AS mt In Python Tasks parameters can be accessed in python tasks via the SAYN API as they are stored on the Task object: self.sayn_config.parameters : accesses the project parameters set in project.yaml and settings.yaml . self.parameters : accesses the task's parameters . For example, you could have the following python task code to access your project parameters: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False sayn_params = self.sayn_config.parameters #code you want to run if err: return self.failed() else: return self.success()","title":"Parameters"},{"location":"parameters/#parameters","text":"","title":"Parameters"},{"location":"parameters/#about","text":"parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and some YAML files. parameters can also be accessed in python tasks.","title":"About"},{"location":"parameters/#defining-parameters","text":"parameters are defined at several levels: project.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from project.yaml . presets : a preset can set a parameters attribute so all tasks within that preset inherit those. tasks : a task can set a parameters attribute.","title":"Defining Parameters"},{"location":"parameters/#accessing-parameters","text":"For the below section, we consider a project with the following setup: project.yaml # ... parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models # ... settings.yaml # ... default_profile: dev profiles: dev: # ... parameters: user_prefix: songoku_ schema_logs: analytics_adhoc schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: # ... parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models","title":"Accessing Parameters"},{"location":"parameters/#in-tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{user_prefix}}{{task.name}}' Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: analytics_adhoc schema: analytics_adhoc table: songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param: file_name: task_autosql_param.sql type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql_param","title":"In tasks"},{"location":"parameters/#in-presets","text":"parameters can be accessed in presets in the same way than they are accessed in tasks . For example, you could have the following modelling preset definition: presets: modelling: type: autosql materialisation: table destination: tmp_schema: '{{schema_staging}}' schema: '{{schema_models}}' table: '{{user_prefix}}{{task.name}}' The interpretation of this preset will work as in the above section, using parameters from the relevant profile at execution time.","title":"In presets"},{"location":"parameters/#in-sql-queries","text":"For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter_name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt.* FROM {{schema_models}}.{{user_prefix}}my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt.* FROM analytics_adhoc.songoku_my_table AS mt","title":"In SQL Queries"},{"location":"parameters/#in-python-tasks","text":"parameters can be accessed in python tasks via the SAYN API as they are stored on the Task object: self.sayn_config.parameters : accesses the project parameters set in project.yaml and settings.yaml . self.parameters : accesses the task's parameters . For example, you could have the following python task code to access your project parameters: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False sayn_params = self.sayn_config.parameters #code you want to run if err: return self.failed() else: return self.success()","title":"In Python Tasks"},{"location":"presets/","text":"Presets About presets enable to define preset tasks which can be used when defining tasks in DAGs. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition. Defining a preset presets are defined in a similar way than tasks within each individual DAG file. Please see below an example of a preset definition: presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' The above defines a modelling preset. Every task referring to it will be an autosql task and inherit all other attributes from the modelling preset. Defining tasks using presets tasks can inherit attributes directly from presets . In order to do so, specify the preset attribute on the task. The below example illustrates this by setting the modelling preset defined above on a task : tasks: #... task_preset: preset: modelling #other task properties #... Defining project-level presets If you use the same preset across multiple DAGs, you can avoid this repetition by defining a project-level preset . For example, if you use the modelling preset defined above across all DAGs of your SAYN project, you can define it directly in project.yaml in a similar way: project.yaml # ... presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' # ... You can then use that project-level preset to define presets within individual DAGs as follows: dag.yaml presets: modelling: preset: modelling # ...","title":"Presets"},{"location":"presets/#presets","text":"","title":"Presets"},{"location":"presets/#about","text":"presets enable to define preset tasks which can be used when defining tasks in DAGs. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition.","title":"About"},{"location":"presets/#defining-a-preset","text":"presets are defined in a similar way than tasks within each individual DAG file. Please see below an example of a preset definition: presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' The above defines a modelling preset. Every task referring to it will be an autosql task and inherit all other attributes from the modelling preset.","title":"Defining a preset"},{"location":"presets/#defining-tasks-using-presets","text":"tasks can inherit attributes directly from presets . In order to do so, specify the preset attribute on the task. The below example illustrates this by setting the modelling preset defined above on a task : tasks: #... task_preset: preset: modelling #other task properties #...","title":"Defining tasks using presets"},{"location":"presets/#defining-project-level-presets","text":"If you use the same preset across multiple DAGs, you can avoid this repetition by defining a project-level preset . For example, if you use the modelling preset defined above across all DAGs of your SAYN project, you can define it directly in project.yaml in a similar way: project.yaml # ... presets: modelling: type: autosql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: '{{task.name}}' # ... You can then use that project-level preset to define presets within individual DAGs as follows: dag.yaml presets: modelling: preset: modelling # ...","title":"Defining project-level presets"},{"location":"project_structure/","text":"SAYN Project Structure SAYN projects are structured as follows: project_name compile/ #only appears after first run dags/ dag.yaml logs/ #only appears after first run sayn.log python/ __init__.py task_1.py sql/ task_2.sql task_3.sql task_4.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where dag files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Structure"},{"location":"project_structure/#sayn-project-structure","text":"SAYN projects are structured as follows: project_name compile/ #only appears after first run dags/ dag.yaml logs/ #only appears after first run sayn.log python/ __init__.py task_1.py sql/ task_2.sql task_3.sql task_4.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where dag files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"SAYN Project Structure"},{"location":"databases/overview/","text":"Databases About SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: PostgreSQL Snowflake SQLite Usage In order to connect to databases, the connection credentials need to be added into the credentials sections of the settings.yaml file. Database credentials need to specify two key attributes: type : the connection type. This needs to be one of the supported databases. connect_args : the connection arguments. Because SAYN uses sqlalchemy, those connect_args match the connect_args from the sqlachemy create_engine method. Please see the database specific pages to see credential examples for each database. SAYN Database API SAYN provides three core functions in order to interact with the default database in Python scripts. The default database is stored on the default_db attribute of the Task object. It can therefore be accessed as follows in any Python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup def run(self): db = self.default_db #code you want to run Please see below the details of the available methods on the default_db . .execute() Functionality Lets you execute any SQL script on the default database. Signature def execute(self, script) script : the SQL script to be executed. There can be multiple SQL queries separated by ; within a single script. Returns None .select() Functionality Runs a SQL query and returns the results in a list of tuples. Signature def select(self, query, params=None) query : the SQL script to be executed. params : parameters of the sqlalchemy execute method. Returns List of tuples containing the results. There is one tuple per row. .load() Functionality Loads data into a destination table. Signature def load_data(self, table, schema, data) table : the destination table. schema : the destination schema. data : the data to be loaded. It should be a list of tuples, with one tuple per row. Returns None.","title":"Overview"},{"location":"databases/overview/#databases","text":"","title":"Databases"},{"location":"databases/overview/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: PostgreSQL Snowflake SQLite","title":"About"},{"location":"databases/overview/#usage","text":"In order to connect to databases, the connection credentials need to be added into the credentials sections of the settings.yaml file. Database credentials need to specify two key attributes: type : the connection type. This needs to be one of the supported databases. connect_args : the connection arguments. Because SAYN uses sqlalchemy, those connect_args match the connect_args from the sqlachemy create_engine method. Please see the database specific pages to see credential examples for each database.","title":"Usage"},{"location":"databases/overview/#sayn-database-api","text":"SAYN provides three core functions in order to interact with the default database in Python scripts. The default database is stored on the default_db attribute of the Task object. It can therefore be accessed as follows in any Python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup def run(self): db = self.default_db #code you want to run Please see below the details of the available methods on the default_db .","title":"SAYN Database API"},{"location":"databases/overview/#execute","text":"","title":".execute()"},{"location":"databases/overview/#functionality","text":"Lets you execute any SQL script on the default database.","title":"Functionality"},{"location":"databases/overview/#signature","text":"def execute(self, script) script : the SQL script to be executed. There can be multiple SQL queries separated by ; within a single script.","title":"Signature"},{"location":"databases/overview/#returns","text":"None","title":"Returns"},{"location":"databases/overview/#select","text":"","title":".select()"},{"location":"databases/overview/#functionality_1","text":"Runs a SQL query and returns the results in a list of tuples.","title":"Functionality"},{"location":"databases/overview/#signature_1","text":"def select(self, query, params=None) query : the SQL script to be executed. params : parameters of the sqlalchemy execute method.","title":"Signature"},{"location":"databases/overview/#returns_1","text":"List of tuples containing the results. There is one tuple per row.","title":"Returns"},{"location":"databases/overview/#load","text":"","title":".load()"},{"location":"databases/overview/#functionality_2","text":"Loads data into a destination table.","title":"Functionality"},{"location":"databases/overview/#signature_2","text":"def load_data(self, table, schema, data) table : the destination table. schema : the destination schema. data : the data to be loaded. It should be a list of tuples, with one tuple per row.","title":"Signature"},{"location":"databases/overview/#returns_2","text":"None.","title":"Returns"},{"location":"databases/postgresql/","text":"PostgreSQL Connection This is an example of PostgreSQL credential details to connect: settings.yaml # ... credentials: postgresql-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name] # ... The connect_args need to match the sqlalchemy create_engine connect_args .","title":"PostgreSQL"},{"location":"databases/postgresql/#postgresql","text":"","title":"PostgreSQL"},{"location":"databases/postgresql/#connection","text":"This is an example of PostgreSQL credential details to connect: settings.yaml # ... credentials: postgresql-conn: type: postgresql connect_args: host: [host] port: [port] user: [username] password: '[password]' #use quotes to avoid conflict with special characters dbname: [database_name] # ... The connect_args need to match the sqlalchemy create_engine connect_args .","title":"Connection"},{"location":"databases/snowflake/","text":"Snowflake Connection This is an example of Snowflake credential details to connect: settings.yaml # ... credentials: snowflake-conn: type: snowflake connect_args: account: [account] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database] schema: [schema] warehouse: [warehouse] role: [role] # ... The connect_args need to match the sqlalchemy create_engine connect_args . Additional Notes Autocommit Autocommit is False by default when creating Snowflake connections through sqlalchemy. If you are using sql tasks, you might want to set the autocommit attribute to True in the connection credentials.","title":"Snowflake"},{"location":"databases/snowflake/#snowflake","text":"","title":"Snowflake"},{"location":"databases/snowflake/#connection","text":"This is an example of Snowflake credential details to connect: settings.yaml # ... credentials: snowflake-conn: type: snowflake connect_args: account: [account] user: [username] password: '[password]' #use quotes to avoid conflict with special characters database: [database] schema: [schema] warehouse: [warehouse] role: [role] # ... The connect_args need to match the sqlalchemy create_engine connect_args .","title":"Connection"},{"location":"databases/snowflake/#additional-notes","text":"","title":"Additional Notes"},{"location":"databases/snowflake/#autocommit","text":"Autocommit is False by default when creating Snowflake connections through sqlalchemy. If you are using sql tasks, you might want to set the autocommit attribute to True in the connection credentials.","title":"Autocommit"},{"location":"databases/sqlite/","text":"SQLite Connection This is an example of SQLite credential details to connect: settings.yaml # ... credentials: sqlite-conn: type: sqlite database: [path_to_database] # ... The attributes other than type need to match the sqlalchemy create_engine connect_args .","title":"SQLite"},{"location":"databases/sqlite/#sqlite","text":"","title":"SQLite"},{"location":"databases/sqlite/#connection","text":"This is an example of SQLite credential details to connect: settings.yaml # ... credentials: sqlite-conn: type: sqlite database: [path_to_database] # ... The attributes other than type need to match the sqlalchemy create_engine connect_args .","title":"Connection"},{"location":"settings/project_yaml/","text":"Settings: project.yaml Role The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . Content Please see below and example of project.yaml file: project.yaml default_db: warehouse required_credentials: - warehouse dags: - base parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models The project.yaml file requires the following to be defined: default_db : the database use at run time. required_credentials : the required credentials to run the project. Credentials details are defined the settings.yaml file. dags : the DAGs of the project (this example only imports one DAG). Those DAGs contain the tasks. In addition, the project.yaml file can define the following in order to make the SAYN project more dynamic and efficient: parameters : those parameters are used to make the tasks dynamic. They are overwritten by parameters in settings.yaml . See the Parameters section for more details. presets : defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"project.yaml"},{"location":"settings/project_yaml/#settings-projectyaml","text":"","title":"Settings: project.yaml"},{"location":"settings/project_yaml/#role","text":"The project.yaml defines the core components of the SAYN project. It is shared across all collaborators .","title":"Role"},{"location":"settings/project_yaml/#content","text":"Please see below and example of project.yaml file: project.yaml default_db: warehouse required_credentials: - warehouse dags: - base parameters: user_prefix: '' schema_logs: analytics_logs schema_staging: analytics_staging schema_models: analytics_models The project.yaml file requires the following to be defined: default_db : the database use at run time. required_credentials : the required credentials to run the project. Credentials details are defined the settings.yaml file. dags : the DAGs of the project (this example only imports one DAG). Those DAGs contain the tasks. In addition, the project.yaml file can define the following in order to make the SAYN project more dynamic and efficient: parameters : those parameters are used to make the tasks dynamic. They are overwritten by parameters in settings.yaml . See the Parameters section for more details. presets : defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"Content"},{"location":"settings/settings_yaml/","text":"Settings: settings.yaml Role The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . Content Overview Please see below and example of settings.yaml file: settings.yaml default_profile: dev profiles: dev: credentials: warehouse: snowflake-songoku parameters: table_prefix: songoku_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: credentials: warehouse: snowflake-prod # no need for prod parameters as those are read from models.yaml credentials: snowflake-songoku: type: snowflake connect_args: account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] snowflake-prod: type: snowflake connect_args: account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] The settings.yaml file requires the following to be defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . credentials : the list of credentials for the user. As can be observed, this file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production. About Credentials The credentials section of the settings.yaml file is used to store both databases and API credentials. Databases SAYN supports various databases. In order to create a database credential, define the type as one of the supported databases (see the Database section for more details) and the connection parameters relevant to the database type. APIs In order to define API credentials, use the type: api and pass the API connection parameters. Please see an example below: # ... credentials: # ... credential_name: type: api api_key: 'api_key' Those API credentials can then be accessed in python tasks through the Task object.","title":"settings.yaml"},{"location":"settings/settings_yaml/#settings-settingsyaml","text":"","title":"Settings: settings.yaml"},{"location":"settings/settings_yaml/#role","text":"The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) .","title":"Role"},{"location":"settings/settings_yaml/#content","text":"","title":"Content"},{"location":"settings/settings_yaml/#overview","text":"Please see below and example of settings.yaml file: settings.yaml default_profile: dev profiles: dev: credentials: warehouse: snowflake-songoku parameters: table_prefix: songoku_ schema_logs: analytics_logs schema_staging: analytics_adhoc schema_models: analytics_adhoc prod: credentials: warehouse: snowflake-prod # no need for prod parameters as those are read from models.yaml credentials: snowflake-songoku: type: snowflake connect_args: account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] snowflake-prod: type: snowflake connect_args: account: [snowflake-account] user: [user-name] password: '[password]' database: [database] schema: [schema] warehouse: [warehouse] role: [role] The settings.yaml file requires the following to be defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . credentials : the list of credentials for the user. As can be observed, this file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production.","title":"Overview"},{"location":"settings/settings_yaml/#about-credentials","text":"The credentials section of the settings.yaml file is used to store both databases and API credentials.","title":"About Credentials"},{"location":"settings/settings_yaml/#databases","text":"SAYN supports various databases. In order to create a database credential, define the type as one of the supported databases (see the Database section for more details) and the connection parameters relevant to the database type.","title":"Databases"},{"location":"settings/settings_yaml/#apis","text":"In order to define API credentials, use the type: api and pass the API connection parameters. Please see an example below: # ... credentials: # ... credential_name: type: api api_key: 'api_key' Those API credentials can then be accessed in python tasks through the Task object.","title":"APIs"},{"location":"tasks/autosql/","text":"autosql Task About The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks In models.yaml An autosql task is defined as follows: task_autosql: type: autosql file_name: task_autosql.sql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : specifies the schema which will be used to store any necessary temporary object created in the process. This is optional. schema : is the destination schema where the object will be created. This is optional. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Using autosql In incremental Mode If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require a delete_key to be set. Please see below an example: task_autosql_incremental: type: autosql file_name: task_autosql_incremental.sql materialisation: incremental destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql delete_key: - dt When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table.","title":"autosql"},{"location":"tasks/autosql/#autosql-task","text":"","title":"autosql Task"},{"location":"tasks/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/autosql/#defining-autosql-tasks-in-modelsyaml","text":"An autosql task is defined as follows: task_autosql: type: autosql file_name: task_autosql.sql materialisation: table destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : specifies the schema which will be used to store any necessary temporary object created in the process. This is optional. schema : is the destination schema where the object will be created. This is optional. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only.","title":"Defining autosql Tasks In models.yaml"},{"location":"tasks/autosql/#using-autosql-in-incremental-mode","text":"If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require a delete_key to be set. Please see below an example: task_autosql_incremental: type: autosql file_name: task_autosql_incremental.sql materialisation: incremental destination: tmp_schema: analytics_staging schema: analytics_models table: task_autosql delete_key: - dt When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table.","title":"Using autosql In incremental Mode"},{"location":"tasks/copy/","text":"copy Task About The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Defining copy Tasks In models.yaml A copy task is defined as follows: task_copy: type: copy source: db: from_db schema: from_schema table: from_table destination: tmp_schema: staging_schema schema: schema table: table_name ddl: columns: - name: column incremental_key: column delete_key: column copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. destination : the destination details. The destination database is the default_db set in models.yaml . tmp_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"copy"},{"location":"tasks/copy/#copy-task","text":"","title":"copy Task"},{"location":"tasks/copy/#about","text":"The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse.","title":"About"},{"location":"tasks/copy/#defining-copy-tasks-in-modelsyaml","text":"A copy task is defined as follows: task_copy: type: copy source: db: from_db schema: from_schema table: from_table destination: tmp_schema: staging_schema schema: schema table: table_name ddl: columns: - name: column incremental_key: column delete_key: column copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. destination : the destination details. The destination database is the default_db set in models.yaml . tmp_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"Defining copy Tasks In models.yaml"},{"location":"tasks/dummy/","text":"dummy Task About The dummy is a task that does not do anything. It is mostly used as a connector between tasks. Defining dummy Tasks In models.yaml A dummy task is defined as follows: task_dummy: type: dummy This task does not require any other setting than its type . Usage dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in your DAG dag.yaml : tasks: #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3: #task definition parents: - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. tasks: #some tasks dummy_task: type: dummy parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1: #task definition parents: - dummy_task task_mlt_parents_2: #task definition parents: - dummy_task task_mlt_parents_3: #task definition parents: - dummy_task","title":"dummy"},{"location":"tasks/dummy/#dummy-task","text":"","title":"dummy Task"},{"location":"tasks/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a connector between tasks.","title":"About"},{"location":"tasks/dummy/#defining-dummy-tasks-in-modelsyaml","text":"A dummy task is defined as follows: task_dummy: type: dummy This task does not require any other setting than its type .","title":"Defining dummy Tasks In models.yaml"},{"location":"tasks/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in your DAG dag.yaml : tasks: #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2: #task definition parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3: #task definition parents: - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. tasks: #some tasks dummy_task: type: dummy parents: - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1: #task definition parents: - dummy_task task_mlt_parents_2: #task definition parents: - dummy_task task_mlt_parents_3: #task definition parents: - dummy_task","title":"Usage"},{"location":"tasks/overview/","text":"Tasks About Tasks are the core components of your SAYN DAGs (Directed Acyclic Graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on the parents you define. Defining Tasks In DAGs Tasks are defined in individual DAGs . Their definition is composed of several attributes. type A task is defined by its type - type is the only attribute shared by all tasks. Below is a simple example of a SAYN task definition: dag.yaml task_1: type: sql file_name: task_1.sql Please see the Task Types section on this page for the list of all task types. parents If a task is dependent upon another task, it can define parents . A task can have as many parents as desired. Please see below an example which shows how to define parents : dag.yaml task_2: type: sql file_name: task_2.sql parents: - task_1 parameters Enables to set some parameters at the task level. Those can then be accessed in the task. This is defined as follows: dag.yaml task_3: type: sql file_name: task_3.sql parameters: field_select: 'a, b, c' For more details on parameters , please see the Parameters section. presets presets enable you to define some standard tasks. Other tasks can then inherit attributes from the pre-defined presets . This is defined as follows: dag.yaml task_4: #this task would inherit everything from the modelling preset preset: modelling For more details on presets , please see the Presets section. tags Tasks can define a tags attribute - you can define as many tags as desired on a task. Those tags can be used in order to run only tasks which are defined with a specific tag. This is useful to group several tasks across multiple DAGs under one structure. Please see below how to define tags on a task: dag.yaml task_5: type: python class: my_module.MyClass tags: - extract type specific attributes Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it. Task Types Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"Tasks are the core components of your SAYN DAGs (Directed Acyclic Graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on the parents you define.","title":"About"},{"location":"tasks/overview/#defining-tasks-in-dags","text":"Tasks are defined in individual DAGs . Their definition is composed of several attributes.","title":"Defining Tasks In DAGs"},{"location":"tasks/overview/#type","text":"A task is defined by its type - type is the only attribute shared by all tasks. Below is a simple example of a SAYN task definition: dag.yaml task_1: type: sql file_name: task_1.sql Please see the Task Types section on this page for the list of all task types.","title":"type"},{"location":"tasks/overview/#parents","text":"If a task is dependent upon another task, it can define parents . A task can have as many parents as desired. Please see below an example which shows how to define parents : dag.yaml task_2: type: sql file_name: task_2.sql parents: - task_1","title":"parents"},{"location":"tasks/overview/#parameters","text":"Enables to set some parameters at the task level. Those can then be accessed in the task. This is defined as follows: dag.yaml task_3: type: sql file_name: task_3.sql parameters: field_select: 'a, b, c' For more details on parameters , please see the Parameters section.","title":"parameters"},{"location":"tasks/overview/#presets","text":"presets enable you to define some standard tasks. Other tasks can then inherit attributes from the pre-defined presets . This is defined as follows: dag.yaml task_4: #this task would inherit everything from the modelling preset preset: modelling For more details on presets , please see the Presets section.","title":"presets"},{"location":"tasks/overview/#tags","text":"Tasks can define a tags attribute - you can define as many tags as desired on a task. Those tags can be used in order to run only tasks which are defined with a specific tag. This is useful to group several tasks across multiple DAGs under one structure. Please see below how to define tags on a task: dag.yaml task_5: type: python class: my_module.MyClass tags: - extract","title":"tags"},{"location":"tasks/overview/#type-specific-attributes","text":"Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it.","title":"type specific attributes"},{"location":"tasks/overview/#task-types","text":"Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Task Types"},{"location":"tasks/python/","text":"python Task About The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. Defining python Tasks In models.yaml A python task is defined as follows: task_python: type: python class: task_python.TaskPython It is defined by the following attributes: type : python . class : the import statement that should be executed to import the Python Class. Please note that Python tasks scripts should be saved in the python folder within the project's root. Please note that for the python tasks to run, you must have an __init__.py file into the python folder so it is treated as a package. Writing A python Task Basics Please see below an example of a python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False #code you want to run if err: return self.failed() else: return self.success() As you can observe, writing a python task requires the following: import the PythonTask class from sayn . define the class you want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.success() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.success() . Using the SAYN API In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API. A lot of useful information is stored on the task in the sayn_config attribute: self.sayn_config.parameters : accesses project config parameters ( project.yaml , settings.yaml ). For more details on parameters , see the Parameters section. self.parameters : accesses the task's parameters. self.sayn_config.api_credentials : dictionary containing the API credentials available for the profile used at run time. self.sayn_config.dbs : dictionary containing database objects specified in the profile used at run time. self.default_db : accesses the default_db specified in the project.yaml file. Using those parameters is extremely useful in order to tailor your python tasks' code in order to separate between development and production environments.","title":"python"},{"location":"tasks/python/#python-task","text":"","title":"python Task"},{"location":"tasks/python/#about","text":"The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models.","title":"About"},{"location":"tasks/python/#defining-python-tasks-in-modelsyaml","text":"A python task is defined as follows: task_python: type: python class: task_python.TaskPython It is defined by the following attributes: type : python . class : the import statement that should be executed to import the Python Class. Please note that Python tasks scripts should be saved in the python folder within the project's root. Please note that for the python tasks to run, you must have an __init__.py file into the python folder so it is treated as a package.","title":"Defining python Tasks In models.yaml"},{"location":"tasks/python/#writing-a-python-task","text":"","title":"Writing A python Task"},{"location":"tasks/python/#basics","text":"Please see below an example of a python task: from sayn import PythonTask class TaskPython(PythonTask): def setup(self): #code doing setup err = False if err: return self.failed() else: return self.ready() def run(self): err = False #code you want to run if err: return self.failed() else: return self.success() As you can observe, writing a python task requires the following: import the PythonTask class from sayn . define the class you want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.success() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.success() .","title":"Basics"},{"location":"tasks/python/#using-the-sayn-api","text":"In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API. A lot of useful information is stored on the task in the sayn_config attribute: self.sayn_config.parameters : accesses project config parameters ( project.yaml , settings.yaml ). For more details on parameters , see the Parameters section. self.parameters : accesses the task's parameters. self.sayn_config.api_credentials : dictionary containing the API credentials available for the profile used at run time. self.sayn_config.dbs : dictionary containing database objects specified in the profile used at run time. self.default_db : accesses the default_db specified in the project.yaml file. Using those parameters is extremely useful in order to tailor your python tasks' code in order to separate between development and production environments.","title":"Using the SAYN API"},{"location":"tasks/sql/","text":"sql Task About The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file. Defining sql Tasks In models.yaml A sql task is defined as follows: task_sql: type: sql file_name: sql_task.sql sql tasks only have one parameter that needs to be set: type : sql . file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"sql"},{"location":"tasks/sql/#sql-task","text":"","title":"sql Task"},{"location":"tasks/sql/#about","text":"The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file.","title":"About"},{"location":"tasks/sql/#defining-sql-tasks-in-modelsyaml","text":"A sql task is defined as follows: task_sql: type: sql file_name: sql_task.sql sql tasks only have one parameter that needs to be set: type : sql . file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"Defining sql Tasks In models.yaml"},{"location":"tutorials/tutorial_part1/","text":"Tutorial: Part 1 What We Will Cover This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project in the folder created by sayn init and creates a small ETL process based on synthetic data. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com . Running SAYN Run the below in order to install SAYN and process your first SAYN run. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run Running sayn run will output logging on your terminal about the process being executed. This is what will happen: SAYN will run all project tasks. Those are all in the DAG file dags/base.yaml . The tasks include: One python task which creates some logs and stores them into several log tables within a test.db SQLite database. This database is created in your project's folder root. Several autosql tasks which create data models including tables and views based on those logs. You can open test.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background. Project Overview The test_sayn folder has the following structure: test_sayn compile/ # only appears after first run dags/ base.yaml logs/ # only appears after first run sayn.log python/ __init__.py load_data.py utils/ __init__.py log_creator.py sql/ dim_arenas.sql dim_fighters.sql dim_tournaments.sql f_battles.sql f_fighter_results.sql f_rankings.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where DAG files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution. Implementing Your Project We will now go through the example project and explain the process of building a SAYN project. You can use the test_sayn folder you created to follow along, all the code is already written there. Step 1: Define the SAYN project with project.yaml Add the project.yaml file at the root level of your directory. Here is the file from the example: project.yaml required_credentials: - warehouse default_db: warehouse dags: - base The following is defined: required_credentials : the required credentials to run the project. Credential details are defined in the settings.yaml file. default_db : the database used at run time. dags : the DAGs of the project (this example has only one dag which can be found at dags/base.yaml ). Those DAGs contain the tasks. Step 2: Define your individual settings with settings.yaml Add the settings.yaml file at the root level of your directory. Here is the file from the example: settings.yaml default_profile: dev profiles: dev: credentials: warehouse: test_db prod: credentials: warehouse: prod_db credentials: test_db: type: sqlite database: test.db prod_db: type: sqlite database: prod.db The following is defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. Here we include the credential details for a dev and a prod profile. credentials : the list of credentials for the user. Step 3: Define your DAG(s) In SAYN, DAGs are defined in yaml files within the dags folder. As seen before, those dags are imported in the project.yaml file in order to be executed. When importing the DAGs in the project.yaml file, you should write the name without the .yaml extension. Our project contains only one DAG: base.yaml . Below is the file: base.yaml tasks: load_data: type: python class: load_data.LoadData dim_tournaments: type: autosql file_name: dim_tournaments.sql materialisation: table destination: tmp_schema: main schema: main table: dim_tournaments parents: - load_data dim_arenas: type: autosql file_name: dim_arenas.sql materialisation: table destination: tmp_schema: main schema: main table: dim_arenas parents: - load_data dim_fighters: type: autosql file_name: dim_fighters.sql materialisation: table destination: tmp_schema: main schema: main table: dim_fighters parents: - load_data f_battles: type: autosql file_name: f_battles.sql materialisation: table destination: tmp_schema: main schema: main table: f_battles parents: - load_data - dim_tournaments - dim_arenas - dim_fighters f_fighter_results: type: autosql file_name: f_fighter_results.sql materialisation: table destination: tmp_schema: main schema: main table: f_fighter_results parents: - f_battles f_rankings: type: autosql file_name: f_rankings.sql materialisation: view destination: tmp_schema: main schema: main table: f_rankings parents: - f_fighter_results The following is defined: tasks : the tasks of the DAG. Each task is defined by a type and various properties respective to its type . In our example, we use two task types: python : lets you run a Python process. The load_data.py is our only python task. It creates some synthetic logs and loads them to our database. For more information about how to build python tasks, visit the Python section autosql : lets you write a SELECT statement and SAYN then creates the table or view automatically for you. Our example has multiple autosql tasks which create models based on the logs. For more information about setting up autosql tasks, visit the autosql section . Running Your Project You can now run your SAYN project with the following commands: sayn run : runs the whole project sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : runs the specific task More options are available to run specific components of your SAYN project. All details can be found in the Commands section. What Next? You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"Tutorial (Part 1)"},{"location":"tutorials/tutorial_part1/#tutorial-part-1","text":"","title":"Tutorial: Part 1"},{"location":"tutorials/tutorial_part1/#what-we-will-cover","text":"This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project in the folder created by sayn init and creates a small ETL process based on synthetic data. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com .","title":"What We Will Cover"},{"location":"tutorials/tutorial_part1/#running-sayn","text":"Run the below in order to install SAYN and process your first SAYN run. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run Running sayn run will output logging on your terminal about the process being executed. This is what will happen: SAYN will run all project tasks. Those are all in the DAG file dags/base.yaml . The tasks include: One python task which creates some logs and stores them into several log tables within a test.db SQLite database. This database is created in your project's folder root. Several autosql tasks which create data models including tables and views based on those logs. You can open test.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background.","title":"Running SAYN"},{"location":"tutorials/tutorial_part1/#project-overview","text":"The test_sayn folder has the following structure: test_sayn compile/ # only appears after first run dags/ base.yaml logs/ # only appears after first run sayn.log python/ __init__.py load_data.py utils/ __init__.py log_creator.py sql/ dim_arenas.sql dim_fighters.sql dim_tournaments.sql f_battles.sql f_fighter_results.sql f_rankings.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where DAG files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Overview"},{"location":"tutorials/tutorial_part1/#implementing-your-project","text":"We will now go through the example project and explain the process of building a SAYN project. You can use the test_sayn folder you created to follow along, all the code is already written there.","title":"Implementing Your Project"},{"location":"tutorials/tutorial_part1/#step-1-define-the-sayn-project-with-projectyaml","text":"Add the project.yaml file at the root level of your directory. Here is the file from the example: project.yaml required_credentials: - warehouse default_db: warehouse dags: - base The following is defined: required_credentials : the required credentials to run the project. Credential details are defined in the settings.yaml file. default_db : the database used at run time. dags : the DAGs of the project (this example has only one dag which can be found at dags/base.yaml ). Those DAGs contain the tasks.","title":"Step 1: Define the SAYN project with project.yaml"},{"location":"tutorials/tutorial_part1/#step-2-define-your-individual-settings-with-settingsyaml","text":"Add the settings.yaml file at the root level of your directory. Here is the file from the example: settings.yaml default_profile: dev profiles: dev: credentials: warehouse: test_db prod: credentials: warehouse: prod_db credentials: test_db: type: sqlite database: test.db prod_db: type: sqlite database: prod.db The following is defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. Here we include the credential details for a dev and a prod profile. credentials : the list of credentials for the user.","title":"Step 2: Define your individual settings with settings.yaml"},{"location":"tutorials/tutorial_part1/#step-3-define-your-dags","text":"In SAYN, DAGs are defined in yaml files within the dags folder. As seen before, those dags are imported in the project.yaml file in order to be executed. When importing the DAGs in the project.yaml file, you should write the name without the .yaml extension. Our project contains only one DAG: base.yaml . Below is the file: base.yaml tasks: load_data: type: python class: load_data.LoadData dim_tournaments: type: autosql file_name: dim_tournaments.sql materialisation: table destination: tmp_schema: main schema: main table: dim_tournaments parents: - load_data dim_arenas: type: autosql file_name: dim_arenas.sql materialisation: table destination: tmp_schema: main schema: main table: dim_arenas parents: - load_data dim_fighters: type: autosql file_name: dim_fighters.sql materialisation: table destination: tmp_schema: main schema: main table: dim_fighters parents: - load_data f_battles: type: autosql file_name: f_battles.sql materialisation: table destination: tmp_schema: main schema: main table: f_battles parents: - load_data - dim_tournaments - dim_arenas - dim_fighters f_fighter_results: type: autosql file_name: f_fighter_results.sql materialisation: table destination: tmp_schema: main schema: main table: f_fighter_results parents: - f_battles f_rankings: type: autosql file_name: f_rankings.sql materialisation: view destination: tmp_schema: main schema: main table: f_rankings parents: - f_fighter_results The following is defined: tasks : the tasks of the DAG. Each task is defined by a type and various properties respective to its type . In our example, we use two task types: python : lets you run a Python process. The load_data.py is our only python task. It creates some synthetic logs and loads them to our database. For more information about how to build python tasks, visit the Python section autosql : lets you write a SELECT statement and SAYN then creates the table or view automatically for you. Our example has multiple autosql tasks which create models based on the logs. For more information about setting up autosql tasks, visit the autosql section .","title":"Step 3: Define your DAG(s)"},{"location":"tutorials/tutorial_part1/#running-your-project","text":"You can now run your SAYN project with the following commands: sayn run : runs the whole project sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : runs the specific task More options are available to run specific components of your SAYN project. All details can be found in the Commands section.","title":"Running Your Project"},{"location":"tutorials/tutorial_part1/#what-next","text":"You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"What Next?"},{"location":"tutorials/tutorial_part2/","text":"Tutorial: Part 2 What We Will Cover This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com . Implementing parameters and presets The Tutorial: Part 1 got you to implement a first ETL process with SAYN. We will now look into two core SAYN features which will enable you to build efficient and dynamic projects as you scale: parameters and presets . For this tutorial, we will use the code generated by sayn init and make amends as we go through the new concepts. A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work. Step 1: Using parameters parameters Setup You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters to project.yaml . Those parameters will contain the default values for the SAYN project: project.yaml required_credentials: - warehouse default_db: warehouse dags: - base parameters: user_prefix: '' #no prefix for prod Then, add the parameters to your profiles in settings.yaml . settings.yaml default_profile: dev profiles: dev: credentials: warehouse: test_db parameters: user_prefix: sg_ prod: credentials: warehouse: prod_db parameters: user_prefix: '' #no prefix for prod credentials: test_db: type: sqlite database: test.db prod_db: type: sqlite database: prod.db Step 2: Making Tasks Dynamic With parameters Now that our parameters are setup, we can use those to make our tasks' code dynamic. We will now change our tasks' code to use the user_prefix parameter. In python tasks For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils/log_creator.py to see how we use the user_prefix parameter to change the table names. Replace your pyhton/load_data.py script with the code below. As you can observe, we create a user_prefix variable by reading self.sayn_config.parameters . The parameters are stored on the Task object and can therefore be accessed in that way. load_data.py import sqlite3 import logging from .utils.log_creator import prepare_data, generate_load_query from sayn import PythonTask class LoadData(PythonTask): def setup(self): err = False # we use this list to control how many battles we want per tournament tournament_battles = [ {\"tournament_id\": 1, \"n_battles\": 1000}, {\"tournament_id\": 2, \"n_battles\": 250}, {\"tournament_id\": 3, \"n_battles\": 500}, ] user_prefix = self.sayn_config.parameters['user_prefix'] try: self.data_to_load = prepare_data(tournament_battles, user_prefix=user_prefix) except Exception as e: err = True logging.error(e) if err: return self.failed() else: return self.ready() def run(self): user_prefix = self.sayn_config.parameters['user_prefix'] # load the logs for log_type, log_details in self.data_to_load.items(): # create table logging.info('Creating table: {log_type}.'.format(log_type=log_type)) self.default_db.execute(log_details['create']) # load logs logging.info('Loading logs: {log_type}.'.format(log_type=log_type)) logs = log_details['data'] for log in logs: q_insert = generate_load_query(log_type, log, user_prefix=user_prefix) self.default_db.execute(q_insert) # done logging.info('Done: {log_type}.'.format(log_type=log_type)) return self.success() In autosql tasks You can also access the project parameters in autosql tasks with the following syntax: {{parameter_name}} . SAYN's compilation process uses Jinja. Replace all the SQL queries with the following: dim_arenas.sql SELECT l.arena_id , l.arena_name FROM {{user_prefix}}logs_arenas l dim_fighters.sql SELECT l.fighter_id , l.fighter_name FROM {{user_prefix}}logs_fighters l dim_tournaments.sql SELECT l.tournament_id , l.tournament_name FROM {{user_prefix}}logs_tournaments l f_battles.sql WITH battles AS ( SELECT l.tournament_id , l.battle_id , l.arena_id , l.fighter1_id , l.fighter2_id , l.winner_id FROM {{user_prefix}}logs_battles l ) SELECT t.tournament_name , t.tournament_name || '-' || CAST(b.battle_id AS VARCHAR) AS battle_id , a.arena_name , f1.fighter_name AS fighter1_name , f2.fighter_name AS fighter2_name , w.fighter_name AS winner_name FROM battles b LEFT JOIN {{user_prefix}}dim_tournaments t ON b.tournament_id = t.tournament_id LEFT JOIN {{user_prefix}}dim_arenas a ON b.arena_id = a.arena_id LEFT JOIN {{user_prefix}}dim_fighters f1 ON b.fighter1_id = f1.fighter_id LEFT JOIN {{user_prefix}}dim_fighters f2 ON b.fighter2_id = f2.fighter_id LEFT JOIN {{user_prefix}}dim_fighters w ON b.winner_id = w.fighter_id f_fighter_results.sql WITH fighter1_outcome AS ( SELECT b.tournament_name , b.battle_id , b.arena_name , b.fighter1_name , CASE WHEN b.fighter1_name = b.winner_name THEN 1 ELSE 0 END AS is_winner FROM {{user_prefix}}f_battles b ) , fighter2_outcome AS ( SELECT b.tournament_name , b.battle_id , b.arena_name , b.fighter2_name , CASE WHEN b.fighter2_name = b.winner_name THEN 1 ELSE 0 END AS is_winner FROM {{user_prefix}}f_battles b ) SELECT f1.tournament_name , f1.battle_id , f1.arena_name , f1.fighter1_name AS fighter_name , f1.is_winner FROM fighter1_outcome f1 UNION SELECT f2.tournament_name , f2.battle_id , f2.arena_name , f2.fighter2_name AS fighter_name , f2.is_winner FROM fighter2_outcome f2 f_rankings SELECT fr.fighter_name , CAST(SUM(fr.is_winner) AS FLOAT) / COUNT(DISTINCT fr.battle_id) AS win_rate FROM {{user_prefix}}f_fighter_results fr GROUP BY 1 ORDER BY 2 DESC Our SQL queries are now all set to read tables which are prefixed with the relevant user_prefix parameter depending on the profile used at execution. However, our autosql tasks still create tables in a static way as we have the table attribute of our autosql tasks hardcoded. The next step will fix this. Step 3: Using presets To Standardise Task Definitions Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets enable to create standardised tasks which can be used to define other tasks by setting a preset attribute. We define a modelling preset and use it in our DAG base.yaml . Replace the file dags/base.yaml with the following: dags/base.yaml presets: modelling: type: autosql file_name: '{{task.name}}.sql' materialisation: table destination: tmp_schema: main schema: main table: '{{user_prefix}}{{task.name}}' parents: - load_data tasks: load_data: type: python class: load_data.LoadData #this task sets modelling as its preset attribute #therefore it inherits all the attributes from the modelling preset dim_tournaments: preset: modelling dim_arenas: preset: modelling dim_fighters: preset: modelling f_battles: preset: modelling parents: - dim_tournaments - dim_arenas - dim_fighters f_fighter_results: preset: modelling parents: - f_battles #for that task, we overwrite the modelling preset materialisation attribute as we want this model to be a view f_rankings: preset: modelling materialisation: view parents: - f_fighter_results Using the modelling preset on tasks' definitions will imply: those tasks will be autosql and materialise as a table . they will all have the load_data task as a parent. and they will have all other attributes set in the modelling preset. Running Our New Project You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a test.db database prefix all tables with sg_ and read from sg_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables We have made our SAYN project dynamic with parameters and made our tasks' definitions more efficients with presets . What Next? This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"Tutorial (Part 2)"},{"location":"tutorials/tutorial_part2/#tutorial-part-2","text":"","title":"Tutorial: Part 2"},{"location":"tutorials/tutorial_part2/#what-we-will-cover","text":"This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com .","title":"What We Will Cover"},{"location":"tutorials/tutorial_part2/#implementing-parameters-and-presets","text":"The Tutorial: Part 1 got you to implement a first ETL process with SAYN. We will now look into two core SAYN features which will enable you to build efficient and dynamic projects as you scale: parameters and presets . For this tutorial, we will use the code generated by sayn init and make amends as we go through the new concepts. A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work.","title":"Implementing parameters and presets"},{"location":"tutorials/tutorial_part2/#step-1-using-parameters","text":"","title":"Step 1: Using parameters"},{"location":"tutorials/tutorial_part2/#parameters-setup","text":"You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters to project.yaml . Those parameters will contain the default values for the SAYN project: project.yaml required_credentials: - warehouse default_db: warehouse dags: - base parameters: user_prefix: '' #no prefix for prod Then, add the parameters to your profiles in settings.yaml . settings.yaml default_profile: dev profiles: dev: credentials: warehouse: test_db parameters: user_prefix: sg_ prod: credentials: warehouse: prod_db parameters: user_prefix: '' #no prefix for prod credentials: test_db: type: sqlite database: test.db prod_db: type: sqlite database: prod.db","title":"parameters Setup"},{"location":"tutorials/tutorial_part2/#step-2-making-tasks-dynamic-with-parameters","text":"Now that our parameters are setup, we can use those to make our tasks' code dynamic. We will now change our tasks' code to use the user_prefix parameter.","title":"Step 2: Making Tasks Dynamic With parameters"},{"location":"tutorials/tutorial_part2/#in-python-tasks","text":"For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils/log_creator.py to see how we use the user_prefix parameter to change the table names. Replace your pyhton/load_data.py script with the code below. As you can observe, we create a user_prefix variable by reading self.sayn_config.parameters . The parameters are stored on the Task object and can therefore be accessed in that way. load_data.py import sqlite3 import logging from .utils.log_creator import prepare_data, generate_load_query from sayn import PythonTask class LoadData(PythonTask): def setup(self): err = False # we use this list to control how many battles we want per tournament tournament_battles = [ {\"tournament_id\": 1, \"n_battles\": 1000}, {\"tournament_id\": 2, \"n_battles\": 250}, {\"tournament_id\": 3, \"n_battles\": 500}, ] user_prefix = self.sayn_config.parameters['user_prefix'] try: self.data_to_load = prepare_data(tournament_battles, user_prefix=user_prefix) except Exception as e: err = True logging.error(e) if err: return self.failed() else: return self.ready() def run(self): user_prefix = self.sayn_config.parameters['user_prefix'] # load the logs for log_type, log_details in self.data_to_load.items(): # create table logging.info('Creating table: {log_type}.'.format(log_type=log_type)) self.default_db.execute(log_details['create']) # load logs logging.info('Loading logs: {log_type}.'.format(log_type=log_type)) logs = log_details['data'] for log in logs: q_insert = generate_load_query(log_type, log, user_prefix=user_prefix) self.default_db.execute(q_insert) # done logging.info('Done: {log_type}.'.format(log_type=log_type)) return self.success()","title":"In python tasks"},{"location":"tutorials/tutorial_part2/#in-autosql-tasks","text":"You can also access the project parameters in autosql tasks with the following syntax: {{parameter_name}} . SAYN's compilation process uses Jinja. Replace all the SQL queries with the following: dim_arenas.sql SELECT l.arena_id , l.arena_name FROM {{user_prefix}}logs_arenas l dim_fighters.sql SELECT l.fighter_id , l.fighter_name FROM {{user_prefix}}logs_fighters l dim_tournaments.sql SELECT l.tournament_id , l.tournament_name FROM {{user_prefix}}logs_tournaments l f_battles.sql WITH battles AS ( SELECT l.tournament_id , l.battle_id , l.arena_id , l.fighter1_id , l.fighter2_id , l.winner_id FROM {{user_prefix}}logs_battles l ) SELECT t.tournament_name , t.tournament_name || '-' || CAST(b.battle_id AS VARCHAR) AS battle_id , a.arena_name , f1.fighter_name AS fighter1_name , f2.fighter_name AS fighter2_name , w.fighter_name AS winner_name FROM battles b LEFT JOIN {{user_prefix}}dim_tournaments t ON b.tournament_id = t.tournament_id LEFT JOIN {{user_prefix}}dim_arenas a ON b.arena_id = a.arena_id LEFT JOIN {{user_prefix}}dim_fighters f1 ON b.fighter1_id = f1.fighter_id LEFT JOIN {{user_prefix}}dim_fighters f2 ON b.fighter2_id = f2.fighter_id LEFT JOIN {{user_prefix}}dim_fighters w ON b.winner_id = w.fighter_id f_fighter_results.sql WITH fighter1_outcome AS ( SELECT b.tournament_name , b.battle_id , b.arena_name , b.fighter1_name , CASE WHEN b.fighter1_name = b.winner_name THEN 1 ELSE 0 END AS is_winner FROM {{user_prefix}}f_battles b ) , fighter2_outcome AS ( SELECT b.tournament_name , b.battle_id , b.arena_name , b.fighter2_name , CASE WHEN b.fighter2_name = b.winner_name THEN 1 ELSE 0 END AS is_winner FROM {{user_prefix}}f_battles b ) SELECT f1.tournament_name , f1.battle_id , f1.arena_name , f1.fighter1_name AS fighter_name , f1.is_winner FROM fighter1_outcome f1 UNION SELECT f2.tournament_name , f2.battle_id , f2.arena_name , f2.fighter2_name AS fighter_name , f2.is_winner FROM fighter2_outcome f2 f_rankings SELECT fr.fighter_name , CAST(SUM(fr.is_winner) AS FLOAT) / COUNT(DISTINCT fr.battle_id) AS win_rate FROM {{user_prefix}}f_fighter_results fr GROUP BY 1 ORDER BY 2 DESC Our SQL queries are now all set to read tables which are prefixed with the relevant user_prefix parameter depending on the profile used at execution. However, our autosql tasks still create tables in a static way as we have the table attribute of our autosql tasks hardcoded. The next step will fix this.","title":"In autosql tasks"},{"location":"tutorials/tutorial_part2/#step-3-using-presets-to-standardise-task-definitions","text":"Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets enable to create standardised tasks which can be used to define other tasks by setting a preset attribute. We define a modelling preset and use it in our DAG base.yaml . Replace the file dags/base.yaml with the following: dags/base.yaml presets: modelling: type: autosql file_name: '{{task.name}}.sql' materialisation: table destination: tmp_schema: main schema: main table: '{{user_prefix}}{{task.name}}' parents: - load_data tasks: load_data: type: python class: load_data.LoadData #this task sets modelling as its preset attribute #therefore it inherits all the attributes from the modelling preset dim_tournaments: preset: modelling dim_arenas: preset: modelling dim_fighters: preset: modelling f_battles: preset: modelling parents: - dim_tournaments - dim_arenas - dim_fighters f_fighter_results: preset: modelling parents: - f_battles #for that task, we overwrite the modelling preset materialisation attribute as we want this model to be a view f_rankings: preset: modelling materialisation: view parents: - f_fighter_results Using the modelling preset on tasks' definitions will imply: those tasks will be autosql and materialise as a table . they will all have the load_data task as a parent. and they will have all other attributes set in the modelling preset.","title":"Step 3: Using presets To Standardise Task Definitions"},{"location":"tutorials/tutorial_part2/#running-our-new-project","text":"You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a test.db database prefix all tables with sg_ and read from sg_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables We have made our SAYN project dynamic with parameters and made our tasks' definitions more efficients with presets .","title":"Running Our New Project"},{"location":"tutorials/tutorial_part2/#what-next","text":"This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"What Next?"}]}