{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00b6 SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected. Use Cases \u00b6 SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse. Data science: integrate and execute data science models. Key Features \u00b6 SAYN has the following key features: YAML based creation of DAGs (Direct Acyclic Graph). This means all analysts, including non Python proficient ones, can contribute to building ETL processes. SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation . Design Principles \u00b6 SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start \u00b6 $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power! Support \u00b6 If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License \u00b6 SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#_1","text":"SAYN is a data-modelling and processing framework for automating Python and SQL tasks. It enables analytics teams to build robust data infrastructures in minutes. Status: SAYN is under active development so some changes can be expected.","title":""},{"location":"#use-cases","text":"SAYN can be used for multiple purposes across the analytics workflow: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse. Data science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key-features","text":"SAYN has the following key features: YAML based creation of DAGs (Direct Acyclic Graph). This means all analysts, including non Python proficient ones, can contribute to building ETL processes. SQL SELECT statements : turn your queries into managed tables and views automatically. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design-principles","text":"SAYN is designed around three core principles: Simplicity : data models and processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN currently supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick-start","text":"$ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"commands/","text":"Commands \u00b6 About \u00b6 SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands. Commands Detail \u00b6 sayn init \u00b6 Initialises a SAYN project in the current working directory. sayn run \u00b6 Runs the whole SAYN project. This command should be run from the project's root. It has the following optional flags which can be cumulated as desired: -t : run specific tasks. -x : exclude specific tasks. -p : select profile to use for run. -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level. sayn run -t \u00b6 You can run specific tasks with the following commands: sayn run -t task_name : run task_name sayn run -t +task_name : run task_name and all its parents. sayn run -t task_name+ : run task_name and all its children. sayn run -t dag:dag_name : run all tasks from the dag dag_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x \u00b6 You can exclude specific tasks from a run with the -x flag. It can be used as follows: sayn run -x task_name : run all tasks except task_name . sayn run -t dag:marketing -x task_name : run all tasks in the marketing DAG except task_name . sayn run -p \u00b6 Runs SAYN using a specific profile. It is used with the same logic than for the -t flag. Please see below some examples: sayn run -p profile_name : runs SAYN using the settings of profile_name . sayn compile \u00b6 Compiles the SAYN code that would be executed. This command should be run from the project's root. The same optional flags than for sayn run apply. dag-image \u00b6 Generates a visualisation of the whole SAYN process. This command should be run from the project's root. This requires graphviz - both the software and the Python package .","title":"Commands"},{"location":"commands/#commands","text":"","title":"Commands"},{"location":"commands/#about","text":"SAYN commands are structured as sayn [command] [additional parameters] . The best way to check SAYN commands is by running sayn --help and sayn [command] --help in your command line. Please see below the available SAYN commands.","title":"About"},{"location":"commands/#commands-detail","text":"","title":"Commands Detail"},{"location":"commands/#sayn-init","text":"Initialises a SAYN project in the current working directory.","title":"sayn init"},{"location":"commands/#sayn-run","text":"Runs the whole SAYN project. This command should be run from the project's root. It has the following optional flags which can be cumulated as desired: -t : run specific tasks. -x : exclude specific tasks. -p : select profile to use for run. -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table. -s : start date for incremental loads. -e : end date for incremental loads. -d : display logs from DEBUG level.","title":"sayn run"},{"location":"commands/#sayn-run-t","text":"You can run specific tasks with the following commands: sayn run -t task_name : run task_name sayn run -t +task_name : run task_name and all its parents. sayn run -t task_name+ : run task_name and all its children. sayn run -t dag:dag_name : run all tasks from the dag dag_name . sayn run -t tag:tag_name run all tasks tagged with tag_name .","title":"sayn run -t"},{"location":"commands/#sayn-run-x","text":"You can exclude specific tasks from a run with the -x flag. It can be used as follows: sayn run -x task_name : run all tasks except task_name . sayn run -t dag:marketing -x task_name : run all tasks in the marketing DAG except task_name .","title":"sayn run -x"},{"location":"commands/#sayn-run-p","text":"Runs SAYN using a specific profile. It is used with the same logic than for the -t flag. Please see below some examples: sayn run -p profile_name : runs SAYN using the settings of profile_name .","title":"sayn run -p"},{"location":"commands/#sayn-compile","text":"Compiles the SAYN code that would be executed. This command should be run from the project's root. The same optional flags than for sayn run apply.","title":"sayn compile"},{"location":"commands/#dag-image","text":"Generates a visualisation of the whole SAYN process. This command should be run from the project's root. This requires graphviz - both the software and the Python package .","title":"dag-image"},{"location":"dags/","text":"DAGs \u00b6 About \u00b6 DAGs (Directed Acyclic Graph) are the essential processes being run by SAYN. They are defined by yaml files stored in the dags folder. Importing a DAG \u00b6 For a DAG to be able to run, it needs to be imported into the dags section of the project.yaml file as follows: project.yaml #... dags : - base #... Please note that you should not add the .yaml extension when importing a DAG within the project.yaml file. Creating a DAG \u00b6 Please see below an example DAG file: dag.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data # ... Each DAG file requires the following to be defined: tasks : the set of tasks that compose the DAG. For more details on tasks , please see the Tasks section. In addition, DAG files can define the following: presets : defines preset task structures so task can inherit attributes from those presets directly. presets defined within DAG files can inherit from preset defined at the project level in project.yaml . See the Presets section for more details.","title":"DAGs"},{"location":"dags/#dags","text":"","title":"DAGs"},{"location":"dags/#about","text":"DAGs (Directed Acyclic Graph) are the essential processes being run by SAYN. They are defined by yaml files stored in the dags folder.","title":"About"},{"location":"dags/#importing-a-dag","text":"For a DAG to be able to run, it needs to be imported into the dags section of the project.yaml file as follows: project.yaml #... dags : - base #... Please note that you should not add the .yaml extension when importing a DAG within the project.yaml file.","title":"Importing a DAG"},{"location":"dags/#creating-a-dag","text":"Please see below an example DAG file: dag.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data # ... Each DAG file requires the following to be defined: tasks : the set of tasks that compose the DAG. For more details on tasks , please see the Tasks section. In addition, DAG files can define the following: presets : defines preset task structures so task can inherit attributes from those presets directly. presets defined within DAG files can inherit from preset defined at the project level in project.yaml . See the Presets section for more details.","title":"Creating a DAG"},{"location":"parameters/","text":"Parameters \u00b6 About \u00b6 parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and some YAML files. parameters can also be accessed in python tasks. Defining Parameters \u00b6 parameters are defined at several levels: project.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from project.yaml . presets : a preset can set a parameters attribute so all tasks within that preset inherit those. tasks : a task can set a parameters attribute. Accessing Parameters \u00b6 For the below section, we consider a project with the following setup: project.yaml # ... parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models # ... settings.yaml # ... default_profile : dev profiles : dev : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : # ... parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models In tasks \u00b6 Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : '{{schema_staging}}' schema : '{{schema_models}}' table : '{{user_prefix}}{{task.name}}' Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param In presets \u00b6 parameters can be accessed in presets in the same way than they are accessed in tasks . For example, you could have the following modelling preset definition: presets : modelling : type : autosql materialisation : table destination : tmp_schema : '{{schema_staging}}' schema : '{{schema_models}}' table : '{{user_prefix}}{{task.name}}' The interpretation of this preset will work as in the above section, using parameters from the relevant profile at execution time. In SQL Queries \u00b6 For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter_name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt In Python Tasks \u00b6 parameters can be accessed in python tasks via the SAYN API as they are stored on the Task object: self.sayn_config.parameters : accesses the project parameters set in project.yaml and settings.yaml . self.parameters : accesses the task's parameters . For example, you could have the following python task code to access your project parameters: from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup err = False if err : return self . failed () else : return self . ready () def run ( self ): err = False sayn_params = self . sayn_config . parameters #code you want to run if err : return self . failed () else : return self . success ()","title":"Parameters"},{"location":"parameters/#parameters","text":"","title":"Parameters"},{"location":"parameters/#about","text":"parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and some YAML files. parameters can also be accessed in python tasks.","title":"About"},{"location":"parameters/#defining-parameters","text":"parameters are defined at several levels: project.yaml file: defines the project's default parameters and their values. settings.yaml file: defines profiles and their parameters. The parameter values of a profile will override the project's default parameters from project.yaml . presets : a preset can set a parameters attribute so all tasks within that preset inherit those. tasks : a task can set a parameters attribute.","title":"Defining Parameters"},{"location":"parameters/#accessing-parameters","text":"For the below section, we consider a project with the following setup: project.yaml # ... parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models # ... settings.yaml # ... default_profile : dev profiles : dev : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : # ... parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models","title":"Accessing Parameters"},{"location":"parameters/#in-tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : '{{schema_staging}}' schema : '{{schema_models}}' table : '{{user_prefix}}{{task.name}}' Note: the table setting uses {{task.name}} . This is because the task object is in the Jinja environment and you can therefore access any task attribute. In this case, {{task.name}} is task_autosql_param . When running sayn run -t task_autosql_param , this would be interpreted as (SAYN uses the default_profile by default): task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If the user desires to run with production parameters, this can be done by leveraging the profile flag: sayn run -t task_autosql_param -p prod . This would therefore use the prod profile parameters and interpret the above block as follows: task_autosql_param : file_name : task_autosql_param.sql type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param","title":"In tasks"},{"location":"parameters/#in-presets","text":"parameters can be accessed in presets in the same way than they are accessed in tasks . For example, you could have the following modelling preset definition: presets : modelling : type : autosql materialisation : table destination : tmp_schema : '{{schema_staging}}' schema : '{{schema_models}}' table : '{{user_prefix}}{{task.name}}' The interpretation of this preset will work as in the above section, using parameters from the relevant profile at execution time.","title":"In presets"},{"location":"parameters/#in-sql-queries","text":"For SQL related tasks ( autosql , sql ), parameters can be accessed in SQL queries with the following syntax: {{parameter_name}} . For example, task_autosql_param defined above could refer to the following query: sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt","title":"In SQL Queries"},{"location":"parameters/#in-python-tasks","text":"parameters can be accessed in python tasks via the SAYN API as they are stored on the Task object: self.sayn_config.parameters : accesses the project parameters set in project.yaml and settings.yaml . self.parameters : accesses the task's parameters . For example, you could have the following python task code to access your project parameters: from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup err = False if err : return self . failed () else : return self . ready () def run ( self ): err = False sayn_params = self . sayn_config . parameters #code you want to run if err : return self . failed () else : return self . success ()","title":"In Python Tasks"},{"location":"presets/","text":"Presets \u00b6 About \u00b6 presets enable to define preset tasks which can be used when defining tasks in DAGs. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition. Defining a preset \u00b6 presets are defined in a similar way than tasks within each individual DAG file. Please see below an example of a preset definition: presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a modelling preset. Every task referring to it will be an autosql task and inherit all other attributes from the modelling preset. Defining tasks using presets \u00b6 tasks can inherit attributes directly from presets . In order to do so, specify the preset attribute on the task. The below example illustrates this by setting the modelling preset defined above on a task : tasks : #... task_preset : preset : modelling #other task properties #... Defining project-level presets \u00b6 If you use the same preset across multiple DAGs, you can avoid this repetition by defining a project-level preset . For example, if you use the modelling preset defined above across all DAGs of your SAYN project, you can define it directly in project.yaml in a similar way: project.yaml # ... presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' # ... You can then use that project-level preset to define presets within individual DAGs as follows: dag.yaml presets : modelling : preset : modelling # ...","title":"Presets"},{"location":"presets/#presets","text":"","title":"Presets"},{"location":"presets/#about","text":"presets enable to define preset tasks which can be used when defining tasks in DAGs. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition.","title":"About"},{"location":"presets/#defining-a-preset","text":"presets are defined in a similar way than tasks within each individual DAG file. Please see below an example of a preset definition: presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a modelling preset. Every task referring to it will be an autosql task and inherit all other attributes from the modelling preset.","title":"Defining a preset"},{"location":"presets/#defining-tasks-using-presets","text":"tasks can inherit attributes directly from presets . In order to do so, specify the preset attribute on the task. The below example illustrates this by setting the modelling preset defined above on a task : tasks : #... task_preset : preset : modelling #other task properties #...","title":"Defining tasks using presets"},{"location":"presets/#defining-project-level-presets","text":"If you use the same preset across multiple DAGs, you can avoid this repetition by defining a project-level preset . For example, if you use the modelling preset defined above across all DAGs of your SAYN project, you can define it directly in project.yaml in a similar way: project.yaml # ... presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' # ... You can then use that project-level preset to define presets within individual DAGs as follows: dag.yaml presets : modelling : preset : modelling # ...","title":"Defining project-level presets"},{"location":"project_structure/","text":"SAYN Project Structure \u00b6 SAYN projects are structured as follows: project_name compile/ #only appears after first run dags/ dag.yaml logs/ #only appears after first run sayn.log python/ __init__.py task_1.py sql/ task_2.sql task_3.sql task_4.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where dag files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Structure"},{"location":"project_structure/#sayn-project-structure","text":"SAYN projects are structured as follows: project_name compile/ #only appears after first run dags/ dag.yaml logs/ #only appears after first run sayn.log python/ __init__.py task_1.py sql/ task_2.sql task_3.sql task_4.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where dag files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"SAYN Project Structure"},{"location":"api/database/","text":"\u00b6 Database \u00b6 Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description * engine (sqlalchemy.Engine A sqlalchemy engine referencing the database * name (str Name of the db as defined in required_credentials in project.yaml * name_in_yaml (str Name of db under credentials in settings.yaml * db_type (str Type of the database create_indexes ( self , table , schema , ddl ) \u00b6 Returns SQL to create indexes from ddl. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for the CREATE INDEX statements create_table_ddl ( self , table , schema , ddl , replace = False ) \u00b6 Returns SQL code for a create table from a select statment. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required replace bool Issue a DROP statement False Returns: Type Description str A SQL script for the CREATE TABLE statement create_table_select ( self , table , schema , select , replace = False , view = False , ddl = {}) \u00b6 Returns SQL code for a create table from a select statment. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required select str A SQL SELECT query to build the table with required replace bool Issue a DROP statement False view bool Indicates if the object to create is a view. Defaults to creating a table False Returns: Type Description str A SQL script for the CREATE...AS drop_table ( self , table , schema , view = False ) \u00b6 Returns a DROP statement. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required view bool Indicates if the object to drop is a view. Defaults to dropping a table False Returns: Type Description str A SQL script for the DROP statements execute ( self , script ) \u00b6 Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required get_table ( self , table , schema , columns = None , required_existing = False ) \u00b6 Create a SQLAlchemy Table object. Parameters: Name Type Description Default table str The table name required schema str The schema or None required columns list A list of column names to build the table object None required_existing bool If True and columns is not None, fills up the table columns with the specification in columns False Returns: Type Description sqlalchemy.Table A table object from sqlalchemy grant_permissions ( self , table , schema , ddl ) \u00b6 Returns a set of GRANT statments. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for the GRANT statements insert ( self , table , schema , select ) \u00b6 Returns an INSERT statment from a SELECT query. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required select str The SELECT statement to issue required Returns: Type Description str A SQL script for the INSERT statement load_data ( self , table , schema , data ) \u00b6 Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required data list A list of dictionaries to load required merge_tables ( self , src_table , src_schema , dst_table , dst_schema , delete_key ) \u00b6 Returns SQL to merge data in incremental loads. Note Data merge is performed by issuing these statements: DELETE from target WHERE data exists in source INSERT into target SELECT * from source Parameters: Name Type Description Default src_table str The source table name required src_schema str The source schema or None required dst_table str The target table name required dst_schema str The target schema or None required delete_key str The column name to use for deleting records from the target table required Returns: Type Description str A SQL script for moving the table move_table ( self , src_table , src_schema , dst_table , dst_schema , ddl ) \u00b6 Returns SQL code to rename a table and change schema. Note Table movement is performed as a series of ALTER statments: ALTER TABLE RENAME ALTER TABLE SET SCHEMA (if the database supports it) ALTER INDEX RENAME (to ensure consistency in the naming). Index names are taken from the ddl field Parameters: Name Type Description Default src_table str The source table name required src_schema str The source schema or None required dst_table str The target table name required dst_schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for moving the table refresh_metadata ( self , only = None , schema = None ) \u00b6 Refreshes the sqlalchemy metadata object. Parameters: Name Type Description Default only list A list of object names to filter the refresh on None schema str The schema name to filter on the refresh None select ( self , query , ** params ) \u00b6 Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query select_stream ( self , query , ** params ) \u00b6 Executes the query and returns an iterator dictionaries with the data. The main difference with select() is that this method executes the query with a server-side cursor (sqlalchemy stream_results = True). Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query validate_columns ( self , columns , ** kwargs ) \u00b6 Validates the columns definition for a task. A column definition can be in the formats: * str: indicates the column name * dict: specificies name, type and other properties supported by the database driver Parameters: Name Type Description Default columns list A list of column definitions required Returns: Type Description list A list of dictionaries with the column definition in a dict format or None if there was an error during validation validate_indexes ( self , indexes , ** kwargs ) \u00b6 Validates the indexes definition for a task. Parameters: Name Type Description Default indexes dict A dictionary of indexes with the column list required Returns: Type Description list A dictionary with the index definition or None if there was an error during validation validate_permissions ( self , permissions , ** kwargs ) \u00b6 Validates the permissions definition for a task. Parameters: Name Type Description Default permissions dict A dictionary in the role -> grant format required Returns: Type Description list A dictionary with the grant list or None if there was an error during validation","title":"Database"},{"location":"api/database/#sayn.database.database","text":"","title":"sayn.database.database"},{"location":"api/database/#sayn.database.database.Database","text":"Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description * engine (sqlalchemy.Engine A sqlalchemy engine referencing the database * name (str Name of the db as defined in required_credentials in project.yaml * name_in_yaml (str Name of db under credentials in settings.yaml * db_type (str Type of the database","title":"Database"},{"location":"api/database/#sayn.database.database.Database.create_indexes","text":"Returns SQL to create indexes from ddl. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for the CREATE INDEX statements","title":"create_indexes()"},{"location":"api/database/#sayn.database.database.Database.create_table_ddl","text":"Returns SQL code for a create table from a select statment. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required replace bool Issue a DROP statement False Returns: Type Description str A SQL script for the CREATE TABLE statement","title":"create_table_ddl()"},{"location":"api/database/#sayn.database.database.Database.create_table_select","text":"Returns SQL code for a create table from a select statment. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required select str A SQL SELECT query to build the table with required replace bool Issue a DROP statement False view bool Indicates if the object to create is a view. Defaults to creating a table False Returns: Type Description str A SQL script for the CREATE...AS","title":"create_table_select()"},{"location":"api/database/#sayn.database.database.Database.drop_table","text":"Returns a DROP statement. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required view bool Indicates if the object to drop is a view. Defaults to dropping a table False Returns: Type Description str A SQL script for the DROP statements","title":"drop_table()"},{"location":"api/database/#sayn.database.database.Database.execute","text":"Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required","title":"execute()"},{"location":"api/database/#sayn.database.database.Database.get_table","text":"Create a SQLAlchemy Table object. Parameters: Name Type Description Default table str The table name required schema str The schema or None required columns list A list of column names to build the table object None required_existing bool If True and columns is not None, fills up the table columns with the specification in columns False Returns: Type Description sqlalchemy.Table A table object from sqlalchemy","title":"get_table()"},{"location":"api/database/#sayn.database.database.Database.grant_permissions","text":"Returns a set of GRANT statments. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for the GRANT statements","title":"grant_permissions()"},{"location":"api/database/#sayn.database.database.Database.insert","text":"Returns an INSERT statment from a SELECT query. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required select str The SELECT statement to issue required Returns: Type Description str A SQL script for the INSERT statement","title":"insert()"},{"location":"api/database/#sayn.database.database.Database.load_data","text":"Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required data list A list of dictionaries to load required","title":"load_data()"},{"location":"api/database/#sayn.database.database.Database.merge_tables","text":"Returns SQL to merge data in incremental loads. Note Data merge is performed by issuing these statements: DELETE from target WHERE data exists in source INSERT into target SELECT * from source Parameters: Name Type Description Default src_table str The source table name required src_schema str The source schema or None required dst_table str The target table name required dst_schema str The target schema or None required delete_key str The column name to use for deleting records from the target table required Returns: Type Description str A SQL script for moving the table","title":"merge_tables()"},{"location":"api/database/#sayn.database.database.Database.move_table","text":"Returns SQL code to rename a table and change schema. Note Table movement is performed as a series of ALTER statments: ALTER TABLE RENAME ALTER TABLE SET SCHEMA (if the database supports it) ALTER INDEX RENAME (to ensure consistency in the naming). Index names are taken from the ddl field Parameters: Name Type Description Default src_table str The source table name required src_schema str The source schema or None required dst_table str The target table name required dst_schema str The target schema or None required ddl dict A ddl task definition required Returns: Type Description str A SQL script for moving the table","title":"move_table()"},{"location":"api/database/#sayn.database.database.Database.refresh_metadata","text":"Refreshes the sqlalchemy metadata object. Parameters: Name Type Description Default only list A list of object names to filter the refresh on None schema str The schema name to filter on the refresh None","title":"refresh_metadata()"},{"location":"api/database/#sayn.database.database.Database.select","text":"Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"select()"},{"location":"api/database/#sayn.database.database.Database.select_stream","text":"Executes the query and returns an iterator dictionaries with the data. The main difference with select() is that this method executes the query with a server-side cursor (sqlalchemy stream_results = True). Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"select_stream()"},{"location":"api/database/#sayn.database.database.Database.validate_columns","text":"Validates the columns definition for a task. A column definition can be in the formats: * str: indicates the column name * dict: specificies name, type and other properties supported by the database driver Parameters: Name Type Description Default columns list A list of column definitions required Returns: Type Description list A list of dictionaries with the column definition in a dict format or None if there was an error during validation","title":"validate_columns()"},{"location":"api/database/#sayn.database.database.Database.validate_indexes","text":"Validates the indexes definition for a task. Parameters: Name Type Description Default indexes dict A dictionary of indexes with the column list required Returns: Type Description list A dictionary with the index definition or None if there was an error during validation","title":"validate_indexes()"},{"location":"api/database/#sayn.database.database.Database.validate_permissions","text":"Validates the permissions definition for a task. Parameters: Name Type Description Default permissions dict A dictionary in the role -> grant format required Returns: Type Description list A dictionary with the grant list or None if there was an error during validation","title":"validate_permissions()"},{"location":"databases/mysql/","text":"Mysql \u00b6 SAYN will consider the following parameters to construct the sqlalchemy url: host user password port database Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : mysql-conn : type : mysql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters database : [ database_name ] ... Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/mysql/#mysql","text":"SAYN will consider the following parameters to construct the sqlalchemy url: host user password port database Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : mysql-conn : type : mysql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters database : [ database_name ] ... Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"Mysql"},{"location":"databases/overview/","text":"Databases \u00b6 About \u00b6 SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Redshift Snowflake PostgreSQL Mysql SQLite Usage \u00b6 Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify a connection timeout for a PostgreSQL connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : postgresql host : ... # other connection parameters connect_args : connect_timeout : 100 Using databases in Python tasks \u00b6 Databases defined in the SAYN project are available to Python tasks via Config.dbs . For convenience though, all Python tasks have a default_db . Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup def run ( self ): data = self . default_db . select ( \"SELECT * FROM test_table\" ) #code you want to run Database class \u00b6 \u00b6 Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description * engine (sqlalchemy.Engine A sqlalchemy engine referencing the database * name (str Name of the db as defined in required_credentials in project.yaml * name_in_yaml (str Name of db under credentials in settings.yaml * db_type (str Type of the database execute ( self , script ) \u00b6 Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required load_data ( self , table , schema , data ) \u00b6 Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required data list A list of dictionaries to load required select ( self , query , ** params ) \u00b6 Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"Overview"},{"location":"databases/overview/#databases","text":"","title":"Databases"},{"location":"databases/overview/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Redshift Snowflake PostgreSQL Mysql SQLite","title":"About"},{"location":"databases/overview/#usage","text":"Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify a connection timeout for a PostgreSQL connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : postgresql host : ... # other connection parameters connect_args : connect_timeout : 100","title":"Usage"},{"location":"databases/overview/#using-databases-in-python-tasks","text":"Databases defined in the SAYN project are available to Python tasks via Config.dbs . For convenience though, all Python tasks have a default_db . Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup def run ( self ): data = self . default_db . select ( \"SELECT * FROM test_table\" ) #code you want to run","title":"Using databases in Python tasks"},{"location":"databases/overview/#database-class","text":"","title":"Database class"},{"location":"databases/overview/#sayn.database.database.Database","text":"Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description * engine (sqlalchemy.Engine A sqlalchemy engine referencing the database * name (str Name of the db as defined in required_credentials in project.yaml * name_in_yaml (str Name of db under credentials in settings.yaml * db_type (str Type of the database","title":"sayn.database.database.Database"},{"location":"databases/overview/#sayn.database.database.Database.execute","text":"Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required","title":"execute()"},{"location":"databases/overview/#sayn.database.database.Database.load_data","text":"Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required data list A list of dictionaries to load required","title":"load_data()"},{"location":"databases/overview/#sayn.database.database.Database.select","text":"Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"select()"},{"location":"databases/postgresql/","text":"PostgreSQL \u00b6 SAYN will consider the following parameters to construct the sqlalchemy url: host port user password dbname Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : postgresql-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] ... Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/postgresql/#postgresql","text":"SAYN will consider the following parameters to construct the sqlalchemy url: host port user password dbname Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : postgresql-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] ... Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/redshift/","text":"Redshift \u00b6 Connecting with regular users \u00b6 SAYN will consider the following parameters to construct the sqlalchemy url: host port user password dbname Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : redshift-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] ... Check the sqlalchemy psycopg2 dialect for extra parameters. Connecting with IAM \u00b6 You can connect to Redshift with SAYN through IAM users. In order to do so, add cluster_id to your credentials in settings.yaml . SAYN will use boto3 to get a temporary password using AWS IAM. settings.yaml ... credentials : redshift-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] cluster_id : [ redshift-cluster-id ] ... For this connection methodology to work: boto3 needs to be installed in the project virtual environment pip install boto3 . AWS credentials need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters .","title":"Redshift"},{"location":"databases/redshift/#redshift","text":"","title":"Redshift"},{"location":"databases/redshift/#connecting-with-regular-users","text":"SAYN will consider the following parameters to construct the sqlalchemy url: host port user password dbname Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : redshift-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] ... Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"Connecting with regular users"},{"location":"databases/redshift/#connecting-with-iam","text":"You can connect to Redshift with SAYN through IAM users. In order to do so, add cluster_id to your credentials in settings.yaml . SAYN will use boto3 to get a temporary password using AWS IAM. settings.yaml ... credentials : redshift-conn : type : postgresql host : [ host ] port : [ port ] user : [ username ] password : '[password]' #use quotes to avoid conflict with special characters dbname : [ database_name ] cluster_id : [ redshift-cluster-id ] ... For this connection methodology to work: boto3 needs to be installed in the project virtual environment pip install boto3 . AWS credentials need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters .","title":"Connecting with IAM"},{"location":"databases/snowflake/","text":"Snowflake \u00b6 SAYN will consider the following parameters to construct the sqlalchemy url: account region user password database warehouse role schema host port Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : snowflake-conn : type : snowflake account : [ account ] user : [ username ] role : [ user_role ] password : '[password]' #use quotes to avoid conflict with special characters database : [ database_name ] ... Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/snowflake/#snowflake","text":"SAYN will consider the following parameters to construct the sqlalchemy url: account region user password database warehouse role schema host port Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml ... credentials : snowflake-conn : type : snowflake account : [ account ] user : [ username ] role : [ user_role ] password : '[password]' #use quotes to avoid conflict with special characters database : [ database_name ] ... Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/sqlite/","text":"SQLite \u00b6 SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy sqlite dialect for extra parameters.","title":"SQLite"},{"location":"databases/sqlite/#sqlite","text":"SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy sqlite dialect for extra parameters.","title":"SQLite"},{"location":"settings/project_yaml/","text":"Settings: project.yaml \u00b6 Role \u00b6 The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . Content \u00b6 Please see below and example of project.yaml file: project.yaml default_db : warehouse required_credentials : - warehouse dags : - base parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The project.yaml file requires the following to be defined: default_db : the database use at run time. required_credentials : the required credentials to run the project. Credentials details are defined the settings.yaml file. dags : the DAGs of the project (this example only imports one DAG). Those DAGs contain the tasks. In addition, the project.yaml file can define the following in order to make the SAYN project more dynamic and efficient: parameters : those parameters are used to make the tasks dynamic. They are overwritten by parameters in settings.yaml . See the Parameters section for more details. presets : defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"project.yaml"},{"location":"settings/project_yaml/#settings-projectyaml","text":"","title":"Settings: project.yaml"},{"location":"settings/project_yaml/#role","text":"The project.yaml defines the core components of the SAYN project. It is shared across all collaborators .","title":"Role"},{"location":"settings/project_yaml/#content","text":"Please see below and example of project.yaml file: project.yaml default_db : warehouse required_credentials : - warehouse dags : - base parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The project.yaml file requires the following to be defined: default_db : the database use at run time. required_credentials : the required credentials to run the project. Credentials details are defined the settings.yaml file. dags : the DAGs of the project (this example only imports one DAG). Those DAGs contain the tasks. In addition, the project.yaml file can define the following in order to make the SAYN project more dynamic and efficient: parameters : those parameters are used to make the tasks dynamic. They are overwritten by parameters in settings.yaml . See the Parameters section for more details. presets : defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"Content"},{"location":"settings/settings_yaml/","text":"Settings: settings.yaml \u00b6 Role \u00b6 The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) . Content \u00b6 Overview \u00b6 Please see below and example of settings.yaml file: settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from models.yaml credentials : snowflake-songoku : type : snowflake connect_args : account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake connect_args : account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] The settings.yaml file requires the following to be defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . credentials : the list of credentials for the user. As can be observed, this file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production. About Credentials \u00b6 The credentials section of the settings.yaml file is used to store both databases and API credentials. Databases \u00b6 SAYN supports various databases. In order to create a database credential, define the type as one of the supported databases (see the Database section for more details) and the connection parameters relevant to the database type. APIs \u00b6 In order to define API credentials, use the type: api and pass the API connection parameters. Please see an example below: # ... credentials : # ... credential_name : type : api api_key : 'api_key' Those API credentials can then be accessed in python tasks through the Task object.","title":"settings.yaml"},{"location":"settings/settings_yaml/#settings-settingsyaml","text":"","title":"Settings: settings.yaml"},{"location":"settings/settings_yaml/#role","text":"The settings.yaml file is used for individual settings to run the SAYN project. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git (it should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project) .","title":"Role"},{"location":"settings/settings_yaml/#content","text":"","title":"Content"},{"location":"settings/settings_yaml/#overview","text":"Please see below and example of settings.yaml file: settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from models.yaml credentials : snowflake-songoku : type : snowflake connect_args : account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake connect_args : account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] The settings.yaml file requires the following to be defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . credentials : the list of credentials for the user. As can be observed, this file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production.","title":"Overview"},{"location":"settings/settings_yaml/#about-credentials","text":"The credentials section of the settings.yaml file is used to store both databases and API credentials.","title":"About Credentials"},{"location":"settings/settings_yaml/#databases","text":"SAYN supports various databases. In order to create a database credential, define the type as one of the supported databases (see the Database section for more details) and the connection parameters relevant to the database type.","title":"Databases"},{"location":"settings/settings_yaml/#apis","text":"In order to define API credentials, use the type: api and pass the API connection parameters. Please see an example below: # ... credentials : # ... credential_name : type : api api_key : 'api_key' Those API credentials can then be accessed in python tasks through the Task object.","title":"APIs"},{"location":"tasks/autosql/","text":"autosql Task \u00b6 About \u00b6 The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks In models.yaml \u00b6 An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : specifies the schema which will be used to store any necessary temporary object created in the process. This is optional. schema : is the destination schema where the object will be created. This is optional. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Using autosql In incremental Mode \u00b6 If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require a delete_key to be set. Please see below an example: autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : - dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table. Defining DDLs \u00b6 You can define the following DDL parameters in the autosql task definition: indexes primary_key autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 ...","title":"autosql"},{"location":"tasks/autosql/#autosql-task","text":"","title":"autosql Task"},{"location":"tasks/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/autosql/#defining-autosql-tasks-in-modelsyaml","text":"An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : specifies the schema which will be used to store any necessary temporary object created in the process. This is optional. schema : is the destination schema where the object will be created. This is optional. table : is the name of the object that will be created. delete_key : specifies the incremental process delete key. This is for incremental materialisation only.","title":"Defining autosql Tasks In models.yaml"},{"location":"tasks/autosql/#using-autosql-in-incremental-mode","text":"If you do not want to have a full refresh of your tables, you can use the autosql task with incremental materialisation . This is extremely useful for large data volumes when full refresh would be too long. SAYN autosql tasks with incremental materialisation require a delete_key to be set. Please see below an example: autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : - dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete rows from the target table that are found in the temporary table based on the delete_key . Load the temporary table in the destination table.","title":"Using autosql In incremental Mode"},{"location":"tasks/autosql/#defining-ddls","text":"You can define the following DDL parameters in the autosql task definition: indexes primary_key autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 ...","title":"Defining DDLs"},{"location":"tasks/copy/","text":"copy Task \u00b6 About \u00b6 The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Defining copy Tasks In models.yaml \u00b6 A copy task is defined as follows: task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name ddl : columns : - name : column incremental_key : column delete_key : column copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. destination : the destination details. The destination database is the default_db set in models.yaml . tmp_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"copy"},{"location":"tasks/copy/#copy-task","text":"","title":"copy Task"},{"location":"tasks/copy/#about","text":"The copy task automatically copies data from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse.","title":"About"},{"location":"tasks/copy/#defining-copy-tasks-in-modelsyaml","text":"A copy task is defined as follows: task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name ddl : columns : - name : column incremental_key : column delete_key : column copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database, this should be part of the required_credentials in models.yaml schema : the source schema. table : the source table. destination : the destination details. The destination database is the default_db set in models.yaml . tmp_schema : the staging schema used in the process of copying data. schema : the destination schema. table : the destination schema. ddl : setting the DDL of the process columns : a list of columns to export The following parameters are optional: incremental_key : the column which will be used for incremental loads. The process will transfer any data with an incremental_key value superior to the maximum found in the source table. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value superior to the maximum found in the source table.","title":"Defining copy Tasks In models.yaml"},{"location":"tasks/dummy/","text":"dummy Task \u00b6 About \u00b6 The dummy is a task that does not do anything. It is mostly used as a connector between tasks. Defining dummy Tasks In models.yaml \u00b6 A dummy task is defined as follows: task_dummy : type : dummy This task does not require any other setting than its type . Usage \u00b6 dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in your DAG dag.yaml : tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"dummy"},{"location":"tasks/dummy/#dummy-task","text":"","title":"dummy Task"},{"location":"tasks/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a connector between tasks.","title":"About"},{"location":"tasks/dummy/#defining-dummy-tasks-in-modelsyaml","text":"A dummy task is defined as follows: task_dummy : type : dummy This task does not require any other setting than its type .","title":"Defining dummy Tasks In models.yaml"},{"location":"tasks/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of similar parents. Let's consider the following setup in your DAG dag.yaml : tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeting the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Usage"},{"location":"tasks/overview/","text":"Tasks \u00b6 About \u00b6 Tasks are the core components of your SAYN DAGs (Directed Acyclic Graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on the parents you define. Defining Tasks In DAGs \u00b6 Tasks are defined in individual DAGs . Their definition is composed of several attributes. type \u00b6 A task is defined by its type - type is the only attribute shared by all tasks. Below is a simple example of a SAYN task definition: dag.yaml task_1 : type : sql file_name : task_1.sql Please see the Task Types section on this page for the list of all task types. parents \u00b6 If a task is dependent upon another task, it can define parents . A task can have as many parents as desired. Please see below an example which shows how to define parents : dag.yaml task_2 : type : sql file_name : task_2.sql parents : - task_1 parameters \u00b6 Enables to set some parameters at the task level. Those can then be accessed in the task. This is defined as follows: dag.yaml task_3 : type : sql file_name : task_3.sql parameters : field_select : 'a, b, c' For more details on parameters , please see the Parameters section. presets \u00b6 presets enable you to define some standard tasks. Other tasks can then inherit attributes from the pre-defined presets . This is defined as follows: dag.yaml task_4 : #this task would inherit everything from the modelling preset preset : modelling For more details on presets , please see the Presets section. tags \u00b6 Tasks can define a tags attribute - you can define as many tags as desired on a task. Those tags can be used in order to run only tasks which are defined with a specific tag. This is useful to group several tasks across multiple DAGs under one structure. Please see below how to define tags on a task: dag.yaml task_5 : type : python class : my_module.MyClass tags : - extract type specific attributes \u00b6 Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it. Task Types \u00b6 Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"Tasks are the core components of your SAYN DAGs (Directed Acyclic Graph). Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Tasks will be executed in an order based on the parents you define.","title":"About"},{"location":"tasks/overview/#defining-tasks-in-dags","text":"Tasks are defined in individual DAGs . Their definition is composed of several attributes.","title":"Defining Tasks In DAGs"},{"location":"tasks/overview/#type","text":"A task is defined by its type - type is the only attribute shared by all tasks. Below is a simple example of a SAYN task definition: dag.yaml task_1 : type : sql file_name : task_1.sql Please see the Task Types section on this page for the list of all task types.","title":"type"},{"location":"tasks/overview/#parents","text":"If a task is dependent upon another task, it can define parents . A task can have as many parents as desired. Please see below an example which shows how to define parents : dag.yaml task_2 : type : sql file_name : task_2.sql parents : - task_1","title":"parents"},{"location":"tasks/overview/#parameters","text":"Enables to set some parameters at the task level. Those can then be accessed in the task. This is defined as follows: dag.yaml task_3 : type : sql file_name : task_3.sql parameters : field_select : 'a, b, c' For more details on parameters , please see the Parameters section.","title":"parameters"},{"location":"tasks/overview/#presets","text":"presets enable you to define some standard tasks. Other tasks can then inherit attributes from the pre-defined presets . This is defined as follows: dag.yaml task_4 : #this task would inherit everything from the modelling preset preset : modelling For more details on presets , please see the Presets section.","title":"presets"},{"location":"tasks/overview/#tags","text":"Tasks can define a tags attribute - you can define as many tags as desired on a task. Those tags can be used in order to run only tasks which are defined with a specific tag. This is useful to group several tasks across multiple DAGs under one structure. Please see below how to define tags on a task: dag.yaml task_5 : type : python class : my_module.MyClass tags : - extract","title":"tags"},{"location":"tasks/overview/#type-specific-attributes","text":"Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it.","title":"type specific attributes"},{"location":"tasks/overview/#task-types","text":"Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Task Types"},{"location":"tasks/python/","text":"python Task \u00b6 About \u00b6 The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. Defining python Tasks In models.yaml \u00b6 A python task is defined as follows: task_python : type : python class : task_python.TaskPython It is defined by the following attributes: type : python . class : the import statement that should be executed to import the Python Class. Please note that Python tasks scripts should be saved in the python folder within the project's root. Please note that for the python tasks to run, you must have an __init__.py file into the python folder so it is treated as a package. Writing A python Task \u00b6 Basics \u00b6 Please see below an example of a python task: from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup err = False if err : return self . failed () else : return self . ready () def run ( self ): err = False #code you want to run if err : return self . failed () else : return self . success () As you can observe, writing a python task requires the following: import the PythonTask class from sayn . define the class you want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.success() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.success() . Using the SAYN API \u00b6 In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API: self.sayn_config.parameters : accesses project config parameters ( project.yaml , settings.yaml ). For more details on parameters , see the Parameters section. self.sayn_config.api_credentials : dictionary containing the API credentials available for the profile used at run time. self.sayn_config.dbs : dictionary containing database objects specified in the profile used at run time. self.parameters : accesses the task's parameters. self.default_db : accesses the default_db specified in the project.yaml file. Using those parameters is extremely useful in order to tailor your python tasks' code in order to separate between development and production environments. Logging for Python tasks with the SAYN API \u00b6 In order to log for Python tasks, you should use the SAYN API. It can be accessed through the self.logger attribute on the task. It has the following methods: self.logger.print(self, text) : print to console. self.logger.debug(self, text) : debug log to console and file. self.logger.info(self, text) : info log to console and file. self.logger.warning(self, text) : warning log to console and file. self.logger.error(self, text) : error log to console and file.","title":"python"},{"location":"tasks/python/#python-task","text":"","title":"python Task"},{"location":"tasks/python/#about","text":"The python task lets you use Python for your task. It will run a Python Class which you define. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models.","title":"About"},{"location":"tasks/python/#defining-python-tasks-in-modelsyaml","text":"A python task is defined as follows: task_python : type : python class : task_python.TaskPython It is defined by the following attributes: type : python . class : the import statement that should be executed to import the Python Class. Please note that Python tasks scripts should be saved in the python folder within the project's root. Please note that for the python tasks to run, you must have an __init__.py file into the python folder so it is treated as a package.","title":"Defining python Tasks In models.yaml"},{"location":"tasks/python/#writing-a-python-task","text":"","title":"Writing A python Task"},{"location":"tasks/python/#basics","text":"Please see below an example of a python task: from sayn import PythonTask class TaskPython ( PythonTask ): def setup ( self ): #code doing setup err = False if err : return self . failed () else : return self . ready () def run ( self ): err = False #code you want to run if err : return self . failed () else : return self . success () As you can observe, writing a python task requires the following: import the PythonTask class from sayn . define the class you want the task to run. The task should inherit from PythonTask . the class you define can overwrite the following methods (those methods need to follow a specific signature and you should only pass self as argument ): setup : runs when setting up the task. It should return self.failed() if an error has occurred otherwise self.ready() . compile : runs when compiling the task. It should return self.failed() if an error has occurred otherwise self.success() . run : runs when executing the task. It should return self.failed() if an error has occurred otherwise self.success() .","title":"Basics"},{"location":"tasks/python/#using-the-sayn-api","text":"In order, to make your python tasks dynamic based on project settings and profiles, you can use the SAYN API: self.sayn_config.parameters : accesses project config parameters ( project.yaml , settings.yaml ). For more details on parameters , see the Parameters section. self.sayn_config.api_credentials : dictionary containing the API credentials available for the profile used at run time. self.sayn_config.dbs : dictionary containing database objects specified in the profile used at run time. self.parameters : accesses the task's parameters. self.default_db : accesses the default_db specified in the project.yaml file. Using those parameters is extremely useful in order to tailor your python tasks' code in order to separate between development and production environments.","title":"Using the SAYN API"},{"location":"tasks/python/#logging-for-python-tasks-with-the-sayn-api","text":"In order to log for Python tasks, you should use the SAYN API. It can be accessed through the self.logger attribute on the task. It has the following methods: self.logger.print(self, text) : print to console. self.logger.debug(self, text) : debug log to console and file. self.logger.info(self, text) : info log to console and file. self.logger.warning(self, text) : warning log to console and file. self.logger.error(self, text) : error log to console and file.","title":"Logging for Python tasks with the SAYN API"},{"location":"tasks/sql/","text":"sql Task \u00b6 About \u00b6 The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file. Defining sql Tasks In models.yaml \u00b6 A sql task is defined as follows: task_sql : type : sql file_name : sql_task.sql sql tasks only have one parameter that needs to be set: type : sql . file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"sql"},{"location":"tasks/sql/#sql-task","text":"","title":"sql Task"},{"location":"tasks/sql/#about","text":"The sql task lets you execute any SQL statement. You can have multiple SQL statements within one file.","title":"About"},{"location":"tasks/sql/#defining-sql-tasks-in-modelsyaml","text":"A sql task is defined as follows: task_sql : type : sql file_name : sql_task.sql sql tasks only have one parameter that needs to be set: type : sql . file_name : the name of the file within the sql folder of the project's root. SAYN automatically looks into this folder so there is no need to prepend sql/ to the file_name .","title":"Defining sql Tasks In models.yaml"},{"location":"tutorials/tutorial_part1/","text":"Tutorial: Part 1 \u00b6 What We Will Cover \u00b6 This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project in the folder created by sayn init and creates a small ETL process based on synthetic data. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com . Running SAYN \u00b6 Run the below in order to install SAYN and process your first SAYN run. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run Running sayn run will output logging on your terminal about the process being executed. This is what will happen: SAYN will run all project tasks. Those are all in the DAG file dags/base.yaml . The tasks include: One python task which creates some logs and stores them into several log tables within a test.db SQLite database. This database is created in your project's folder root. Several autosql tasks which create data models including tables and views based on those logs. You can open test.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background. Project Overview \u00b6 The test_sayn folder has the following structure: test_sayn compile/ # only appears after first run dags/ base.yaml logs/ # only appears after first run sayn.log python/ __init__.py load_data.py utils/ __init__.py log_creator.py sql/ dim_arenas.sql dim_fighters.sql dim_tournaments.sql f_battles.sql f_fighter_results.sql f_rankings.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where DAG files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution. Implementing Your Project \u00b6 We will now go through the example project and explain the process of building a SAYN project. You can use the test_sayn folder you created to follow along, all the code is already written there. Step 1: Define the SAYN project with project.yaml \u00b6 Add the project.yaml file at the root level of your directory. Here is the file from the example: project.yaml required_credentials : - warehouse default_db : warehouse dags : - base The following is defined: required_credentials : the required credentials to run the project. Credential details are defined in the settings.yaml file. default_db : the database used at run time. dags : the DAGs of the project (this example has only one dag which can be found at dags/base.yaml ). Those DAGs contain the tasks. Step 2: Define your individual settings with settings.yaml \u00b6 Add the settings.yaml file at the root level of your directory. Here is the file from the example: settings.yaml default_profile : dev profiles : dev : credentials : warehouse : test_db prod : credentials : warehouse : prod_db credentials : test_db : type : sqlite database : test.db prod_db : type : sqlite database : prod.db The following is defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. Here we include the credential details for a dev and a prod profile. credentials : the list of credentials for the user. Step 3: Define your DAG(s) \u00b6 In SAYN, DAGs are defined in yaml files within the dags folder. As seen before, those dags are imported in the project.yaml file in order to be executed. When importing the DAGs in the project.yaml file, you should write the name without the .yaml extension. Our project contains only one DAG: base.yaml . Below is the file: base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : main schema : main table : dim_tournaments parents : - load_data dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : tmp_schema : main schema : main table : dim_arenas parents : - load_data dim_fighters : type : autosql file_name : dim_fighters.sql materialisation : table destination : tmp_schema : main schema : main table : dim_fighters parents : - load_data f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : tmp_schema : main schema : main table : f_battles parents : - load_data - dim_tournaments - dim_arenas - dim_fighters f_fighter_results : type : autosql file_name : f_fighter_results.sql materialisation : table destination : tmp_schema : main schema : main table : f_fighter_results parents : - f_battles f_rankings : type : autosql file_name : f_rankings.sql materialisation : view destination : tmp_schema : main schema : main table : f_rankings parents : - f_fighter_results The following is defined: tasks : the tasks of the DAG. Each task is defined by a type and various properties respective to its type . In our example, we use two task types: python : lets you run a Python process. The load_data.py is our only python task. It creates some synthetic logs and loads them to our database. For more information about how to build python tasks, visit the Python section autosql : lets you write a SELECT statement and SAYN then creates the table or view automatically for you. Our example has multiple autosql tasks which create models based on the logs. For more information about setting up autosql tasks, visit the autosql section . Running Your Project \u00b6 You can now run your SAYN project with the following commands: sayn run : runs the whole project sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : runs the specific task More options are available to run specific components of your SAYN project. All details can be found in the Commands section. What Next? \u00b6 You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"Tutorial (Part 1)"},{"location":"tutorials/tutorial_part1/#tutorial-part-1","text":"","title":"Tutorial: Part 1"},{"location":"tutorials/tutorial_part1/#what-we-will-cover","text":"This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project in the folder created by sayn init and creates a small ETL process based on synthetic data. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com .","title":"What We Will Cover"},{"location":"tutorials/tutorial_part1/#running-sayn","text":"Run the below in order to install SAYN and process your first SAYN run. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run Running sayn run will output logging on your terminal about the process being executed. This is what will happen: SAYN will run all project tasks. Those are all in the DAG file dags/base.yaml . The tasks include: One python task which creates some logs and stores them into several log tables within a test.db SQLite database. This database is created in your project's folder root. Several autosql tasks which create data models including tables and views based on those logs. You can open test.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background.","title":"Running SAYN"},{"location":"tutorials/tutorial_part1/#project-overview","text":"The test_sayn folder has the following structure: test_sayn compile/ # only appears after first run dags/ base.yaml logs/ # only appears after first run sayn.log python/ __init__.py load_data.py utils/ __init__.py log_creator.py sql/ dim_arenas.sql dim_fighters.sql dim_tournaments.sql f_battles.sql f_fighter_results.sql f_rankings.sql .gitignore project.yaml readme.md settings.yaml Please see below the role of each component: project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. dags : folder where DAG files are stored. SAYN tasks are defined in those files. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Overview"},{"location":"tutorials/tutorial_part1/#implementing-your-project","text":"We will now go through the example project and explain the process of building a SAYN project. You can use the test_sayn folder you created to follow along, all the code is already written there.","title":"Implementing Your Project"},{"location":"tutorials/tutorial_part1/#step-1-define-the-sayn-project-with-projectyaml","text":"Add the project.yaml file at the root level of your directory. Here is the file from the example: project.yaml required_credentials : - warehouse default_db : warehouse dags : - base The following is defined: required_credentials : the required credentials to run the project. Credential details are defined in the settings.yaml file. default_db : the database used at run time. dags : the DAGs of the project (this example has only one dag which can be found at dags/base.yaml ). Those DAGs contain the tasks.","title":"Step 1: Define the SAYN project with project.yaml"},{"location":"tutorials/tutorial_part1/#step-2-define-your-individual-settings-with-settingsyaml","text":"Add the settings.yaml file at the root level of your directory. Here is the file from the example: settings.yaml default_profile : dev profiles : dev : credentials : warehouse : test_db prod : credentials : warehouse : prod_db credentials : test_db : type : sqlite database : test.db prod_db : type : sqlite database : prod.db The following is defined: default_profile : the profile used by default at execution time. profiles : the list of available profiles to the user. Here we include the credential details for a dev and a prod profile. credentials : the list of credentials for the user.","title":"Step 2: Define your individual settings with settings.yaml"},{"location":"tutorials/tutorial_part1/#step-3-define-your-dags","text":"In SAYN, DAGs are defined in yaml files within the dags folder. As seen before, those dags are imported in the project.yaml file in order to be executed. When importing the DAGs in the project.yaml file, you should write the name without the .yaml extension. Our project contains only one DAG: base.yaml . Below is the file: base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : main schema : main table : dim_tournaments parents : - load_data dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : tmp_schema : main schema : main table : dim_arenas parents : - load_data dim_fighters : type : autosql file_name : dim_fighters.sql materialisation : table destination : tmp_schema : main schema : main table : dim_fighters parents : - load_data f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : tmp_schema : main schema : main table : f_battles parents : - load_data - dim_tournaments - dim_arenas - dim_fighters f_fighter_results : type : autosql file_name : f_fighter_results.sql materialisation : table destination : tmp_schema : main schema : main table : f_fighter_results parents : - f_battles f_rankings : type : autosql file_name : f_rankings.sql materialisation : view destination : tmp_schema : main schema : main table : f_rankings parents : - f_fighter_results The following is defined: tasks : the tasks of the DAG. Each task is defined by a type and various properties respective to its type . In our example, we use two task types: python : lets you run a Python process. The load_data.py is our only python task. It creates some synthetic logs and loads them to our database. For more information about how to build python tasks, visit the Python section autosql : lets you write a SELECT statement and SAYN then creates the table or view automatically for you. Our example has multiple autosql tasks which create models based on the logs. For more information about setting up autosql tasks, visit the autosql section .","title":"Step 3: Define your DAG(s)"},{"location":"tutorials/tutorial_part1/#running-your-project","text":"You can now run your SAYN project with the following commands: sayn run : runs the whole project sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : runs the specific task More options are available to run specific components of your SAYN project. All details can be found in the Commands section.","title":"Running Your Project"},{"location":"tutorials/tutorial_part1/#what-next","text":"You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"What Next?"},{"location":"tutorials/tutorial_part2/","text":"Tutorial: Part 2 \u00b6 What We Will Cover \u00b6 This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com . Implementing parameters and presets \u00b6 The Tutorial: Part 1 got you to implement a first ETL process with SAYN. We will now look into two core SAYN features which will enable you to build efficient and dynamic projects as you scale: parameters and presets . For this tutorial, we will use the code generated by sayn init and make amends as we go through the new concepts. A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work. Step 1: Using parameters \u00b6 parameters Setup \u00b6 You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters to project.yaml . Those parameters will contain the default values for the SAYN project: project.yaml required_credentials : - warehouse default_db : warehouse dags : - base parameters : user_prefix : '' #no prefix for prod Then, add the parameters to your profiles in settings.yaml . settings.yaml default_profile : dev profiles : dev : credentials : warehouse : test_db parameters : user_prefix : sg_ prod : credentials : warehouse : prod_db parameters : user_prefix : '' #no prefix for prod credentials : test_db : type : sqlite database : test.db prod_db : type : sqlite database : prod.db Step 2: Making Tasks Dynamic With parameters \u00b6 Now that our parameters are setup, we can use those to make our tasks' code dynamic. We will now change our tasks' code to use the user_prefix parameter. In python tasks \u00b6 For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils/log_creator.py to see how we use the user_prefix parameter to change the table names. Replace your pyhton/load_data.py script with the code below. As you can observe, we create a user_prefix variable by reading self.sayn_config.parameters . The parameters are stored on the Task object and can therefore be accessed in that way. load_data.py import sqlite3 import logging from .utils.log_creator import prepare_data , generate_load_query from sayn import PythonTask class LoadData ( PythonTask ): def setup ( self ): err = False # we use this list to control how many battles we want per tournament tournament_battles = [ { \"tournament_id\" : 1 , \"n_battles\" : 1000 }, { \"tournament_id\" : 2 , \"n_battles\" : 250 }, { \"tournament_id\" : 3 , \"n_battles\" : 500 }, ] user_prefix = self . sayn_config . parameters [ 'user_prefix' ] try : self . data_to_load = prepare_data ( tournament_battles , user_prefix = user_prefix ) except Exception as e : err = True logging . error ( e ) if err : return self . failed () else : return self . ready () def run ( self ): user_prefix = self . sayn_config . parameters [ 'user_prefix' ] # load the logs for log_type , log_details in self . data_to_load . items (): # create table logging . info ( 'Creating table: {log_type} .' . format ( log_type = log_type )) self . default_db . execute ( log_details [ 'create' ]) # load logs logging . info ( 'Loading logs: {log_type} .' . format ( log_type = log_type )) logs = log_details [ 'data' ] for log in logs : q_insert = generate_load_query ( log_type , log , user_prefix = user_prefix ) self . default_db . execute ( q_insert ) # done logging . info ( 'Done: {log_type} .' . format ( log_type = log_type )) return self . success () In autosql tasks \u00b6 You can also access the project parameters in autosql tasks with the following syntax: {{parameter_name}} . SAYN's compilation process uses Jinja. Replace all the SQL queries with the following: dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l dim_fighters.sql SELECT l . fighter_id , l . fighter_name FROM {{ user_prefix }} logs_fighters l dim_tournaments.sql SELECT l . tournament_id , l . tournament_name FROM {{ user_prefix }} logs_tournaments l f_battles.sql WITH battles AS ( SELECT l . tournament_id , l . battle_id , l . arena_id , l . fighter1_id , l . fighter2_id , l . winner_id FROM {{ user_prefix }} logs_battles l ) SELECT t . tournament_name , t . tournament_name || '-' || CAST ( b . battle_id AS VARCHAR ) AS battle_id , a . arena_name , f1 . fighter_name AS fighter1_name , f2 . fighter_name AS fighter2_name , w . fighter_name AS winner_name FROM battles b LEFT JOIN {{ user_prefix }} dim_tournaments t ON b . tournament_id = t . tournament_id LEFT JOIN {{ user_prefix }} dim_arenas a ON b . arena_id = a . arena_id LEFT JOIN {{ user_prefix }} dim_fighters f1 ON b . fighter1_id = f1 . fighter_id LEFT JOIN {{ user_prefix }} dim_fighters f2 ON b . fighter2_id = f2 . fighter_id LEFT JOIN {{ user_prefix }} dim_fighters w ON b . winner_id = w . fighter_id f_fighter_results.sql WITH fighter1_outcome AS ( SELECT b . tournament_name , b . battle_id , b . arena_name , b . fighter1_name , CASE WHEN b . fighter1_name = b . winner_name THEN 1 ELSE 0 END AS is_winner FROM {{ user_prefix }} f_battles b ) , fighter2_outcome AS ( SELECT b . tournament_name , b . battle_id , b . arena_name , b . fighter2_name , CASE WHEN b . fighter2_name = b . winner_name THEN 1 ELSE 0 END AS is_winner FROM {{ user_prefix }} f_battles b ) SELECT f1 . tournament_name , f1 . battle_id , f1 . arena_name , f1 . fighter1_name AS fighter_name , f1 . is_winner FROM fighter1_outcome f1 UNION SELECT f2 . tournament_name , f2 . battle_id , f2 . arena_name , f2 . fighter2_name AS fighter_name , f2 . is_winner FROM fighter2_outcome f2 f_rankings SELECT fr . fighter_name , CAST ( SUM ( fr . is_winner ) AS FLOAT ) / COUNT ( DISTINCT fr . battle_id ) AS win_rate FROM {{ user_prefix }} f_fighter_results fr GROUP BY 1 ORDER BY 2 DESC Our SQL queries are now all set to read tables which are prefixed with the relevant user_prefix parameter depending on the profile used at execution. However, our autosql tasks still create tables in a static way as we have the table attribute of our autosql tasks hardcoded. The next step will fix this. Step 3: Using presets To Standardise Task Definitions \u00b6 Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets enable to create standardised tasks which can be used to define other tasks by setting a preset attribute. We define a modelling preset and use it in our DAG base.yaml . Replace the file dags/base.yaml with the following: dags/base.yaml presets : modelling : type : autosql file_name : '{{task.name}}.sql' materialisation : table destination : tmp_schema : main schema : main table : '{{user_prefix}}{{task.name}}' parents : - load_data tasks : load_data : type : python class : load_data.LoadData #this task sets modelling as its preset attribute #therefore it inherits all the attributes from the modelling preset dim_tournaments : preset : modelling dim_arenas : preset : modelling dim_fighters : preset : modelling f_battles : preset : modelling parents : - dim_tournaments - dim_arenas - dim_fighters f_fighter_results : preset : modelling parents : - f_battles #for that task, we overwrite the modelling preset materialisation attribute as we want this model to be a view f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Using the modelling preset on tasks' definitions will imply: those tasks will be autosql and materialise as a table . they will all have the load_data task as a parent. and they will have all other attributes set in the modelling preset. Running Our New Project \u00b6 You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a test.db database prefix all tables with sg_ and read from sg_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables We have made our SAYN project dynamic with parameters and made our tasks' definitions more efficients with presets . What Next? \u00b6 This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"Tutorial (Part 2)"},{"location":"tutorials/tutorial_part2/#tutorial-part-2","text":"","title":"Tutorial: Part 2"},{"location":"tutorials/tutorial_part2/#what-we-will-cover","text":"This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. If you need any help or want to ask a question, please reach out to the team at sayn@173tech.com .","title":"What We Will Cover"},{"location":"tutorials/tutorial_part2/#implementing-parameters-and-presets","text":"The Tutorial: Part 1 got you to implement a first ETL process with SAYN. We will now look into two core SAYN features which will enable you to build efficient and dynamic projects as you scale: parameters and presets . For this tutorial, we will use the code generated by sayn init and make amends as we go through the new concepts. A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work.","title":"Implementing parameters and presets"},{"location":"tutorials/tutorial_part2/#step-1-using-parameters","text":"","title":"Step 1: Using parameters"},{"location":"tutorials/tutorial_part2/#parameters-setup","text":"You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters to project.yaml . Those parameters will contain the default values for the SAYN project: project.yaml required_credentials : - warehouse default_db : warehouse dags : - base parameters : user_prefix : '' #no prefix for prod Then, add the parameters to your profiles in settings.yaml . settings.yaml default_profile : dev profiles : dev : credentials : warehouse : test_db parameters : user_prefix : sg_ prod : credentials : warehouse : prod_db parameters : user_prefix : '' #no prefix for prod credentials : test_db : type : sqlite database : test.db prod_db : type : sqlite database : prod.db","title":"parameters Setup"},{"location":"tutorials/tutorial_part2/#step-2-making-tasks-dynamic-with-parameters","text":"Now that our parameters are setup, we can use those to make our tasks' code dynamic. We will now change our tasks' code to use the user_prefix parameter.","title":"Step 2: Making Tasks Dynamic With parameters"},{"location":"tutorials/tutorial_part2/#in-python-tasks","text":"For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils/log_creator.py to see how we use the user_prefix parameter to change the table names. Replace your pyhton/load_data.py script with the code below. As you can observe, we create a user_prefix variable by reading self.sayn_config.parameters . The parameters are stored on the Task object and can therefore be accessed in that way. load_data.py import sqlite3 import logging from .utils.log_creator import prepare_data , generate_load_query from sayn import PythonTask class LoadData ( PythonTask ): def setup ( self ): err = False # we use this list to control how many battles we want per tournament tournament_battles = [ { \"tournament_id\" : 1 , \"n_battles\" : 1000 }, { \"tournament_id\" : 2 , \"n_battles\" : 250 }, { \"tournament_id\" : 3 , \"n_battles\" : 500 }, ] user_prefix = self . sayn_config . parameters [ 'user_prefix' ] try : self . data_to_load = prepare_data ( tournament_battles , user_prefix = user_prefix ) except Exception as e : err = True logging . error ( e ) if err : return self . failed () else : return self . ready () def run ( self ): user_prefix = self . sayn_config . parameters [ 'user_prefix' ] # load the logs for log_type , log_details in self . data_to_load . items (): # create table logging . info ( 'Creating table: {log_type} .' . format ( log_type = log_type )) self . default_db . execute ( log_details [ 'create' ]) # load logs logging . info ( 'Loading logs: {log_type} .' . format ( log_type = log_type )) logs = log_details [ 'data' ] for log in logs : q_insert = generate_load_query ( log_type , log , user_prefix = user_prefix ) self . default_db . execute ( q_insert ) # done logging . info ( 'Done: {log_type} .' . format ( log_type = log_type )) return self . success ()","title":"In python tasks"},{"location":"tutorials/tutorial_part2/#in-autosql-tasks","text":"You can also access the project parameters in autosql tasks with the following syntax: {{parameter_name}} . SAYN's compilation process uses Jinja. Replace all the SQL queries with the following: dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l dim_fighters.sql SELECT l . fighter_id , l . fighter_name FROM {{ user_prefix }} logs_fighters l dim_tournaments.sql SELECT l . tournament_id , l . tournament_name FROM {{ user_prefix }} logs_tournaments l f_battles.sql WITH battles AS ( SELECT l . tournament_id , l . battle_id , l . arena_id , l . fighter1_id , l . fighter2_id , l . winner_id FROM {{ user_prefix }} logs_battles l ) SELECT t . tournament_name , t . tournament_name || '-' || CAST ( b . battle_id AS VARCHAR ) AS battle_id , a . arena_name , f1 . fighter_name AS fighter1_name , f2 . fighter_name AS fighter2_name , w . fighter_name AS winner_name FROM battles b LEFT JOIN {{ user_prefix }} dim_tournaments t ON b . tournament_id = t . tournament_id LEFT JOIN {{ user_prefix }} dim_arenas a ON b . arena_id = a . arena_id LEFT JOIN {{ user_prefix }} dim_fighters f1 ON b . fighter1_id = f1 . fighter_id LEFT JOIN {{ user_prefix }} dim_fighters f2 ON b . fighter2_id = f2 . fighter_id LEFT JOIN {{ user_prefix }} dim_fighters w ON b . winner_id = w . fighter_id f_fighter_results.sql WITH fighter1_outcome AS ( SELECT b . tournament_name , b . battle_id , b . arena_name , b . fighter1_name , CASE WHEN b . fighter1_name = b . winner_name THEN 1 ELSE 0 END AS is_winner FROM {{ user_prefix }} f_battles b ) , fighter2_outcome AS ( SELECT b . tournament_name , b . battle_id , b . arena_name , b . fighter2_name , CASE WHEN b . fighter2_name = b . winner_name THEN 1 ELSE 0 END AS is_winner FROM {{ user_prefix }} f_battles b ) SELECT f1 . tournament_name , f1 . battle_id , f1 . arena_name , f1 . fighter1_name AS fighter_name , f1 . is_winner FROM fighter1_outcome f1 UNION SELECT f2 . tournament_name , f2 . battle_id , f2 . arena_name , f2 . fighter2_name AS fighter_name , f2 . is_winner FROM fighter2_outcome f2 f_rankings SELECT fr . fighter_name , CAST ( SUM ( fr . is_winner ) AS FLOAT ) / COUNT ( DISTINCT fr . battle_id ) AS win_rate FROM {{ user_prefix }} f_fighter_results fr GROUP BY 1 ORDER BY 2 DESC Our SQL queries are now all set to read tables which are prefixed with the relevant user_prefix parameter depending on the profile used at execution. However, our autosql tasks still create tables in a static way as we have the table attribute of our autosql tasks hardcoded. The next step will fix this.","title":"In autosql tasks"},{"location":"tutorials/tutorial_part2/#step-3-using-presets-to-standardise-task-definitions","text":"Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets enable to create standardised tasks which can be used to define other tasks by setting a preset attribute. We define a modelling preset and use it in our DAG base.yaml . Replace the file dags/base.yaml with the following: dags/base.yaml presets : modelling : type : autosql file_name : '{{task.name}}.sql' materialisation : table destination : tmp_schema : main schema : main table : '{{user_prefix}}{{task.name}}' parents : - load_data tasks : load_data : type : python class : load_data.LoadData #this task sets modelling as its preset attribute #therefore it inherits all the attributes from the modelling preset dim_tournaments : preset : modelling dim_arenas : preset : modelling dim_fighters : preset : modelling f_battles : preset : modelling parents : - dim_tournaments - dim_arenas - dim_fighters f_fighter_results : preset : modelling parents : - f_battles #for that task, we overwrite the modelling preset materialisation attribute as we want this model to be a view f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Using the modelling preset on tasks' definitions will imply: those tasks will be autosql and materialise as a table . they will all have the load_data task as a parent. and they will have all other attributes set in the modelling preset.","title":"Step 3: Using presets To Standardise Task Definitions"},{"location":"tutorials/tutorial_part2/#running-our-new-project","text":"You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a test.db database prefix all tables with sg_ and read from sg_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables We have made our SAYN project dynamic with parameters and made our tasks' definitions more efficients with presets .","title":"Running Our New Project"},{"location":"tutorials/tutorial_part2/#what-next","text":"This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"What Next?"}]}