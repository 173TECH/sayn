{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00b6 SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow. Use Cases \u00b6 SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models. Key Features \u00b6 SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation . Design Principles \u00b6 SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start \u00b6 $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power! Support \u00b6 If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License \u00b6 SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#_1","text":"SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow.","title":""},{"location":"#use-cases","text":"SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key-features","text":"SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design-principles","text":"SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick-start","text":"$ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"cli/","text":"SAYN CLI \u00b6 About \u00b6 SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options. Available commands \u00b6 sayn init \u00b6 Initialises a SAYN project in the current working directory with the SAYN tutorial . sayn run \u00b6 Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks. Filtering tasks \u00b6 Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Both -t and -x can be specified multiple times, accumulating their values. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 -t task2 : runs task1 and task2 only. sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors. Incremental tasks options \u00b6 SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console. sayn compile \u00b6 Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply. sayn dag-image \u00b6 Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"CLI"},{"location":"cli/#sayn-cli","text":"","title":"SAYN CLI"},{"location":"cli/#about","text":"SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options.","title":"About"},{"location":"cli/#available-commands","text":"","title":"Available commands"},{"location":"cli/#sayn-init","text":"Initialises a SAYN project in the current working directory with the SAYN tutorial .","title":"sayn init"},{"location":"cli/#sayn-run","text":"Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks.","title":"sayn run"},{"location":"cli/#filtering-tasks","text":"Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Both -t and -x can be specified multiple times, accumulating their values. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 -t task2 : runs task1 and task2 only. sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors.","title":"Filtering tasks"},{"location":"cli/#incremental-tasks-options","text":"SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console.","title":"Incremental tasks options"},{"location":"cli/#sayn-compile","text":"Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply.","title":"sayn compile"},{"location":"cli/#sayn-dag-image","text":"Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"sayn dag-image"},{"location":"installation/","text":"Installation \u00b6 SAYN is available for installation using pip and is regularly tested in Python 3.6+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"installation/#installation","text":"SAYN is available for installation using pip and is regularly tested in Python 3.6+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"parameters/","text":"Parameters \u00b6 parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks. Project parameters \u00b6 Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile. Task parameters \u00b6 Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL. Using parameters \u00b6 Using parameters in tasks \u00b6 Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }} In SQL queries \u00b6 For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt In Python Tasks \u00b6 Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"Parameters"},{"location":"parameters/#parameters","text":"parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks.","title":"Parameters"},{"location":"parameters/#project-parameters","text":"Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile.","title":"Project parameters"},{"location":"parameters/#task-parameters","text":"Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL.","title":"Task parameters"},{"location":"parameters/#using-parameters","text":"","title":"Using parameters"},{"location":"parameters/#using-parameters-in-tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }}","title":"Using parameters in tasks"},{"location":"parameters/#in-sql-queries","text":"For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt","title":"In SQL queries"},{"location":"parameters/#in-python-tasks","text":"Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"In Python Tasks"},{"location":"presets/","text":"Presets \u00b6 Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition. Defining a preset \u00b6 Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder). Preset inheritance \u00b6 Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Presets"},{"location":"presets/#presets","text":"Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition.","title":"Presets"},{"location":"presets/#defining-a-preset","text":"Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder).","title":"Defining a preset"},{"location":"presets/#preset-inheritance","text":"Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Preset inheritance"},{"location":"project_structure/","text":"SAYN Project Structure \u00b6 SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Structure"},{"location":"project_structure/#sayn-project-structure","text":"SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"SAYN Project Structure"},{"location":"api/database/","text":"\u00b6 Database \u00b6 Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine. execute ( self , script ) \u00b6 Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required grant_permissions ( self , table , schema , ddl , execute = False ) \u00b6 Returns a set of GRANT statements. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required execute bool Execute the query before returning it False Returns: Type Description str A SQL script for the GRANT statements load_data ( self , table , data , schema = None , batch_size = None , replace = False , ddl = None ) \u00b6 Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks None read_data ( self , query , ** params ) \u00b6 Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"Database"},{"location":"api/database/#sayn.database","text":"","title":"sayn.database"},{"location":"api/database/#sayn.database.Database","text":"Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine.","title":"Database"},{"location":"api/database/#sayn.database.Database.execute","text":"Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required","title":"execute()"},{"location":"api/database/#sayn.database.Database.grant_permissions","text":"Returns a set of GRANT statements. Parameters: Name Type Description Default table str The target table name required schema str The target schema or None required ddl dict A ddl task definition required execute bool Execute the query before returning it False Returns: Type Description str A SQL script for the GRANT statements","title":"grant_permissions()"},{"location":"api/database/#sayn.database.Database.load_data","text":"Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks None","title":"load_data()"},{"location":"api/database/#sayn.database.Database.read_data","text":"Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"read_data()"},{"location":"api/python_task/","text":"\u00b6 Task \u00b6 Base class for tasks in SAYN. Attributes: Name Type Description name str Name of the task as defined in the task group. group str Name of the task group where the task was defined. run_arguments dict Dictionary containing the values for the arguments specified in the cli. task_parameters dict Provides access to the parameters specified in the task. project_parameters dict Provides access to the global parameters of the project. parameters dict Convinience property joining project and task parameters. connections dict Dictionary of connections specified for the project. tracker sayn.logging.TaskEventTracker Message tracker for the current task. jinja_env jinja2.Environment Jinja environment for this task. The environment comes pre-populated with the parameter values relevant to the task. compile_obj ( self , obj , ** params ) \u00b6 Compiles the object into a string using the task jinja environment. Parameters: Name Type Description Default obj str/Path/Template The object to compile. If the object is not a Jinja template object, self.get_template will be called first. required params dict An optional dictionary of additional values to use for compilation. Note: Project and task parameters values are already set in the environment, so there's no need to pass them on {} debug ( self , message , details = None ) \u00b6 Print a debug message when executing sayn in debug mode ( sayn run -d ) error ( self , message , details = None ) \u00b6 Prints an error message which will be persisted on the screen after the task concludes execution. Executing this method doesn't abort the task or changes the task status. Use return self.fail for that instead. Parameters: Name Type Description Default message str An optinal error message to print to the screen. required fail ( self , msg = None ) \u00b6 Returned on failure in any stage. finish_current_step ( self , result = Result . Ok : None ) \u00b6 Specifies the end of the current step get_template ( self , obj ) \u00b6 Returns a Jinja template object. Parameters: Name Type Description Default obj str/Path The object to transform into a template. If a pathlib.Path is specified, the template will be read from disk. required info ( self , message , details = None ) \u00b6 Prints an info message. ready ( self ) \u00b6 (Deprecated: use success instead) Returned on successful execution. set_run_steps ( self , steps ) \u00b6 Sets the run steps for the task, allowing the CLI to indicate task execution progress. start_step ( self , step ) \u00b6 Specifies the start of a task step step ( self , step ) \u00b6 Step context Usage: with self . step ( 'Generate Data' ): data = generate_data () Parameters: Name Type Description Default step str name of the step being executed. required success ( self ) \u00b6 Returned on successful execution. warning ( self , message , details = None ) \u00b6 Prints a warning message which will be persisted on the screen after the task concludes execution.","title":"PythonTask"},{"location":"api/python_task/#sayn.tasks","text":"","title":"sayn.tasks"},{"location":"api/python_task/#sayn.tasks.Task","text":"Base class for tasks in SAYN. Attributes: Name Type Description name str Name of the task as defined in the task group. group str Name of the task group where the task was defined. run_arguments dict Dictionary containing the values for the arguments specified in the cli. task_parameters dict Provides access to the parameters specified in the task. project_parameters dict Provides access to the global parameters of the project. parameters dict Convinience property joining project and task parameters. connections dict Dictionary of connections specified for the project. tracker sayn.logging.TaskEventTracker Message tracker for the current task. jinja_env jinja2.Environment Jinja environment for this task. The environment comes pre-populated with the parameter values relevant to the task.","title":"Task"},{"location":"api/python_task/#sayn.tasks.Task.compile_obj","text":"Compiles the object into a string using the task jinja environment. Parameters: Name Type Description Default obj str/Path/Template The object to compile. If the object is not a Jinja template object, self.get_template will be called first. required params dict An optional dictionary of additional values to use for compilation. Note: Project and task parameters values are already set in the environment, so there's no need to pass them on {}","title":"compile_obj()"},{"location":"api/python_task/#sayn.tasks.Task.debug","text":"Print a debug message when executing sayn in debug mode ( sayn run -d )","title":"debug()"},{"location":"api/python_task/#sayn.tasks.Task.error","text":"Prints an error message which will be persisted on the screen after the task concludes execution. Executing this method doesn't abort the task or changes the task status. Use return self.fail for that instead. Parameters: Name Type Description Default message str An optinal error message to print to the screen. required","title":"error()"},{"location":"api/python_task/#sayn.tasks.Task.fail","text":"Returned on failure in any stage.","title":"fail()"},{"location":"api/python_task/#sayn.tasks.Task.finish_current_step","text":"Specifies the end of the current step","title":"finish_current_step()"},{"location":"api/python_task/#sayn.tasks.Task.get_template","text":"Returns a Jinja template object. Parameters: Name Type Description Default obj str/Path The object to transform into a template. If a pathlib.Path is specified, the template will be read from disk. required","title":"get_template()"},{"location":"api/python_task/#sayn.tasks.Task.info","text":"Prints an info message.","title":"info()"},{"location":"api/python_task/#sayn.tasks.Task.ready","text":"(Deprecated: use success instead) Returned on successful execution.","title":"ready()"},{"location":"api/python_task/#sayn.tasks.Task.set_run_steps","text":"Sets the run steps for the task, allowing the CLI to indicate task execution progress.","title":"set_run_steps()"},{"location":"api/python_task/#sayn.tasks.Task.start_step","text":"Specifies the start of a task step","title":"start_step()"},{"location":"api/python_task/#sayn.tasks.Task.step","text":"Step context Usage: with self . step ( 'Generate Data' ): data = generate_data () Parameters: Name Type Description Default step str name of the step being executed. required","title":"step()"},{"location":"api/python_task/#sayn.tasks.Task.success","text":"Returned on successful execution.","title":"success()"},{"location":"api/python_task/#sayn.tasks.Task.warning","text":"Prints a warning message which will be persisted on the screen after the task concludes execution.","title":"warning()"},{"location":"databases/mysql/","text":"MySQL \u00b6 The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/mysql/#mysql","text":"The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/overview/","text":"Databases \u00b6 About \u00b6 SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Redshift Snowflake PostgreSQL MySQL SQLite Usage \u00b6 Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200 Using databases in Python tasks \u00b6 Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Overview"},{"location":"databases/overview/#databases","text":"","title":"Databases"},{"location":"databases/overview/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: Redshift Snowflake PostgreSQL MySQL SQLite","title":"About"},{"location":"databases/overview/#usage","text":"Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200","title":"Usage"},{"location":"databases/overview/#using-databases-in-python-tasks","text":"Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Using databases in Python tasks"},{"location":"databases/postgresql/","text":"PostgreSQL \u00b6 The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/postgresql/#postgresql","text":"The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/redshift/","text":"Redshift \u00b6 The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters. Connection types \u00b6 SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based. Standard user/password connection \u00b6 If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Connecting with IAM \u00b6 With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters . Redshift specific DDL \u00b6 Indexes \u00b6 Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition. Sorting \u00b6 Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY . Distribution \u00b6 We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Redshift"},{"location":"databases/redshift/#redshift","text":"The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters.","title":"Redshift"},{"location":"databases/redshift/#connection-types","text":"SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based.","title":"Connection types"},{"location":"databases/redshift/#standard-userpassword-connection","text":"If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models","title":"Standard user/password connection"},{"location":"databases/redshift/#connecting-with-iam","text":"With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters .","title":"Connecting with IAM"},{"location":"databases/redshift/#redshift-specific-ddl","text":"","title":"Redshift specific DDL"},{"location":"databases/redshift/#indexes","text":"Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition.","title":"Indexes"},{"location":"databases/redshift/#sorting","text":"Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY .","title":"Sorting"},{"location":"databases/redshift/#distribution","text":"We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles ddl : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Distribution"},{"location":"databases/snowflake/","text":"Snowflake \u00b6 The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/snowflake/#snowflake","text":"The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/sqlite/","text":"SQLite \u00b6 SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"databases/sqlite/#sqlite","text":"SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"settings/project_yaml/","text":"Settings: project.yaml \u00b6 The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"project.yaml"},{"location":"settings/project_yaml/#settings-projectyaml","text":"The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details.","title":"Settings: project.yaml"},{"location":"settings/settings_yaml/","text":"Settings: settings.yaml \u00b6 The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production. Defining credentials \u00b6 Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects. Using environment variables \u00b6 Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are json encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh export SAYN_CREDENTIAL_warehouse = \"{'type': 'snowflake', 'account': ...\" export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence.","title":"settings.yaml"},{"location":"settings/settings_yaml/#settings-settingsyaml","text":"The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production.","title":"Settings: settings.yaml"},{"location":"settings/settings_yaml/#defining-credentials","text":"Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects.","title":"Defining credentials"},{"location":"settings/settings_yaml/#using-environment-variables","text":"Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are json encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh export SAYN_CREDENTIAL_warehouse = \"{'type': 'snowflake', 'account': ...\" export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence.","title":"Using environment variables"},{"location":"tasks/autosql/","text":"autosql Task \u00b6 About \u00b6 The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks \u00b6 An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. schema : the (optional) destination schema where the object will be created. table : is the name of the object that will be created. db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . Using autosql In incremental Mode \u00b6 autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2 Defining DDLs \u00b6 Autosql tasks support the definition of optional DDL. Each DDL entry is independent to others (you can define only DDLs which are relevant to you). Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages. ALTER TABLE DDLs \u00b6 The following DDLs will be issued by SAYN with ALTER TABLE statements: indexes: the indexes to add on the table. primary_key: this should be added in the indexes section using the primary_key name for the index. permissions: the permissions you want to give to each role. You should map each role to the rights you want to grant separated by commas (e.g. SELECT, DELETE). autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 permissions : role_name : SELECT ... CREATE TABLE DDLs \u00b6 SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. columns can define the following attributes: name: the column name. type: the column type. primary: set to True if the column is part of the primary key. unique: set to True to enforce a unique constraint on the column. not_null: set to True to enforce a non null constraint on the column. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"AutoSQL"},{"location":"tasks/autosql/#autosql-task","text":"","title":"autosql Task"},{"location":"tasks/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/autosql/#defining-autosql-tasks","text":"An autosql task is defined as follows: autosql task definition ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ... An autosql task is defined by the following attributes: type : autosql . file_name : the name of the file within the sql folder of the project's root . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. schema : the (optional) destination schema where the object will be created. table : is the name of the object that will be created. db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"Defining autosql Tasks"},{"location":"tasks/autosql/#using-autosql-in-incremental-mode","text":"autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2","title":"Using autosql In incremental Mode"},{"location":"tasks/autosql/#defining-ddls","text":"Autosql tasks support the definition of optional DDL. Each DDL entry is independent to others (you can define only DDLs which are relevant to you). Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages.","title":"Defining DDLs"},{"location":"tasks/autosql/#alter-table-ddls","text":"The following DDLs will be issued by SAYN with ALTER TABLE statements: indexes: the indexes to add on the table. primary_key: this should be added in the indexes section using the primary_key name for the index. permissions: the permissions you want to give to each role. You should map each role to the rights you want to grant separated by commas (e.g. SELECT, DELETE). autosql with DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : indexes : primary_key : columns : - column1 - column2 idx1 : columns : - column1 permissions : role_name : SELECT ...","title":"ALTER TABLE DDLs"},{"location":"tasks/autosql/#create-table-ddls","text":"SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. columns can define the following attributes: name: the column name. type: the column type. primary: set to True if the column is part of the primary key. unique: set to True to enforce a unique constraint on the column. not_null: set to True to enforce a non null constraint on the column. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns DDL ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"CREATE TABLE DDLs"},{"location":"tasks/copy/","text":"copy Task \u00b6 About \u00b6 The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Defining copy tasks \u00b6 A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . By default, tables will be copied in full every time SAYN runs, but it can be changed into an incremental load by adding incremental_key and delete_key : incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior to the maximum found in the destination. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset. Data types and DDL \u00b6 copy tasks accept a ddl field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id ddl : columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Copy"},{"location":"tasks/copy/#copy-task","text":"","title":"copy Task"},{"location":"tasks/copy/#about","text":"The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse.","title":"About"},{"location":"tasks/copy/#defining-copy-tasks","text":"A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . By default, tables will be copied in full every time SAYN runs, but it can be changed into an incremental load by adding incremental_key and delete_key : incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior to the maximum found in the destination. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset.","title":"Defining copy tasks"},{"location":"tasks/copy/#data-types-and-ddl","text":"copy tasks accept a ddl field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id ddl : columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Data types and DDL"},{"location":"tasks/dummy/","text":"dummy Task \u00b6 About \u00b6 The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups. Defining dummy tasks \u00b6 A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy Usage \u00b6 dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Dummy"},{"location":"tasks/dummy/#dummy-task","text":"","title":"dummy Task"},{"location":"tasks/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups.","title":"About"},{"location":"tasks/dummy/#defining-dummy-tasks","text":"A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy","title":"Defining dummy tasks"},{"location":"tasks/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Usage"},{"location":"tasks/overview/","text":"Tasks \u00b6 About \u00b6 Tasks are the backbone of your SAYN project. They are used by SAYN to create a DAG (Directed Acyclic Graph). Info A Directed Acyclic Graph is a concept which enables to conveniently model tasks and dependencies. It uses the following key principles graph : a specific data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined with the parents list. To relate back to the DAG concept, this implies each task in SAYN represents a node and edges are defined by the parents attribute of each task. For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Task Types \u00b6 Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file. Defining Tasks \u00b6 Tasks are defined in YAML files located under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it. Task Groups \u00b6 Task groups are a convenient way to segment and organise your data processes in your SAYN project. Each YAML file in the tasks folder represents a task group. Tip When growing a SAYN project, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. Each task group file defines tasks (required) and presets (optional). tasks/base.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data Property Description Required tasks The set of tasks that compose the task group. For more details on tasks , please see the Tasks section. Yes presets Defines preset task structures shared by several tasks. Presets defined within task group files can inherit from presets defined at the project level in project.yaml . See the Presets section for more details. Optional Task Attributes \u00b6 Task attributes can be used when defining tasks in a dynamic way. The following example shows how to use the task name and task group dynamically when defining a task: tasks/base.yaml tasks : sql_task : type : sql file_name : '{{task.group}}/{{task.name}}.sql' This will effectively tell the task to look for a file located at base/sql_task.sql in the sql folder.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"Tasks are the backbone of your SAYN project. They are used by SAYN to create a DAG (Directed Acyclic Graph). Info A Directed Acyclic Graph is a concept which enables to conveniently model tasks and dependencies. It uses the following key principles graph : a specific data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined with the parents list. To relate back to the DAG concept, this implies each task in SAYN represents a node and edges are defined by the parents attribute of each task. For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers!","title":"About"},{"location":"tasks/overview/#task-types","text":"Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. dummy : those tasks do not do anything. They can be used as connectors between tasks. sql : executes any SQL statement. There can be multiple statements within the SQL file.","title":"Task Types"},{"location":"tasks/overview/#defining-tasks","text":"Tasks are defined in YAML files located under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it.","title":"Defining Tasks"},{"location":"tasks/overview/#task-groups","text":"Task groups are a convenient way to segment and organise your data processes in your SAYN project. Each YAML file in the tasks folder represents a task group. Tip When growing a SAYN project, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. Each task group file defines tasks (required) and presets (optional). tasks/base.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' tasks : load_data : type : python class : load_data.LoadData #task defined without preset dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : dim_tournaments parents : - load_data #task defined using a preset dim_arenas : preset : modelling file_name : dim_arenas.sql parents : - load_data Property Description Required tasks The set of tasks that compose the task group. For more details on tasks , please see the Tasks section. Yes presets Defines preset task structures shared by several tasks. Presets defined within task group files can inherit from presets defined at the project level in project.yaml . See the Presets section for more details. Optional","title":"Task Groups"},{"location":"tasks/overview/#task-attributes","text":"Task attributes can be used when defining tasks in a dynamic way. The following example shows how to use the task name and task group dynamically when defining a task: tasks/base.yaml tasks : sql_task : type : sql file_name : '{{task.group}}/{{task.name}}.sql' This will effectively tell the task to look for a file located at base/sql_task.sql in the sql folder.","title":"Task Attributes"},{"location":"tasks/python/","text":"python Task \u00b6 About \u00b6 The python task allows you run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. Defining python Tasks \u00b6 A python task is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder. Writing A python Task \u00b6 Basics \u00b6 The basic code to construct a python task is: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def setup ( self ): # Do some checked return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Please note that python tasks need to return either self.success() or self.fail() in order to run. Using the SAYN API \u00b6 When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. Here's a list of properties available: self.parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. self.run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). self.connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. self.default_db : provides access to the default_db database object specified in the project.yaml file. Logging for Python tasks with the SAYN API \u00b6 The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py self . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with self . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Python"},{"location":"tasks/python/#python-task","text":"","title":"python Task"},{"location":"tasks/python/#about","text":"The python task allows you run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models.","title":"About"},{"location":"tasks/python/#defining-python-tasks","text":"A python task is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder.","title":"Defining python Tasks"},{"location":"tasks/python/#writing-a-python-task","text":"","title":"Writing A python Task"},{"location":"tasks/python/#basics","text":"The basic code to construct a python task is: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def setup ( self ): # Do some checked return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Please note that python tasks need to return either self.success() or self.fail() in order to run.","title":"Basics"},{"location":"tasks/python/#using-the-sayn-api","text":"When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. Here's a list of properties available: self.parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. self.run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). self.connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. self.default_db : provides access to the default_db database object specified in the project.yaml file.","title":"Using the SAYN API"},{"location":"tasks/python/#logging-for-python-tasks-with-the-sayn-api","text":"The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py self . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with self . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Logging for Python tasks with the SAYN API"},{"location":"tasks/sql/","text":"sql Task \u00b6 About \u00b6 The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql . Defining sql tasks \u00b6 A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : path to a file under the sql folder containing the SQL script to execute. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"SQL"},{"location":"tasks/sql/#sql-task","text":"","title":"sql Task"},{"location":"tasks/sql/#about","text":"The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql .","title":"About"},{"location":"tasks/sql/#defining-sql-tasks","text":"A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : path to a file under the sql folder containing the SQL script to execute. db : the (optional) destination database. Info You do not need to specify db unless you want the destination database to be different than the default_db you define in project.yaml (which is the default database used by SAYN). If you define the db attribute, it needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"Defining sql tasks"},{"location":"tutorials/tutorial_part1/","text":"Tutorial: Part 1 \u00b6 This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section . This project generates some random data with a python task and performs some modelling on it with autosql tasks. Running SAYN \u00b6 To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You can open dev.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background. Project Overview \u00b6 The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution. Implementing your project \u00b6 Now let's see how the tutorial project would be created from scratch. Step 1: Define the project in project.yaml \u00b6 The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped. Step 2: Define your individual settings with settings.yaml \u00b6 The settings.yaml file at the root level of your directory and contains: settings.yaml profiles : dev : credentials : warehouse : dev_db prod : credentials : warehouse : prod_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db prod_db : type : sqlite database : prod.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and credentials defined below. In this case we define 2 profiles dev and prod. default_profile : the profile used by default at execution time. It can be overriden using sayn run -p prod . credentials : here we define the credentials. In this case we have two for dev and prod, that are used as warehouse on each profile. Step 3: Define your tasks \u00b6 In SAYN, tasks are defined in yaml files within the tasks folder. Each file is considered a task group . Our project contains only one task group: base.yaml : tasks/base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : table : dim_tournaments parents : - load_data # ... The tasks entry contains a map of tasks definitions. In this case we're using two types of tasks: python : lets you define a task written in Python. Python tasks are useful to complete your extraction and load layers if you're using an ELT tool or for data science models defined in Python. autosql : lets you write a SELECT statement while SAYN manages the table or view creation automatically for you. Our example has multiple autosql tasks which create models based on the logs. Tip Although this tutorial only has one file in the tasks folder, you can separate tasks in multiple files. SAYN automatically includes any file from the tasks folder with a .yaml extension when creating the DAG. Each file is considered a task group . load_data task \u00b6 In our example project the only python task is load_data which creates some synthetic logs and loads them to our database. The code can be found in the class LoadData in python/load_data.py . Let's have a look at the main elements of a python task: python/load_data.py # ... from sayn import PythonTask class LoadData ( PythonTask ): def run ( self ): # Your code here The above is the beginning of the python task. When the execution of sayn run hits the load_data task the code in the run method will execute. A task in SAYN can be split into multiple steps, which is useful for debugging when errors occur. In this case, we first generate all data and then we load each dataset one by one. We can define the steps a task will follow with the self.set_run_steps method. python/load_data.py def run ( self ): # ... self . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) # ... To indicate SAYN what step is executing, we can use the following construct: python/load_data.py def run ( self ): # ... with self . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( self . fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( self . arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( self . tournaments ) ] Here our \"Generate Dimensions\" step simply generates the dimension variables with an id. The final core element is accessing databases. In our project we defined a single credential called warehouse and we made this the default_db . To access this we just need to use self.default_db . python/load_data.py self . default_db . execute ( q_create ) The main method in Database objects is execute which accepts a sql script via parameter and executes it in a transaction. Another method used in this tutorial is load_data which loads a dataset into the database automatically creating a table for it first. For more information about how to build python tasks, visit the python tasks section . Autosql tasks \u00b6 Let's have a look at one of the autosql tasks ( dim_tournaments ). As you can see in tasks/base.yaml above, we specify a file_name which contains: sql/dim_tournaments.yaml SELECT l . tournament_id , l . tournament_name FROM logs_tournaments l This is a simple SELECT statement that SAYN will use when creating a table called dim_tournaments as defined in the destination field in the base.yaml file. For more information about setting up autosql tasks, visit the autosql tasks section . Running Your Project \u00b6 So far we've used sayn run to execute our project, however SAYN provides more options: sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : allows the filtering the tasks to run. More options are available to run specific components of your SAYN project. All details can be found in the SAYN cli section. What Next? \u00b6 You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"Tutorial (Part 1)"},{"location":"tutorials/tutorial_part1/#tutorial-part-1","text":"This tutorial covers the basic concepts of SAYN and will get you going quickly. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section . This project generates some random data with a python task and performs some modelling on it with autosql tasks.","title":"Tutorial: Part 1"},{"location":"tutorials/tutorial_part1/#running-sayn","text":"To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You can open dev.db and see the tables and views created by sayn run . You can use DB Browser for SQLite in order to view the content of the database. As you can observe, sayn run created a small ETL process which models battles from various tournaments. That's it, you made your first SAYN run! We will now explain what happens in the background.","title":"Running SAYN"},{"location":"tutorials/tutorial_part1/#project-overview","text":"The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution.","title":"Project Overview"},{"location":"tutorials/tutorial_part1/#implementing-your-project","text":"Now let's see how the tutorial project would be created from scratch.","title":"Implementing your project"},{"location":"tutorials/tutorial_part1/#step-1-define-the-project-in-projectyaml","text":"The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped.","title":"Step 1: Define the project in project.yaml"},{"location":"tutorials/tutorial_part1/#step-2-define-your-individual-settings-with-settingsyaml","text":"The settings.yaml file at the root level of your directory and contains: settings.yaml profiles : dev : credentials : warehouse : dev_db prod : credentials : warehouse : prod_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db prod_db : type : sqlite database : prod.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and credentials defined below. In this case we define 2 profiles dev and prod. default_profile : the profile used by default at execution time. It can be overriden using sayn run -p prod . credentials : here we define the credentials. In this case we have two for dev and prod, that are used as warehouse on each profile.","title":"Step 2: Define your individual settings with settings.yaml"},{"location":"tutorials/tutorial_part1/#step-3-define-your-tasks","text":"In SAYN, tasks are defined in yaml files within the tasks folder. Each file is considered a task group . Our project contains only one task group: base.yaml : tasks/base.yaml tasks : load_data : type : python class : load_data.LoadData dim_tournaments : type : autosql file_name : dim_tournaments.sql materialisation : table destination : table : dim_tournaments parents : - load_data # ... The tasks entry contains a map of tasks definitions. In this case we're using two types of tasks: python : lets you define a task written in Python. Python tasks are useful to complete your extraction and load layers if you're using an ELT tool or for data science models defined in Python. autosql : lets you write a SELECT statement while SAYN manages the table or view creation automatically for you. Our example has multiple autosql tasks which create models based on the logs. Tip Although this tutorial only has one file in the tasks folder, you can separate tasks in multiple files. SAYN automatically includes any file from the tasks folder with a .yaml extension when creating the DAG. Each file is considered a task group .","title":"Step 3: Define your tasks"},{"location":"tutorials/tutorial_part1/#load_data-task","text":"In our example project the only python task is load_data which creates some synthetic logs and loads them to our database. The code can be found in the class LoadData in python/load_data.py . Let's have a look at the main elements of a python task: python/load_data.py # ... from sayn import PythonTask class LoadData ( PythonTask ): def run ( self ): # Your code here The above is the beginning of the python task. When the execution of sayn run hits the load_data task the code in the run method will execute. A task in SAYN can be split into multiple steps, which is useful for debugging when errors occur. In this case, we first generate all data and then we load each dataset one by one. We can define the steps a task will follow with the self.set_run_steps method. python/load_data.py def run ( self ): # ... self . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) # ... To indicate SAYN what step is executing, we can use the following construct: python/load_data.py def run ( self ): # ... with self . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( self . fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( self . arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( self . tournaments ) ] Here our \"Generate Dimensions\" step simply generates the dimension variables with an id. The final core element is accessing databases. In our project we defined a single credential called warehouse and we made this the default_db . To access this we just need to use self.default_db . python/load_data.py self . default_db . execute ( q_create ) The main method in Database objects is execute which accepts a sql script via parameter and executes it in a transaction. Another method used in this tutorial is load_data which loads a dataset into the database automatically creating a table for it first. For more information about how to build python tasks, visit the python tasks section .","title":"load_data task"},{"location":"tutorials/tutorial_part1/#autosql-tasks","text":"Let's have a look at one of the autosql tasks ( dim_tournaments ). As you can see in tasks/base.yaml above, we specify a file_name which contains: sql/dim_tournaments.yaml SELECT l . tournament_id , l . tournament_name FROM logs_tournaments l This is a simple SELECT statement that SAYN will use when creating a table called dim_tournaments as defined in the destination field in the base.yaml file. For more information about setting up autosql tasks, visit the autosql tasks section .","title":"Autosql tasks"},{"location":"tutorials/tutorial_part1/#running-your-project","text":"So far we've used sayn run to execute our project, however SAYN provides more options: sayn run -p [profile_name] : runs the whole project with the specific profile. In our case using the profile prod will create a prod.db SQLite database and process all data there. sayn run -t [task_name] : allows the filtering the tasks to run. More options are available to run specific components of your SAYN project. All details can be found in the SAYN cli section.","title":"Running Your Project"},{"location":"tutorials/tutorial_part1/#what-next","text":"You now know the basics of SAYN, congratulations! You can continue learning by going through the Tutorial: Part 2 which shows you several tricks to make your SAYN project more dynamic and efficient. Otherwise, you can go through the rest of the documentation as SAYN has much more to offer! Enjoy SAYN :)","title":"What Next?"},{"location":"tutorials/tutorial_part2/","text":"Tutorial: Part 2 \u00b6 This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. In Tutorial: Part 1 we implemented our first ETL process with SAYN. We will now expand on that by adding parameters and presets . A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work. Step 1: Define the project parameters \u00b6 You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters at the end of project.yaml . This is a YAML map which defines the default value. project.yaml # ... parameters : user_prefix : '' #no prefix for prod In this case we defined a parameter user_prefix that we will use to name tables. This is useful when multiple users are testing a project as it allows the final table name to be different between collaborators. Now we can define the value of the parameter on each profile. We do this in the settings.yaml . settings.yaml profiles : dev : credentials : warehouse : dev_db parameters : user_prefix : up_ prod : credentials : warehouse : prod_db Note how we don't redefine the parameter in our prod profile as the default value is more appropriate. Step 2: Making Tasks Dynamic With parameters \u00b6 Now that our parameters are setup, we can use those to make our tasks' code dynamic. In python tasks \u00b6 For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils.py to see how we use the user_prefix parameter to change the table names. python/load_data.py # ... def run ( self ): user_prefix = self . parameters [ 'user_prefix' ] # ... q_create = get_create_table ( log_type , user_prefix ) # ... In autosql tasks \u00b6 The files in the sql folder are always interpreted as Jinja templates. This means that in order to access parameters all we have to do is enclose it in {{ }} Jinja blocks. For example, in order to reference the tables created by load_data the dim_arenas task can be changed like this: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l Now sayn run will transform the above into valid SQL creating compile/base/dim_arenas.sql with it. The file path following the rule compile/task_group_name/autosql_task_name.sql : compile/base/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM up_logs_arenas l SAYN provides a sayn compile command that works like sayn run except that it won't execute the code. What it does though, is generate the compiled files that SAYN would run with the sayn run command. Step 3: Making task definitions dynamic with parameters \u00b6 Now that our python task generates tables with the user_prefix in the name and our autosql tasks will select data from it. What we also need to do is change the table names our autosql tasks are generating. For that, let's take dim_arenas and modify it so that it generates a table called up_dim_arenas (or other user_prefix defined in settings.yaml ): tasks/base.yaml tasks : # ... dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data # ... Note the value of destination.table is now some Jinja code that will compile to the value of user_prefix followed by the name of the task. Step 4: Using presets to standardise task definitions \u00b6 Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets allow you to define common properties shared by several tasks. tasks/base.yaml presets : modelling : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data tasks : # ... dim_arenas : preset : modelling # ... Now the modelling preset has to dynamic properties: * table : defined like we did in the previous so that the create table contains the user_prefix in the name. * file_name : that uses the task name to point at the correct file in the sql folder. In addition, modelling is defined so that tasks referencing it: * are autosql tasks. * Materialise as tables. * Have load_data as a parent task, so that models always run after our log generator. When a task references a preset, we're not restricted to the values defined in the preset. A task can override those values. Take f_rankings for example: tasks/base.yaml tasks : # ... f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Here we're overloading 2 properties: * materialisation which will make f_rankings a view rather than a table. * parents which will make f_ranking depend on f_fighter_results as well as load_data as defined in the preset. Running Our New Project \u00b6 You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a dev.db database prefix all tables with up_ and read from up_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables What Next? \u00b6 This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"Tutorial (Part 2)"},{"location":"tutorials/tutorial_part2/#tutorial-part-2","text":"This tutorial builds upon the Tutorial: Part 1 and introduces you to SAYN concepts which will enable you to make your projects more dynamic and efficient. In Tutorial: Part 1 we implemented our first ETL process with SAYN. We will now expand on that by adding parameters and presets . A Github repository is available with the final code if you prefer to have all the code written directly. After cloning, you simply need to make sure to rename the sample_settings.yaml file to settings.yaml for the project to work.","title":"Tutorial: Part 2"},{"location":"tutorials/tutorial_part2/#step-1-define-the-project-parameters","text":"You can use parameters in order to make your SAYN tasks' code dynamic. We will set one project parameter called user_prefix . This will enable us to distinguish which user generated tables. First, add paramaters at the end of project.yaml . This is a YAML map which defines the default value. project.yaml # ... parameters : user_prefix : '' #no prefix for prod In this case we defined a parameter user_prefix that we will use to name tables. This is useful when multiple users are testing a project as it allows the final table name to be different between collaborators. Now we can define the value of the parameter on each profile. We do this in the settings.yaml . settings.yaml profiles : dev : credentials : warehouse : dev_db parameters : user_prefix : up_ prod : credentials : warehouse : prod_db Note how we don't redefine the parameter in our prod profile as the default value is more appropriate.","title":"Step 1: Define the project parameters"},{"location":"tutorials/tutorial_part2/#step-2-making-tasks-dynamic-with-parameters","text":"Now that our parameters are setup, we can use those to make our tasks' code dynamic.","title":"Step 2: Making Tasks Dynamic With parameters"},{"location":"tutorials/tutorial_part2/#in-python-tasks","text":"For the Python load_data task, we will access the user_prefix parameter and then pass it to the functions doing the data processing. You can look into python/utils.py to see how we use the user_prefix parameter to change the table names. python/load_data.py # ... def run ( self ): user_prefix = self . parameters [ 'user_prefix' ] # ... q_create = get_create_table ( log_type , user_prefix ) # ...","title":"In python tasks"},{"location":"tutorials/tutorial_part2/#in-autosql-tasks","text":"The files in the sql folder are always interpreted as Jinja templates. This means that in order to access parameters all we have to do is enclose it in {{ }} Jinja blocks. For example, in order to reference the tables created by load_data the dim_arenas task can be changed like this: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM {{ user_prefix }} logs_arenas l Now sayn run will transform the above into valid SQL creating compile/base/dim_arenas.sql with it. The file path following the rule compile/task_group_name/autosql_task_name.sql : compile/base/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM up_logs_arenas l SAYN provides a sayn compile command that works like sayn run except that it won't execute the code. What it does though, is generate the compiled files that SAYN would run with the sayn run command.","title":"In autosql tasks"},{"location":"tutorials/tutorial_part2/#step-3-making-task-definitions-dynamic-with-parameters","text":"Now that our python task generates tables with the user_prefix in the name and our autosql tasks will select data from it. What we also need to do is change the table names our autosql tasks are generating. For that, let's take dim_arenas and modify it so that it generates a table called up_dim_arenas (or other user_prefix defined in settings.yaml ): tasks/base.yaml tasks : # ... dim_arenas : type : autosql file_name : dim_arenas.sql materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data # ... Note the value of destination.table is now some Jinja code that will compile to the value of user_prefix followed by the name of the task.","title":"Step 3: Making task definitions dynamic with parameters"},{"location":"tutorials/tutorial_part2/#step-4-using-presets-to-standardise-task-definitions","text":"Because most of our tasks have a similar configuration, we can significantly reduce the YAML task definitions using presets . presets allow you to define common properties shared by several tasks. tasks/base.yaml presets : modelling : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : table : '{{ user_prefix }}{{ task.name }}' parents : - load_data tasks : # ... dim_arenas : preset : modelling # ... Now the modelling preset has to dynamic properties: * table : defined like we did in the previous so that the create table contains the user_prefix in the name. * file_name : that uses the task name to point at the correct file in the sql folder. In addition, modelling is defined so that tasks referencing it: * are autosql tasks. * Materialise as tables. * Have load_data as a parent task, so that models always run after our log generator. When a task references a preset, we're not restricted to the values defined in the preset. A task can override those values. Take f_rankings for example: tasks/base.yaml tasks : # ... f_rankings : preset : modelling materialisation : view parents : - f_fighter_results Here we're overloading 2 properties: * materialisation which will make f_rankings a view rather than a table. * parents which will make f_ranking depend on f_fighter_results as well as load_data as defined in the preset.","title":"Step 4: Using presets to standardise task definitions"},{"location":"tutorials/tutorial_part2/#running-our-new-project","text":"You can now test running sayn run or sayn -p prod . The two options will do the following: sayn run : use our dev profile create all tables into a dev.db database prefix all tables with up_ and read from up_ prefixed tables sayn run -p prod : use our prod profile create all tables into a prod.db database will not use a prefix when creating / reading tables","title":"Running Our New Project"},{"location":"tutorials/tutorial_part2/#what-next","text":"This is it, you should now have a good understanding of the core ways of using SAYN. You can play further with this project and easily transfer it to a PostgreSQL database for example by: changing the credentials in settings.yaml . setting the tmp_schema and schema attributes of your modelling preset to public . Otherwise, you can learn more about the specific SAYN features by having a look at the specific sections of the documentation. Enjoy SAYN :)","title":"What Next?"}]}