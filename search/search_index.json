{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00b6 SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow. Use Cases \u00b6 SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models. Key Features \u00b6 SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation . Design Principles \u00b6 SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process. Quick Start \u00b6 SAYN supports Python 3.7 to 3.10. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power! Release Updates \u00b6 If you want to receive update emails about SAYN releases, you can sign up here . Support \u00b6 If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com . License \u00b6 SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"Home"},{"location":"#_1","text":"SAYN is a modern data processing and modelling framework. Users define tasks (incl. Python, automated SQL transformations and more) and their relationships, SAYN takes care of the rest. It is designed for simplicity, flexibility and centralisation in order to bring significant efficiency gains to the data engineering workflow.","title":""},{"location":"#use_cases","text":"SAYN can be used for multiple purposes across the data engineering and analytics workflows: Data extraction: complement tools such as Fivetran or Stitch with customised extraction processes. Data modelling: transform raw data in your data warehouse (e.g. aggregate activity or sessions, calculate marketing campaign ROI, etc.). Data science: integrate and execute data science models.","title":"Use Cases"},{"location":"#key_features","text":"SAYN has the following key features: YAML based DAG (Direct Acyclic Graph) creation. This means all analysts, including non Python proficient ones, can easily add tasks to ETL processes with SAYN. Automated SQL transformations : write your SELECT statement. SAYN turns it into a table/view and manages everything for you. Jinja parameters : switch easily between development and product environment and other tricks with Jinja templating. Python tasks : use Python scripts to complement your extraction and loading layer and build data science models. Multiple databases supported. and much more... See the Documentation .","title":"Key Features"},{"location":"#design_principles","text":"SAYN aims to empower data engineers and analysts through its three core design principles: Simplicity : data processes should be easy to create, scale and maintain. So your team can focus on data transformation instead of writing processes. SAYN orchestrates all your tasks systematically and provides a lot of automation features. Flexibility : the power of data is unlimited and so should your tooling. SAYN supports both SQL and Python so your analysts can choose the most optimal solution for each process. Centralisation : all analytics code should live in one place, making your life easier and allowing dependencies throughout the whole analytics process.","title":"Design Principles"},{"location":"#quick_start","text":"SAYN supports Python 3.7 to 3.10. $ pip install sayn $ sayn init test_sayn $ cd test_sayn $ sayn run This is it! You completed your first SAYN run on the example project. Continue with the Tutorial: Part 1 which will give you a good overview of SAYN's true power!","title":"Quick Start"},{"location":"#release_updates","text":"If you want to receive update emails about SAYN releases, you can sign up here .","title":"Release Updates"},{"location":"#support","text":"If you need any help with SAYN, or simply want to know more, please contact the team at sayn@173tech.com .","title":"Support"},{"location":"#license","text":"SAYN is open source under the Apache 2.0 license. Made with by 173tech .","title":"License"},{"location":"cli/","text":"SAYN CLI \u00b6 About \u00b6 SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options. Available Commands \u00b6 sayn init \u00b6 Initialises a SAYN project in the current working directory with the SAYN tutorial . sayn run \u00b6 Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks. Filtering Tasks \u00b6 Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Multiple tasks can be included after the argument, accumulating their values. Note that both -t and -x can be specified multiple times, resulting in the same outcome. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 task2 : runs task1 and task2 only. sayn run -t task1 -t task2 : runs task1 and task2 only. (equivalent to the one above.) sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors. Quite often we want to make some changes to a small set of tasks, explore the new results, make some more changes and repeat. When doing this we might not want to have an up to date copy of all upstream objects and instead we might want to use production as the source of your models. For this we can use the flag -u ( --upstream-prod ) which selects from production for tables not produced by the currently filtered tasks. Head over to database objects for more details Both task filtering and upstream prod arguments can be set using default_run in settings.yaml . Example: settings.yaml profiles: dev: default_run: -x group:extract This example will make it so that running sayn run will already exclude the tasks in the group called extract . Incremental Tasks Options \u00b6 SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console. sayn compile \u00b6 Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply. sayn dag-image \u00b6 Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"CLI"},{"location":"cli/#sayn_cli","text":"","title":"SAYN CLI"},{"location":"cli/#about","text":"SAYN's CLI tool is the main means for interacting with SAYN projects. Use sayn --help to see all options.","title":"About"},{"location":"cli/#available_commands","text":"","title":"Available Commands"},{"location":"cli/#sayn_init","text":"Initialises a SAYN project in the current working directory with the SAYN tutorial .","title":"sayn init"},{"location":"cli/#sayn_run","text":"Executes the project. Without arguments it will run all tasks, using the default profile defined in settings.yaml . This default behaviour can be overridden with some arguments: -p profile_name : use the specified profile instead of the default. -d : extra information to the screen, including messages from self.debug in python tasks.","title":"sayn run"},{"location":"cli/#filtering_tasks","text":"Sometimes we don't want to execute all tasks defined in the project. In these instances we can use the following arguments to filter: -t task_query : tasks to include. -x task_query : exclude specific tasks. Multiple tasks can be included after the argument, accumulating their values. Note that both -t and -x can be specified multiple times, resulting in the same outcome. Examples: sayn run -t task_name : run task_name only. sayn run -t task1 task2 : runs task1 and task2 only. sayn run -t task1 -t task2 : runs task1 and task2 only. (equivalent to the one above.) sayn run -t +task_name : run task_name and all its ancestors. sayn run -t task_name+ : run task_name and all its descendants. sayn run -t group:group_name : run all tasks specified in the group group_name . sayn run -t tag:tag_name run all tasks tagged with tag_name . sayn run -x task_name : run all tasks except task_name . sayn run -t group:marketing -x +task_name : run all tasks in the marketing task group except task_name and its ancestors. Quite often we want to make some changes to a small set of tasks, explore the new results, make some more changes and repeat. When doing this we might not want to have an up to date copy of all upstream objects and instead we might want to use production as the source of your models. For this we can use the flag -u ( --upstream-prod ) which selects from production for tables not produced by the currently filtered tasks. Head over to database objects for more details Both task filtering and upstream prod arguments can be set using default_run in settings.yaml . Example: settings.yaml profiles: dev: default_run: -x group:extract This example will make it so that running sayn run will already exclude the tasks in the group called extract .","title":"Filtering Tasks"},{"location":"cli/#incremental_tasks_options","text":"SAYN uses 3 arguments to manage incremental executions: full_load , start_dt and end_dt ; which can be overridden with these arguments to sayn run : -f : do a full refresh. Mostly useful on incremental tasks to refresh the whole table (default: False). -s : start date for incremental loads (default: yesterday). -e : end date for incremental loads (default: yesterday). These values are available to sql and autosql tasks as well as python tasks with self.run_arguments . When the sayn run command is executed, these values define the Period specified in the console.","title":"Incremental Tasks Options"},{"location":"cli/#sayn_compile","text":"Works like run except it doesn't execute the sql code. The same optional flags than for sayn run apply.","title":"sayn compile"},{"location":"cli/#sayn_dag-image","text":"Generates a visualisation of the whole SAYN process. This requires graphviz installed in your system and the python package, which can be installed with pip install \"sayn[graphviz]\" .","title":"sayn dag-image"},{"location":"database_objects/","text":"Database objects \u00b6 The most common task found in SAYN projects is the autosql task. This is a type of tasks where you write a SELECT statement and SAYN handles the database object creation. When you run the project you would want a task selecting from a table to run after the task that creates said table. In order to simplify task dependencies, SAYN considers database objects core concepts and provides some tools to treat them as such. Object specification \u00b6 In a relational database we typically find database tables organised into schemas, so for example logs.tournaments refers to a table (or view) called tournaments in the logs schema, whereas arenas refers to a table (or view) called arenas in the default schema. SAYN uses the same format to refer to database tables and views, but as we'll see this allows for a more dynamic use. Compilation of object names \u00b6 In a real world scenario we want to write our code in a way that dynamically changes depending on the profile we're running on (eg: test vs production). This allows for multiple people to collaborate on the same project, wihtout someone's actions affecting the work of others in the team. Let's consider this example task: tasks/core.yaml tasks: example_task: type: autosql materialisation: table file_name: example_task.sql destination: schema: models table: example_model This task uses the SELECT statement in the file sql/example_task.sql and creates a table example_model in the models schema of the default database. Now, if someone in your team runs this task, models.example_model will be replace with their new code and if someone else in the team is executing a task that reads from it it can produce undesired results. A way to solve this problem could be to have different databases for each person in a team but that can easily lead to complicated database setups, potential data governance issues and increased database costs, as you might need a copy of the data per person working with it. In SAYN there's another solution: we express database object names like schema.table but the code that's execution in the database is transformed according to personal settings. For example, we could have a schema called analytics_models where our production lives and another called test_models where we store data produced during development, with table names like USER_PREFIX_table rather than table so there's no collision and we minimise data redundancy. Warning Name configuration only affects the default_db . When a SAYN project has more than 1 database connection you can still use the macros described in this page to set dependencies, but the resulting value of the macro is exactly the same as the input. Name configuration \u00b6 The modifications described above are setup with prefixes, suffixes and overrides. For example: settings.yaml profiles: test: schema_prefix: test table_prefix: up The above will make every schema.table specification to be compiled to test_schema.up_table . Following the example in the previous section, if we want the to call the production schema analytics_models we can do so by adding the prefix in the project.yaml file: project.yaml schema_prefix: analytics Having both files above configured like that will make it so that referencing models.example_model on production will be translated to analytics_models.example_model whereas the same code during testing will be translated as test_models.up_example_model . In other words, what we define in project.yaml is the default behaviour which can be overriden in settings.yaml . Aside from schema_prefix and table_prefix we also have suffix ( schema_suffix and table_suffix ) which as expected would instead of prepending the value and an underscore, it adds that at the end. Info Although the name of these settigns is table_* this also applies to views in the database. Similarly in some databases the concept of schema is called differently (eg: dataset in BigQuery) but schema is still used for all databases in SAYN. Referencing database objects \u00b6 So far we've seen how the properties of an autosql task use this database object specification, but the real power of this feature is when used in the code of the task itself, which we do this with the src and out macros. For example: settings.yaml profiles: test: schema_prefix: test table_prefix: up sql/example_model.sql SELECT * FROM {{ src('logs.raw_table') }} Here we're calling the src macro that does 2 things: Using prefixes and suffixes translates logs.raw_table to the appropriate table name Declares that example_task (as defined earlier in this page) depends on the task(s) that produce logs.raw_table So the output code to be executed in the database will be: compile/core/example_model.sql -- SAYN adds the table management code SELECT * FROM test_logs.up_raw_table The counterpart to src is out which similarly translates the value to the appropriate database name, as well as it defines database objects produced by the task. In autosql tasks out is not present since there's no usage for it, however this is useful for sql tasks: sql/example_sql.sql CREATE OR REPLACE TABLE {{ out('models.sql_example') }} AS SELECT * FROM {{ src('logs.raw_table') }} This code tells SAYN that this sql task produces the table models.sql_example and depends on the table logs.raw_table , while simultaneously producing this example code to be executed in the database: compile/core/example_sql.sql CREATE OR REPLACE TABLE test_models.up_sql_example AS SELECT * FROM test_logs.up_raw_table src and out are also available to python tasks, however we use them with context.src or self.src : python/example.py @task(sources='logs.raw_table') def example_python(context, warehouse): table_name = context.src('logs.raw_table') data = warehouse.read_data(f\"select * from {table_name}\") ... python/advanced_example.py class MyTask(PythonTask): def config(self): self.table_name = self.src('logs.raw_table') def run(self): data = self.default_db.read_data(f\"select * from {self.table_name}\") ... The above examples are equivalent to each other and we use context.src in the decorator form and self.src in the more advanced class model. context.out and self.out are also available in python tasks and their behaviour is the same as with sql and autosql tasks. Info src should only be used for tables that are managed by the SAYN project. If an external EL tool is being used to load data into your warehouse, references to these tables should be hardcoded instead, as their names never change depending on your SAYN profile, nor there are any task dependencies to infer from using src . Note that calling src and out in the run method of a python task class or in the function code when using a decorator doesn't affect task dependencies, it simply outputs the translated database object name. The task dependency behaviour in python tasks is done by either calling self.src or self.out in the config method of the class or by passing these references to the task decorator in the sources and outputs arguments as seen in this example. For more details head to the python task section . Altering the behaviour of src \u00b6 A very common situation when working in your data pipeline is when we have a lot of data to work with but at any point in time while modelling we find ourselves working only a subset of it. Working with sample data can be inconvenient during development because it hinders our ability to evaluate the result and the alternative, having a duplicate of the data for every person in the team, can be costly both in terms of money and time producing and maintaining these duplicates. For this reason SAYN comes equiped with 2 features that simplifies this switchin: from_prod and upstream prod. from_prod is most useful when a team member never deals with a part of the SAYN project. For example, a data analyst that only deals with modelling tasks in a SAYN project that also has extraction tasks. Upstream prod is most useful when we're doing changes to a small set of task, so we don't want to have to repopulate all the upstream tables. from_prod configuration \u00b6 The first mechanism is from_prod which we set in the settings.yaml file and override the behaviour of src . An example: project.yaml schema_prefix: analytics sql/core/test_table.sql SELECT * FROM {{ src('logs.extract_table') }} settings.yaml profiles: dev: table_prefix: up schema_prefix: test from_prod: - \"logs.*\" In the above example we have a task selecting data from logs.extract_table which for the purpose of this example we can assume is created by an extraction task pulling data from an API. On production, src('logs.extract_table') will be translated as analytics_logs.extract_table , whereas during development it will be translated as test_logs.up_extract_table , given the configuration in the dev profile in settings.yaml . However there's also a from_prod entry with logs.* which is telling SAYN that all tables or views from the logs schema should come from production, so the final code for the test_table task will actually be: compile/core/test_table_select.sql SELECT * FROM analytics_logs.extract_table As you can see, we just need to specify a list of tables in from_prod to always read from the production configuration, that is, the settings shared by all team members as specified in project.yaml . To make it easier to use, wildcards ( * ) are accepted, so that we can specify a whole schema like in the example, but we can also specify a list of tables explicitely instead. from_prod can also be specified using environment variables with export SAYN_FROM_PROD=\"logs.*\" where the value is a comma separated list of tables. Warning To avoid accidentally affecting production tables, from_prod only affects src . The result of calling out always evaluate to your configuration in settings.yaml or environment variables. Upstream prod \u00b6 The second mechanism to override the behaviour of src is upstream prod. We use upstream prod by specifying the flag ( -u or --upstream-prod ) when running SAYN while filtering, for example sayn run -t example_task -u . When we do this, any reference to tables produced by tasks not present in the current execution will use the parameters defined in project.yaml . For example: project.yaml schema_prefix: analytics settings.yaml profiles: test: schema_prefix: test table_prefix: up sql/example_model.sql SELECT * FROM {{ src('logs.raw_table') }} Running sayn run -t example_task will run the following code in the database: compile/core/example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_example_model AS SELECT * FROM test_logs.up_raw_table So the src macro translates logs.raw_table to the testing name test_logs.up_raw_table . However, with upstream prod ( sayn run -t example_task -u ) the code executed will be: compile/core/example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_example_model AS SELECT * FROM analytics_logs.raw_table Since no task in this execution creates logs.raw_table in SAYN translates that instead to the production name analytics_logs.raw_table , while the table created is still the test version. Let' assume now that we have another task that we want to include in the execution: sql/another_example_model.sql SELECT * FROM {{ src('models.example_model') }} So when run sayn run -t example_task another_example_task -u the code for the example_task will remain the same as above, but the code executed for another_example_model will be: compile/core/another_example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_another_example_model AS SELECT * FROM test_models.up_example_model Because example_task is part of this exeuction and produces the table models.example_model reference by another_example_task models.example_model is translated using the testing settings into test_models.up_example_model unlike logs.raw_table which as no task producing it is present in this execution, will be translated into the production name. With upstream prod it becomes a lot easier to work with your modelling layer without having to duplicate all your upstream tables for every person in the team or being forced to work with sampled data. Advanced usage \u00b6 For a more advanced usage, we also have schema_override and table_override which allows us to completely change the behaviour. With override what we do is define the exact value that a schema or table name will have based on some Jinja template logic. To this template 3 values are passed: table : the name of the table specified in sayn code schema : the name of the schema specified in sayn code connection : the name of the connection it refers to settings.yaml profiles: test: schema_override: \"{% if schema != 'logs' %}test{% else %}analytics{% endif %}_{{ schema }}\" table_override: \"{% if schema != 'logs' %}up_{{ table }}{% else %}{{ table }}{% endif %}\" With this example, a reference to models.example_model will be translated as test_models.up_example_model but a reference to logs.raw_logs will be translated as analytics_logs.raw_logs . This can be useful in cases where someone in the team never works with data ingestion, so every modelling task ran by them will always reads from production data, rather than having to duplicate the data or having to work with a sample of this raw data. Warning Note that with the above example of override, a task writting to the logs schema will always write to the production version analytics_logs so to avoid issue you should always have good permissions setup in your database.","title":"Database objects"},{"location":"database_objects/#database_objects","text":"The most common task found in SAYN projects is the autosql task. This is a type of tasks where you write a SELECT statement and SAYN handles the database object creation. When you run the project you would want a task selecting from a table to run after the task that creates said table. In order to simplify task dependencies, SAYN considers database objects core concepts and provides some tools to treat them as such.","title":"Database objects"},{"location":"database_objects/#object_specification","text":"In a relational database we typically find database tables organised into schemas, so for example logs.tournaments refers to a table (or view) called tournaments in the logs schema, whereas arenas refers to a table (or view) called arenas in the default schema. SAYN uses the same format to refer to database tables and views, but as we'll see this allows for a more dynamic use.","title":"Object specification"},{"location":"database_objects/#compilation_of_object_names","text":"In a real world scenario we want to write our code in a way that dynamically changes depending on the profile we're running on (eg: test vs production). This allows for multiple people to collaborate on the same project, wihtout someone's actions affecting the work of others in the team. Let's consider this example task: tasks/core.yaml tasks: example_task: type: autosql materialisation: table file_name: example_task.sql destination: schema: models table: example_model This task uses the SELECT statement in the file sql/example_task.sql and creates a table example_model in the models schema of the default database. Now, if someone in your team runs this task, models.example_model will be replace with their new code and if someone else in the team is executing a task that reads from it it can produce undesired results. A way to solve this problem could be to have different databases for each person in a team but that can easily lead to complicated database setups, potential data governance issues and increased database costs, as you might need a copy of the data per person working with it. In SAYN there's another solution: we express database object names like schema.table but the code that's execution in the database is transformed according to personal settings. For example, we could have a schema called analytics_models where our production lives and another called test_models where we store data produced during development, with table names like USER_PREFIX_table rather than table so there's no collision and we minimise data redundancy. Warning Name configuration only affects the default_db . When a SAYN project has more than 1 database connection you can still use the macros described in this page to set dependencies, but the resulting value of the macro is exactly the same as the input.","title":"Compilation of object names"},{"location":"database_objects/#name_configuration","text":"The modifications described above are setup with prefixes, suffixes and overrides. For example: settings.yaml profiles: test: schema_prefix: test table_prefix: up The above will make every schema.table specification to be compiled to test_schema.up_table . Following the example in the previous section, if we want the to call the production schema analytics_models we can do so by adding the prefix in the project.yaml file: project.yaml schema_prefix: analytics Having both files above configured like that will make it so that referencing models.example_model on production will be translated to analytics_models.example_model whereas the same code during testing will be translated as test_models.up_example_model . In other words, what we define in project.yaml is the default behaviour which can be overriden in settings.yaml . Aside from schema_prefix and table_prefix we also have suffix ( schema_suffix and table_suffix ) which as expected would instead of prepending the value and an underscore, it adds that at the end. Info Although the name of these settigns is table_* this also applies to views in the database. Similarly in some databases the concept of schema is called differently (eg: dataset in BigQuery) but schema is still used for all databases in SAYN.","title":"Name configuration"},{"location":"database_objects/#referencing_database_objects","text":"So far we've seen how the properties of an autosql task use this database object specification, but the real power of this feature is when used in the code of the task itself, which we do this with the src and out macros. For example: settings.yaml profiles: test: schema_prefix: test table_prefix: up sql/example_model.sql SELECT * FROM {{ src('logs.raw_table') }} Here we're calling the src macro that does 2 things: Using prefixes and suffixes translates logs.raw_table to the appropriate table name Declares that example_task (as defined earlier in this page) depends on the task(s) that produce logs.raw_table So the output code to be executed in the database will be: compile/core/example_model.sql -- SAYN adds the table management code SELECT * FROM test_logs.up_raw_table The counterpart to src is out which similarly translates the value to the appropriate database name, as well as it defines database objects produced by the task. In autosql tasks out is not present since there's no usage for it, however this is useful for sql tasks: sql/example_sql.sql CREATE OR REPLACE TABLE {{ out('models.sql_example') }} AS SELECT * FROM {{ src('logs.raw_table') }} This code tells SAYN that this sql task produces the table models.sql_example and depends on the table logs.raw_table , while simultaneously producing this example code to be executed in the database: compile/core/example_sql.sql CREATE OR REPLACE TABLE test_models.up_sql_example AS SELECT * FROM test_logs.up_raw_table src and out are also available to python tasks, however we use them with context.src or self.src : python/example.py @task(sources='logs.raw_table') def example_python(context, warehouse): table_name = context.src('logs.raw_table') data = warehouse.read_data(f\"select * from {table_name}\") ... python/advanced_example.py class MyTask(PythonTask): def config(self): self.table_name = self.src('logs.raw_table') def run(self): data = self.default_db.read_data(f\"select * from {self.table_name}\") ... The above examples are equivalent to each other and we use context.src in the decorator form and self.src in the more advanced class model. context.out and self.out are also available in python tasks and their behaviour is the same as with sql and autosql tasks. Info src should only be used for tables that are managed by the SAYN project. If an external EL tool is being used to load data into your warehouse, references to these tables should be hardcoded instead, as their names never change depending on your SAYN profile, nor there are any task dependencies to infer from using src . Note that calling src and out in the run method of a python task class or in the function code when using a decorator doesn't affect task dependencies, it simply outputs the translated database object name. The task dependency behaviour in python tasks is done by either calling self.src or self.out in the config method of the class or by passing these references to the task decorator in the sources and outputs arguments as seen in this example. For more details head to the python task section .","title":"Referencing database objects"},{"location":"database_objects/#altering_the_behaviour_of_src","text":"A very common situation when working in your data pipeline is when we have a lot of data to work with but at any point in time while modelling we find ourselves working only a subset of it. Working with sample data can be inconvenient during development because it hinders our ability to evaluate the result and the alternative, having a duplicate of the data for every person in the team, can be costly both in terms of money and time producing and maintaining these duplicates. For this reason SAYN comes equiped with 2 features that simplifies this switchin: from_prod and upstream prod. from_prod is most useful when a team member never deals with a part of the SAYN project. For example, a data analyst that only deals with modelling tasks in a SAYN project that also has extraction tasks. Upstream prod is most useful when we're doing changes to a small set of task, so we don't want to have to repopulate all the upstream tables.","title":"Altering the behaviour of src"},{"location":"database_objects/#from_prod_configuration","text":"The first mechanism is from_prod which we set in the settings.yaml file and override the behaviour of src . An example: project.yaml schema_prefix: analytics sql/core/test_table.sql SELECT * FROM {{ src('logs.extract_table') }} settings.yaml profiles: dev: table_prefix: up schema_prefix: test from_prod: - \"logs.*\" In the above example we have a task selecting data from logs.extract_table which for the purpose of this example we can assume is created by an extraction task pulling data from an API. On production, src('logs.extract_table') will be translated as analytics_logs.extract_table , whereas during development it will be translated as test_logs.up_extract_table , given the configuration in the dev profile in settings.yaml . However there's also a from_prod entry with logs.* which is telling SAYN that all tables or views from the logs schema should come from production, so the final code for the test_table task will actually be: compile/core/test_table_select.sql SELECT * FROM analytics_logs.extract_table As you can see, we just need to specify a list of tables in from_prod to always read from the production configuration, that is, the settings shared by all team members as specified in project.yaml . To make it easier to use, wildcards ( * ) are accepted, so that we can specify a whole schema like in the example, but we can also specify a list of tables explicitely instead. from_prod can also be specified using environment variables with export SAYN_FROM_PROD=\"logs.*\" where the value is a comma separated list of tables. Warning To avoid accidentally affecting production tables, from_prod only affects src . The result of calling out always evaluate to your configuration in settings.yaml or environment variables.","title":"from_prod configuration"},{"location":"database_objects/#upstream_prod","text":"The second mechanism to override the behaviour of src is upstream prod. We use upstream prod by specifying the flag ( -u or --upstream-prod ) when running SAYN while filtering, for example sayn run -t example_task -u . When we do this, any reference to tables produced by tasks not present in the current execution will use the parameters defined in project.yaml . For example: project.yaml schema_prefix: analytics settings.yaml profiles: test: schema_prefix: test table_prefix: up sql/example_model.sql SELECT * FROM {{ src('logs.raw_table') }} Running sayn run -t example_task will run the following code in the database: compile/core/example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_example_model AS SELECT * FROM test_logs.up_raw_table So the src macro translates logs.raw_table to the testing name test_logs.up_raw_table . However, with upstream prod ( sayn run -t example_task -u ) the code executed will be: compile/core/example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_example_model AS SELECT * FROM analytics_logs.raw_table Since no task in this execution creates logs.raw_table in SAYN translates that instead to the production name analytics_logs.raw_table , while the table created is still the test version. Let' assume now that we have another task that we want to include in the execution: sql/another_example_model.sql SELECT * FROM {{ src('models.example_model') }} So when run sayn run -t example_task another_example_task -u the code for the example_task will remain the same as above, but the code executed for another_example_model will be: compile/core/another_example_task_create_table.sql CREATE OR REPLACE TABLE test_models.up_another_example_model AS SELECT * FROM test_models.up_example_model Because example_task is part of this exeuction and produces the table models.example_model reference by another_example_task models.example_model is translated using the testing settings into test_models.up_example_model unlike logs.raw_table which as no task producing it is present in this execution, will be translated into the production name. With upstream prod it becomes a lot easier to work with your modelling layer without having to duplicate all your upstream tables for every person in the team or being forced to work with sampled data.","title":"Upstream prod"},{"location":"database_objects/#advanced_usage","text":"For a more advanced usage, we also have schema_override and table_override which allows us to completely change the behaviour. With override what we do is define the exact value that a schema or table name will have based on some Jinja template logic. To this template 3 values are passed: table : the name of the table specified in sayn code schema : the name of the schema specified in sayn code connection : the name of the connection it refers to settings.yaml profiles: test: schema_override: \"{% if schema != 'logs' %}test{% else %}analytics{% endif %}_{{ schema }}\" table_override: \"{% if schema != 'logs' %}up_{{ table }}{% else %}{{ table }}{% endif %}\" With this example, a reference to models.example_model will be translated as test_models.up_example_model but a reference to logs.raw_logs will be translated as analytics_logs.raw_logs . This can be useful in cases where someone in the team never works with data ingestion, so every modelling task ran by them will always reads from production data, rather than having to duplicate the data or having to work with a sample of this raw data. Warning Note that with the above example of override, a task writting to the logs schema will always write to the production version analytics_logs so to avoid issue you should always have good permissions setup in your database.","title":"Advanced usage"},{"location":"installation/","text":"Installation \u00b6 SAYN is available for installation using pip and is regularly tested in Python 3.7+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"installation/#installation","text":"SAYN is available for installation using pip and is regularly tested in Python 3.7+ on both MacOS and Linux environments. pip install sayn Tip It is recommended to separate the python environment from project to project, so you might want to create a virtual environment first by running the following in a terminal: python -m venv sayn_venv source sayn_venv/bin/activate pip install sayn This default installation will not install any extra database drivers , so only support for sqlite will be available. Extra drivers can be installed using pip's optional packages specification: pip install \"sayn[postgresql]\" Check the database section for a full list of supported databases. By default the tutorials use sqlite, so with the setup above you're already setup to follow the tutorial .","title":"Installation"},{"location":"parameters/","text":"Parameters \u00b6 parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks. Project Parameters \u00b6 Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile. Task Parameters \u00b6 Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL. Using Parameters \u00b6 Using Parameters In tasks \u00b6 Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }} In SQL Queries \u00b6 For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt In python Tasks \u00b6 Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"Parameters"},{"location":"parameters/#parameters","text":"parameters are a really powerful SAYN feature. They enable you to make your code dynamic and easily switch between profiles. For example, parameters are key to separating development and production environments. SAYN uses Jinja templating for both the SQL queries and YAML properties. parameters can also be accessed in python tasks.","title":"Parameters"},{"location":"parameters/#project_parameters","text":"Project parameters are defined in project.yaml : project.yaml parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models The value set in project.yaml is the default value those parameters will have. This should match with the value used on production. Note Parameters are interpreted as yaml values, so for example schema_logs above would end up as a string. In the above example user_prefix would also be a string (empty string by default) because we included the double quote, but if we didn't include those quotations, the value would be python's None when we use it in both python and sql tasks. To override those default values, we just need to set them in the profile. For example, for a dev environment we can do the following: settings.yaml # ... default_profile : dev profiles : dev : credentials : # ... parameters : user_prefix : songoku_ schema_logs : analytics_adhoc schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : # ... In the above, we're overriding the values of the project parameters for the dev profile, but not for the prod profile.","title":"Project Parameters"},{"location":"parameters/#task_parameters","text":"Tasks can also define parameters. This is useful if there's a way for several tasks to share the same code: tasks/base.yaml task1 : type : sql file_name : task_template.sql parameters : src_table : 'table1' task2 : type : sql file_name : task_template.sql parameters : src_table : 'table2' sql/task_template.yaml SELECT dt , COUNT ( 1 ) AS c FROM {{ src_table }} GROUP BY 1 In the above example both task1 and task2 are sql tasks pointing at the same file sql/task_template.sql , the difference between the 2 is the value of the src_table parameter which is used to change the source table in the SQL.","title":"Task Parameters"},{"location":"parameters/#using_parameters","text":"","title":"Using Parameters"},{"location":"parameters/#using_parameters_in_tasks","text":"Task attributes are interpreted as Jinja parameters. Therefore, you can make the tasks' definition dynamic. This example uses an autosql task: tasks/base.yaml task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}task_autosql_param' In this example we're using schema_staging , schema_models and user_prefix project parameters so that the values would change depending on the profile. Note the use of quotation in the yaml file when we template task properties. When running sayn run -t task_autosql_param , this would be interpreted based on the dev profile, which we set as default above and evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_adhoc schema : analytics_adhoc table : songoku_task_autosql_param If we used the prod profile instead ( sayn run -t task_autosql_param -p prod ) the task will evaluate as: Example task_autosql_param : type : autosql file_name : task_autosql_param.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql_param This task example is even more powerful when used in presets in combination with the jinja variable task : tasks/base.yaml presets : preset_auto_param : type : autosql file_name : '{{ task.name }}.sql' materialisation : table destination : tmp_schema : '{{ schema_staging }}' schema : '{{ schema_models }}' table : '{{ user_prefix }}{{ task.name }}' tasks : task_autosql_param : preset : preset_auto_param Here we extract all values from task_autosql_param into a preset preset_auto_param that can be reused in multiple tasks. The name of the task is then used to reference the correct sql file and the correct table name using {{ task.name }}","title":"Using Parameters In tasks"},{"location":"parameters/#in_sql_queries","text":"For SQL related tasks ( autosql , sql ), use parameters within the SQL code with the same jinja syntax {{ parameter_name }} : sql/task_autosql_param.sql SELECT mt . * FROM {{ schema_models }} . {{ user_prefix }} my_table AS mt This SQL query would then be compiled with the relevant paramaters based on the profile of the execution. If using the dev profile, this would therefore be compiled as: compiled/base/task_autosql_param.sql SELECT mt . * FROM analytics_adhoc . songoku_my_table AS mt","title":"In SQL Queries"},{"location":"parameters/#in_python_tasks","text":"Parameters are accessible to python tasks as well as properties of the task class with self.project_parameters , self.task_parameters and self.parameters , which are all python dictionaries. self.parameters is the most convenient one as it combines both project and task parameters in a single dictionary. python/task_python.py from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): param1 = self . parameters [ 'param1' ] # Some code using param1 return self . success ()","title":"In python Tasks"},{"location":"presets/","text":"Presets \u00b6 Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition. Defining The preset \u00b6 Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder). Preset Inheritance \u00b6 Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Presets"},{"location":"presets/#presets","text":"Presets are used to define common task configuration. If a task specifies a preset attribute, it will then inherit all attributes from the referred preset . This makes presets great to avoid repetition.","title":"Presets"},{"location":"presets/#defining_the_preset","text":"Preset presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' The above defines a preset called modelling . Every task referring to it will be an autosql task and inherit all other attributes from it. For a task to use this configuration, we use the preset property in the task. tasks/base.yaml tasks : task_name : preset : modelling #other task properties Presets can be defined both in project.yaml and in any task group file (files in the tasks folder).","title":"Defining The preset"},{"location":"presets/#preset_inheritance","text":"Presets can reference other presets, the behaviour of this reference being exactly as it works for task. project.yaml presets : modelling : type : autosql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : '{{task.name}}' modelling_view : preset : modelling materialisation : view In the above example, modelling_view is a preset with exactly the same properties as preset modelling except it will generate a view when materialising an autosql task.","title":"Preset Inheritance"},{"location":"project_structure/","text":"SAYN Project Structure \u00b6 SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"Project Structure"},{"location":"project_structure/#sayn_project_structure","text":"SAYN projects are structured as follows: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 base.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 load_data.py \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 log_creator.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt project.yaml : defines the core components of the SAYN project. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. tasks : folder where the task files are stored. Each file is considered a task group. python : folder where python tasks are stored. sql : folder where sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where SQL queries are compiled before execution.","title":"SAYN Project Structure"},{"location":"api/database/","text":"\u00b6 Database \u00b6 Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine. execute ( self , script ) \u00b6 Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required load_data ( self , table , data , schema = None , batch_size = None , replace = False , ** ddl ) \u00b6 Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks {} Returns: Type Description int Number of records loaded read_data ( self , query , ** params ) \u00b6 Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"Database"},{"location":"api/database/#sayn.database","text":"","title":"sayn.database"},{"location":"api/database/#sayn.database.Database","text":"Base class for databases in SAYN. Databases are implemented using sqlalchemy, and the engine attribute is available when working in python tasks without the need for calling create_engine. Attributes: Name Type Description engine sqlalchemy.Engine A sqlalchemy engine referencing the database. name str Name of the db as defined in required_credentials in project.yaml . name_in_yaml str Name of db under credentials in settings.yaml . db_type str Type of the database. metadata sqlalchemy.MetaData A metadata object associated with the engine.","title":"Database"},{"location":"api/database/#sayn.database.Database.execute","text":"Executes a script in the database. Multiple statements are supported. Parameters: Name Type Description Default script sql The SQL script to execute required","title":"execute()"},{"location":"api/database/#sayn.database.Database.load_data","text":"Loads a list of values into the database The default loading mechanism is an INSERT...VALUES, but database drivers will implement more appropriate methods. Parameters: Name Type Description Default table str The name of the target table required data list A list of dictionaries to load required schema str An optional schema to reference the table None batch_size int The max size of each load batch. Defaults to max_batch_rows in the credentials configuration (settings.yaml) None replace bool Indicates whether the target table is to be replaced (True) or new records are to be appended to the existing table (default) False ddl dict An optional ddl specification in the same format as used in autosql and copy tasks {} Returns: Type Description int Number of records loaded","title":"load_data()"},{"location":"api/database/#sayn.database.Database.read_data","text":"Executes the query and returns a list of dictionaries with the data. Parameters: Name Type Description Default query str The SELECT query to execute required params dict sqlalchemy parameters to use when building the final query as per sqlalchemy.engine.Connection.execute {} Returns: Type Description list A list of dictionaries with the results of the query","title":"read_data()"},{"location":"api/python_task/","text":"\u00b6","title":"PythonTask"},{"location":"api/python_task/#sayn.tasks","text":"","title":"sayn.tasks"},{"location":"databases/bigquery/","text":"BigQuery \u00b6 The BigQuery driver depends on pybigquery and can be installed with: pip install \"sayn[bigquery]\" Warning SAYN 0.6 switched from pybigquery to sqlalchemy-bigquery . When upgrading, pybigquery should be uninstalled with pip uninstall pybigquery . The Bigquery connector looks for the following parameters: Parameter Description Default project GCP project where the cluster is Required credentials_path Path relative to the project to the json for the service account to use Required location Default location for tables created Dataset default dataset Dataset to use when running queries. Can be specified in sql For advanced configurations, SAYN will pass other parameters to create_engine , so check the pybigquery dialect for extra parameters. Bigquery Specific DDL \u00b6 Partitioning \u00b6 SAYN supports specifying the partitioning model for tables created with autosql and copy tasks. To do this we specify partition in the ddl field. The value is a string matching a BigQuery partition expression . tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : partition : DATE(_PARTITIONTIME) Clustering \u00b6 We can also specify the clustering for the table with the cluster property in autosql and copy tasks. The value in this case is a list of columns. If the ddl for the task includes the list of columns, the columns specified in the cluster should be present in the column list. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : cluster : - arena_name","title":"BigQuery"},{"location":"databases/bigquery/#bigquery","text":"The BigQuery driver depends on pybigquery and can be installed with: pip install \"sayn[bigquery]\" Warning SAYN 0.6 switched from pybigquery to sqlalchemy-bigquery . When upgrading, pybigquery should be uninstalled with pip uninstall pybigquery . The Bigquery connector looks for the following parameters: Parameter Description Default project GCP project where the cluster is Required credentials_path Path relative to the project to the json for the service account to use Required location Default location for tables created Dataset default dataset Dataset to use when running queries. Can be specified in sql For advanced configurations, SAYN will pass other parameters to create_engine , so check the pybigquery dialect for extra parameters.","title":"BigQuery"},{"location":"databases/bigquery/#bigquery_specific_ddl","text":"","title":"Bigquery Specific DDL"},{"location":"databases/bigquery/#partitioning","text":"SAYN supports specifying the partitioning model for tables created with autosql and copy tasks. To do this we specify partition in the ddl field. The value is a string matching a BigQuery partition expression . tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : partition : DATE(_PARTITIONTIME)","title":"Partitioning"},{"location":"databases/bigquery/#clustering","text":"We can also specify the clustering for the table with the cluster property in autosql and copy tasks. The value in this case is a list of columns. If the ddl for the task includes the list of columns, the columns specified in the cluster should be present in the column list. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : cluster : - arena_name","title":"Clustering"},{"location":"databases/mysql/","text":"MySQL \u00b6 The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/mysql/#mysql","text":"The MySQL driver depends on pymysql and can be installed with: pip install \"sayn[mysql]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default host Host name or public IP of the server Required port Connection port 3306 user User name used to connect Required password Password for that user Required database Database in use upon connection Required Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : mysql-conn : type : mysql host : warehouse.company.com port : 3306 user : mysql_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models Check the sqlalchemy mysql-connector dialect for extra parameters.","title":"MySQL"},{"location":"databases/overview/","text":"Databases \u00b6 About \u00b6 SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: BigQuery MySQL PostgreSQL Redshift Snowflake SQLite Usage \u00b6 Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200 Using Databases In python Tasks \u00b6 Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Overview"},{"location":"databases/overview/#databases","text":"","title":"Databases"},{"location":"databases/overview/#about","text":"SAYN uses sqlalchemy in order to manage database connections. It currently supports the following databases: BigQuery MySQL PostgreSQL Redshift Snowflake SQLite","title":"About"},{"location":"databases/overview/#usage","text":"Database connections are defined as credentials in settings.yaml by specifying the database type and other connection parameters. settings.yaml credentials : db_name : type : redshift host : ... # other connection parameters ... You can check the list of connection parameters in the database specific pages of this section. If a parameter that is not listed on the database page is included in settings.yaml , that parameter will be passed to sqlalchemy.create_engine . Refer to sqlalchemy's documentation if you need to fine tune the connection. For example, to specify the default timezone for a Snowflake connection, this can be specified in the connect_args parameter: settings.yaml credentials : db_name : type : snowflake account : ... # other connection parameters connect_args : timezone : UTC All databases support a parameter max_batch_rows that controls the default size of a batch when using load_data or in copy tasks. If you get an error when running SAYN indicating the amount of data is too large, adjust this value. settings.yaml credentials : dev_db : type : sqlite database : dev.db max_batch_rows : 200","title":"Usage"},{"location":"databases/overview/#using_databases_in_python_tasks","text":"Databases and other credentials defined in the SAYN project are available to Python tasks via self.connections . For convenience though, all Python tasks have a default_db property that gives you access to the default database declared in project.yaml . The database python class provides several methods and properties to make it easier to work with python tasks. For example you can easily read or load data with self.default_db (see example below) or use self.default_db.engine to call DataFrame.read_sql from pandas. Example PythonTask from sayn import PythonTask class TaskPython ( PythonTask ): def run ( self ): data = self . default_db . read_data ( \"SELECT * FROM test_table\" ) # do something with that data","title":"Using Databases In python Tasks"},{"location":"databases/postgresql/","text":"PostgreSQL \u00b6 The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/postgresql/#postgresql","text":"The PostgreSQL driver depends on psycopg2 and can be installed with: pip install \"sayn[postgresql]\" The PostgreSQL connector looks for the following parameters in the credentials settings: Parameter Description Default host Host name or public IP of the server Required port Connection port 5432 user User name used to connect Required password Password for that user Required dbname Database in use upon connection Required Other parameters specified will be passed to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : postgresql-conn : type : postgresql host : warehouse.company.com port : 5432 user : pg_user password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Check the sqlalchemy psycopg2 dialect for extra parameters.","title":"PostgreSQL"},{"location":"databases/redshift/","text":"Redshift \u00b6 The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters. Connection Types \u00b6 SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based. Standard User/Password Connection \u00b6 If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models Connecting With IAM \u00b6 With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters . Redshift Specific DDL \u00b6 Indexes \u00b6 Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition. Sorting \u00b6 Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY . Distribution \u00b6 We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Redshift"},{"location":"databases/redshift/#redshift","text":"The Redshift driver depends on psycopg2 and can be installed with: pip install \"sayn[redshift]\" The Redshift connector looks for the following parameters: Parameter Description Default host Host name or public IP of the cluster Required on standard user/password connection port Connection port 5439 user User name used to connect Required password Password for that user Required on standard user/password connection cluster_id Cluster id as registered in AWS dbname Database in use upon connection Required For advanced configurations, SAYN will pass other parameters to create_engine , so check the sqlalchemy psycopg2 dialect for extra parameters.","title":"Redshift"},{"location":"databases/redshift/#connection_types","text":"SAYN supports 2 connection models for Redshift: standard user/password connection and IAM based.","title":"Connection Types"},{"location":"databases/redshift/#standard_userpassword_connection","text":"If you have a user name and password for redshift use the first model and ensure host and password are specified. settings.yaml credentials : redshift-conn : type : redshift host : my-redshift-cluster.adhfjlasdljfd.eu-west-1.redshift.amazonaws.com port : 5439 user : awsuser password : 'Pas$w0rd' #use quotes to avoid conflict with special characters dbname : models","title":"Standard User/Password Connection"},{"location":"databases/redshift/#connecting_with_iam","text":"With an IAM based connection SAYN uses the AWS API to obtain a temporary password to stablish the connection, so only user, dbname and cluster_id are required. settings.yaml credentials : redshift-conn : type : redshift cluster_id : my-redshift-cluster user : awsuser dbname : models For this connection type to work: boto3 needs to be installed in the project virtual environment pip install boto3 . The AWS cli need to be setup . The user and dbname still need to be specified (use the database user, not the IAM:user ). host and port can be skipped and these values will be obtained using boto3's redshift describe-clusters .","title":"Connecting With IAM"},{"location":"databases/redshift/#redshift_specific_ddl","text":"","title":"Redshift Specific DDL"},{"location":"databases/redshift/#indexes","text":"Redshift doesn't support index definitions, and so autosql and copy tasks will forbid its definition in the ddl entry in the task definition.","title":"Indexes"},{"location":"databases/redshift/#sorting","text":"Table sorting can be specified under the ddl entry in the task definition tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : sorting : columns : - arena_name - fighter1_name With the above example, the table f_battles will be sorted by arena_name and fighter1_name using a compound key (Redshift default). The type of sorting can be changed to interleaved. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : sorting : type : interleaved columns : - arena_name - fighter1_name For more information, read the latest docs about SORTKEY .","title":"Sorting"},{"location":"databases/redshift/#distribution","text":"We can also specify the type of distribution: even, all or key based. If not specified, the Redshift default is even distribution. tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : distribution : all If we want to distribute the table by a given column use the following: tasks/base.yaml tasks : f_battles : type : autosql file_name : f_battles.sql materialisation : table destination : table : f_battles table_properties : distribution : key(tournament_name) For more information, read the latest docs about DISTKEY .","title":"Distribution"},{"location":"databases/snowflake/","text":"Snowflake \u00b6 The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/snowflake/#snowflake","text":"The Snowflake driver depends on the sqlalchemy snowflake and can be installed with: pip install \"sayn[snowflake]\" SAYN will consider the following parameters to construct the sqlalchemy url: Parameter Description Default account account.region Required user User name used to connect Required password Password for that user Required database Database in use upon connection Required role User role to use on this connection Default role for user warehouse Warehouse to use to run queries Default warehouse for user schema Default schema for the connection Other parameters specified will be passed on to sqlalchemy.create_engine when creating the engine. settings.yaml credentials : snowflake-conn : type : snowflake account : xy12345.us-east-1 user : snowflake_user role : etl password : 'Pas$w0rd' #use quotes to avoid conflict with special characters database : models warehouse : etl-warehouse Check the sqlalchemy snowflake dialect for extra parameters.","title":"Snowflake"},{"location":"databases/sqlite/","text":"SQLite \u00b6 SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"databases/sqlite/#sqlite","text":"SAYN will form the sqlalchemy connection URL with the database parameter, which should point to a file path relative to the SAYN project. Any parameter other than database will be passed to sqlalchemy.create_engine . settings.yaml credentials : sqlite-conn : type : sqlite database : [ path_to_database ] Check the sqlalchemy SQLite dialect for extra parameters.","title":"SQLite"},{"location":"project_examples/bbc_news_nlp/","text":"SAYN Project Example: BBC News NLP \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from BBC RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Implementation Details \u00b6 Step 1: Extract Task Group \u00b6 Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data Task Details ( load_data ) \u00b6 First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_bbc_feeds links : - http://feeds.bbci.co.uk/news/england/rss.xml - http://feeds.bbci.co.uk/news/wales/rss.xml - http://feeds.bbci.co.uk/news/scotland/rss.xml - http://feeds.bbci.co.uk/news/northern_ireland/rss.xml - http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml - http://feeds.bbci.co.uk/news/world/middle_east/rss.xml - http://feeds.bbci.co.uk/news/world/latin_america/rss.xml - http://feeds.bbci.co.uk/news/world/europe/rss.xml - http://feeds.bbci.co.uk/news/world/asia/rss.xml - http://feeds.bbci.co.uk/news/world/africa/rss.xml Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending BBC data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method LoadData Class \u00b6 Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_bbc_data : fetches data from the BBC RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_bbc_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. Utility Method ( fetch_bbc_data ) \u00b6 The fetch_bbc_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. Lastly, the function assigns a unique_id to each article which is based on its article id and the source it originates from. This is because the same article may be published in multiple sources with the same id, which means our original ids are not unique and could be misleading. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_bbc_data ( self , link ): \"\"\"Parse and label RSS BBC News data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # remove incompatible columns data . drop ( [ \"title_detail\" , \"summary_detail\" , \"links\" , \"published_parsed\" ], axis = 1 , inplace = True , ) # get the source (this only works for BBC RSS feeds) data [ \"source\" ] = link [ 29 : - 8 ] . replace ( \"/\" , \"_\" ) # generating ids to be unique, since same story ids can be published in different sources data [ \"unique_id\" ] = data [ \"id\" ] + data [ \"source\" ] return data def setup ( self ): self . set_run_steps ([ \"Appending BBC data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending BBC data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_bbc_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml Step 2: Modelling Group \u00b6 Quick Summary: Create the SQL query dim_bbc_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml Task Details ( dim_bbc_feeds ) \u00b6 Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_bbc_feeds.sql sql/dim_bbc_feeds.sql SELECT DISTINCT unique_id , id , title , summary , link , guidislink , published , source FROM {{ user_prefix }} logs_bbc_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_bbc_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data Step 3: Data Science Group \u00b6 Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_bbc_feeds_nlp_stats to calculate aggregate statistics grouped by source Group Overview \u00b6 Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_bbc_feeds table, therefore we will need to set their their table parameters to dim_bbc_feeds . Since both of these tasks are children of the dim_bbc_feeds task, we will also need to set their parents attributes to dim_bbc_feeds . The nlp task has a text parameter, this parameter specifies which columns have text for processing. The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords (e.g. \"say\" and its variations seem to be very common in summaries, however they are not very informative). tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds text : - title - summary wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds stopwords : - say - said - says - will - country - US - England - Scotland - Wales - NI - Ireland - Europe - BBC - yn Task Details ( wordcloud ) \u00b6 The wordcloud task will have the following steps: Grouping texts : aggregates article summaries and groups them by source (summaries are used instead of titles since they tend to be longer) Generating clouds : generates a wordcloud for each source, as well as the full dataset RenderCloud Class \u00b6 Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"firebrick\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) except : mask = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . summary ) sources = df . groupby ( \"source\" ) grouped_texts = sources . summary . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating bbc_wordcloud.png\" ) self . word_cloud ( \"bbc\" , full_text , stopwords , b_colour = \"white\" , c_colour = \"black\" ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords ) return self . success () Task Details ( nlp ) \u00b6 The nlp task will have the following steps: Processing texts : loops through text_fields, generates text statistics on each entry Updating database : similar to LoadData step, has additional debugging information LanguageProcessing Class \u00b6 Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] text_fields = self . parameters [ \"text\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) for t in text_fields : self . info ( f \"Processing texts for { t } field\" ) self . desc_text ( df , t , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success () Task Details ( dim_bbc_feeds_nlp_stats ) \u00b6 Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_bbc_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_bbc_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_tl , AVG ( title_words ) AS average_tw , AVG ( title_sentences ) AS average_ts , AVG ( summary_letters ) AS average_sl , AVG ( summary_words ) AS average_sw , AVG ( summary_sentences ) AS average_ss FROM {{ user_prefix }} dim_bbc_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_bbc_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data dim_bbc_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp Step 4: Run the project \u00b6 All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"SAYN Project Example: BBC News NLP"},{"location":"project_examples/bbc_news_nlp/#sayn_project_example_bbc_news_nlp","text":"","title":"SAYN Project Example: BBC News NLP"},{"location":"project_examples/bbc_news_nlp/#project_description","text":"","title":"Project Description"},{"location":"project_examples/bbc_news_nlp/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from BBC RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data","title":"Overview"},{"location":"project_examples/bbc_news_nlp/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow","title":"Features Used"},{"location":"project_examples/bbc_news_nlp/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/bbc_news_nlp/#implementation_details","text":"","title":"Implementation Details"},{"location":"project_examples/bbc_news_nlp/#step_1_extract_task_group","text":"Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data","title":"Step 1: Extract Task Group"},{"location":"project_examples/bbc_news_nlp/#task_details_load_data","text":"First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_bbc_feeds links : - http://feeds.bbci.co.uk/news/england/rss.xml - http://feeds.bbci.co.uk/news/wales/rss.xml - http://feeds.bbci.co.uk/news/scotland/rss.xml - http://feeds.bbci.co.uk/news/northern_ireland/rss.xml - http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml - http://feeds.bbci.co.uk/news/world/middle_east/rss.xml - http://feeds.bbci.co.uk/news/world/latin_america/rss.xml - http://feeds.bbci.co.uk/news/world/europe/rss.xml - http://feeds.bbci.co.uk/news/world/asia/rss.xml - http://feeds.bbci.co.uk/news/world/africa/rss.xml Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending BBC data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method","title":"Task Details (load_data)"},{"location":"project_examples/bbc_news_nlp/#loaddata_class","text":"Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_bbc_data : fetches data from the BBC RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_bbc_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run.","title":"LoadData Class"},{"location":"project_examples/bbc_news_nlp/#utility_method_fetch_bbc_data","text":"The fetch_bbc_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. Lastly, the function assigns a unique_id to each article which is based on its article id and the source it originates from. This is because the same article may be published in multiple sources with the same id, which means our original ids are not unique and could be misleading. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_bbc_data ( self , link ): \"\"\"Parse and label RSS BBC News data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # remove incompatible columns data . drop ( [ \"title_detail\" , \"summary_detail\" , \"links\" , \"published_parsed\" ], axis = 1 , inplace = True , ) # get the source (this only works for BBC RSS feeds) data [ \"source\" ] = link [ 29 : - 8 ] . replace ( \"/\" , \"_\" ) # generating ids to be unique, since same story ids can be published in different sources data [ \"unique_id\" ] = data [ \"id\" ] + data [ \"source\" ] return data def setup ( self ): self . set_run_steps ([ \"Appending BBC data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending BBC data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_bbc_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml","title":"Utility Method (fetch_bbc_data)"},{"location":"project_examples/bbc_news_nlp/#step_2_modelling_group","text":"Quick Summary: Create the SQL query dim_bbc_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml","title":"Step 2: Modelling Group"},{"location":"project_examples/bbc_news_nlp/#task_details_dim_bbc_feeds","text":"Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_bbc_feeds.sql sql/dim_bbc_feeds.sql SELECT DISTINCT unique_id , id , title , summary , link , guidislink , published , source FROM {{ user_prefix }} logs_bbc_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_bbc_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data","title":"Task Details (dim_bbc_feeds)"},{"location":"project_examples/bbc_news_nlp/#step_3_data_science_group","text":"Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_bbc_feeds_nlp_stats to calculate aggregate statistics grouped by source","title":"Step 3: Data Science Group"},{"location":"project_examples/bbc_news_nlp/#group_overview","text":"Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_bbc_feeds table, therefore we will need to set their their table parameters to dim_bbc_feeds . Since both of these tasks are children of the dim_bbc_feeds task, we will also need to set their parents attributes to dim_bbc_feeds . The nlp task has a text parameter, this parameter specifies which columns have text for processing. The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords (e.g. \"say\" and its variations seem to be very common in summaries, however they are not very informative). tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds text : - title - summary wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_bbc_feeds parameters : table : dim_bbc_feeds stopwords : - say - said - says - will - country - US - England - Scotland - Wales - NI - Ireland - Europe - BBC - yn","title":"Group Overview"},{"location":"project_examples/bbc_news_nlp/#task_details_wordcloud","text":"The wordcloud task will have the following steps: Grouping texts : aggregates article summaries and groups them by source (summaries are used instead of titles since they tend to be longer) Generating clouds : generates a wordcloud for each source, as well as the full dataset","title":"Task Details (wordcloud)"},{"location":"project_examples/bbc_news_nlp/#rendercloud_class","text":"Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"firebrick\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) except : mask = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . summary ) sources = df . groupby ( \"source\" ) grouped_texts = sources . summary . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating bbc_wordcloud.png\" ) self . word_cloud ( \"bbc\" , full_text , stopwords , b_colour = \"white\" , c_colour = \"black\" ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords ) return self . success ()","title":"RenderCloud Class"},{"location":"project_examples/bbc_news_nlp/#task_details_nlp","text":"The nlp task will have the following steps: Processing texts : loops through text_fields, generates text statistics on each entry Updating database : similar to LoadData step, has additional debugging information","title":"Task Details (nlp)"},{"location":"project_examples/bbc_news_nlp/#languageprocessing_class","text":"Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] text_fields = self . parameters [ \"text\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) for t in text_fields : self . info ( f \"Processing texts for { t } field\" ) self . desc_text ( df , t , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success ()","title":"LanguageProcessing Class"},{"location":"project_examples/bbc_news_nlp/#task_details_dim_bbc_feeds_nlp_stats","text":"Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_bbc_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_bbc_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_tl , AVG ( title_words ) AS average_tw , AVG ( title_sentences ) AS average_ts , AVG ( summary_letters ) AS average_sl , AVG ( summary_words ) AS average_sw , AVG ( summary_sentences ) AS average_ss FROM {{ user_prefix }} dim_bbc_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_bbc_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_bbc_feeds : preset : modelling parents : - load_data dim_bbc_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp","title":"Task Details (dim_bbc_feeds_nlp_stats)"},{"location":"project_examples/bbc_news_nlp/#step_4_run_the_project","text":"All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Step 4: Run the project"},{"location":"project_examples/facebook_data_project/","text":"SAYN Project Example: Facebook Data Project \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts Facebook Messenger data Loads it into a SQLite database Cleans the extracted data Calculates reply times for chat data Performs some basic text and sentiment analysis on the transformed data Generates wordcloud timelapse GIFs for each conversation Generates a bar chart race GIF for most shared sites in chat data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: Data processing: numpy , pandas , nltk , vaderSentiment Visualisations: matplotlib , wordcloud , pillow , bar_chart_race By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. You can also connect this database to your preferred visualisation tool. Running The Project \u00b6 To run the project, you will need to: clone the repository with git clone https://github.com/173TECH/facebook_data_project.git . rename the sample_settings.yaml file to settings.yaml . install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. install ImageMagick , details here: https://imagemagick.org/ use sayn run from the root of the project folder to run all SAYN commands. Attention This project comes with a sample dataset, you should use this dataset to test run the project. After a successful run you should see 3 new files in python/img , these should be the following: sample_Goku_timelapse.gif sample_Vegeta_timelapse.gif chart_race.gif Adding Your Facebook Messenger Data \u00b6 For this you will need your Facebook Messenger data in JSON format, you can get request it by doing the following: Sign in to Facebook Go to Settings & Privacy > Settings > Your Facebook Information > Download Your Information Change format to JSON and click Create File (this can take a while depending on your date range and media quality) Once you have the data, you can find the chat data in messages/inbox (you should see a collection of folders corresponding to each of your chats): Copy and paste the chat folders you are interested into the data folder in this project. In tasks/data_science.yaml , change the facebook_name parameter to your full name on Facebook Note: If you use a large amount of chat data you will experience longer load times for certain tasks Note If you use a large amount of chat data you will experience longer load times for certain tasks","title":"Facebook Data Project"},{"location":"project_examples/facebook_data_project/#sayn_project_example_facebook_data_project","text":"","title":"SAYN Project Example: Facebook Data Project"},{"location":"project_examples/facebook_data_project/#project_description","text":"","title":"Project Description"},{"location":"project_examples/facebook_data_project/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts Facebook Messenger data Loads it into a SQLite database Cleans the extracted data Calculates reply times for chat data Performs some basic text and sentiment analysis on the transformed data Generates wordcloud timelapse GIFs for each conversation Generates a bar chart race GIF for most shared sites in chat data","title":"Overview"},{"location":"project_examples/facebook_data_project/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: Data processing: numpy , pandas , nltk , vaderSentiment Visualisations: matplotlib , wordcloud , pillow , bar_chart_race By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. You can also connect this database to your preferred visualisation tool.","title":"Features Used"},{"location":"project_examples/facebook_data_project/#running_the_project","text":"To run the project, you will need to: clone the repository with git clone https://github.com/173TECH/facebook_data_project.git . rename the sample_settings.yaml file to settings.yaml . install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. install ImageMagick , details here: https://imagemagick.org/ use sayn run from the root of the project folder to run all SAYN commands. Attention This project comes with a sample dataset, you should use this dataset to test run the project. After a successful run you should see 3 new files in python/img , these should be the following: sample_Goku_timelapse.gif sample_Vegeta_timelapse.gif chart_race.gif","title":"Running The Project"},{"location":"project_examples/facebook_data_project/#adding_your_facebook_messenger_data","text":"For this you will need your Facebook Messenger data in JSON format, you can get request it by doing the following: Sign in to Facebook Go to Settings & Privacy > Settings > Your Facebook Information > Download Your Information Change format to JSON and click Create File (this can take a while depending on your date range and media quality) Once you have the data, you can find the chat data in messages/inbox (you should see a collection of folders corresponding to each of your chats): Copy and paste the chat folders you are interested into the data folder in this project. In tasks/data_science.yaml , change the facebook_name parameter to your full name on Facebook Note: If you use a large amount of chat data you will experience longer load times for certain tasks Note If you use a large amount of chat data you will experience longer load times for certain tasks","title":"Adding Your Facebook Messenger Data"},{"location":"project_examples/reddit_news_nlp/","text":"SAYN Project Example: Reddit News NLP \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from Reddit RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data Features Used \u00b6 Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Implementation Details \u00b6 Step 1: Extract Task Group \u00b6 Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data Task Details ( load_data ) \u00b6 First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_reddit_feeds links : - https://www.reddit.com/r/USnews/new/.rss - https://www.reddit.com/r/UKnews/new/.rss - https://www.reddit.com/r/EUnews/new/.rss Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending Reddit data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method LoadData Class \u00b6 Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_reddit_data : fetches data from the Reddit RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_reddit_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. Utility Method ( fetch_reddit_data ) \u00b6 The fetch_reddit_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_reddit_data ( self , link ): \"\"\"Parse and label RSS Reddit data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # select columns of interest data = data . loc [:, [ \"id\" , \"link\" , \"updated\" , \"published\" , \"title\" ]] # get the source, only works for Reddit RSS feeds source_elements = link . split ( \"/\" ) data [ \"source\" ] = source_elements [ 4 ] + \"_\" + source_elements [ 5 ] return data def setup ( self ): self . set_run_steps ([ \"Appending Reddit data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending Reddit data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_reddit_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml Step 2: Modelling Group \u00b6 Quick Summary: Create the SQL query dim_reddit_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml Task Details ( dim_reddit_feeds ) \u00b6 Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_reddit_feeds.sql sql/dim_reddit_feeds.sql SELECT DISTINCT id , title , published , updated , link , source FROM {{ user_prefix }} logs_reddit_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_reddit_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data Step 3: Data Science Group \u00b6 Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_reddit_feeds_nlp_stats to calculate aggregate statistics grouped by source Group Overview \u00b6 Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_reddit_feeds table, therefore we will need to set their their table parameters to dim_reddit_feeds . Since both of these tasks are children of the dim_reddit_feeds task, we will also need to set their parents attributes to dim_reddit_feeds . The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords. tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds stopwords : - Reddit Task Details ( wordcloud ) \u00b6 The wordcloud task will have the following steps: Grouping texts : aggregates article titles and groups them by source Generating clouds : generates a wordcloud for each source, as well as the full dataset RenderCloud Class \u00b6 Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"black\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) image_colours = ImageColorGenerator ( mask ) except : mask = None image_colours = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , color_func = image_colours , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . title ) sources = df . groupby ( \"source\" ) grouped_texts = sources . title . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating reddit_wordcloud.png\" ) self . word_cloud ( \"reddit\" , full_text , stopwords ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords , b_colour = \"black\" , c_colour = \"white\" ) return self . success () Task Details ( nlp ) \u00b6 The nlp task will have the following steps: Processing texts : generates text statistics for each title Updating database : similar to LoadData step, has additional debugging information LanguageProcessing Class \u00b6 Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) self . info ( f \"Processing texts for title field\" ) self . desc_text ( df , \"title\" , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success () Task Details ( dim_reddit_feeds_nlp_stats ) \u00b6 Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_reddit_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_reddit_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_letters , AVG ( title_words ) AS average_words , AVG ( title_sentences ) AS average_sentences FROM {{ user_prefix }} dim_reddit_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_reddit_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data dim_reddit_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp Step 4: Run the project \u00b6 All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Reddit News NLP"},{"location":"project_examples/reddit_news_nlp/#sayn_project_example_reddit_news_nlp","text":"","title":"SAYN Project Example: Reddit News NLP"},{"location":"project_examples/reddit_news_nlp/#project_description","text":"","title":"Project Description"},{"location":"project_examples/reddit_news_nlp/#overview","text":"This is an example SAYN project which shows how to use SAYN for data modelling and processing. You can find the GitHub repository here . This project does the following: Extracts article data from Reddit RSS feeds Loads it into a SQLite database Cleans the extracted data Performs some basic text analysis on the transformed data","title":"Overview"},{"location":"project_examples/reddit_news_nlp/#features_used","text":"Python tasks to extract and analyse data Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. In addition to SAYN, this project uses the following packages: RSS feed data extraction: feedparser Data processing: numpy , pandas , nltk Visualisations: matplotlib , wordcloud , pillow","title":"Features Used"},{"location":"project_examples/reddit_news_nlp/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_nlp_news_scraping . Rename the sample_settings.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/reddit_news_nlp/#implementation_details","text":"","title":"Implementation Details"},{"location":"project_examples/reddit_news_nlp/#step_1_extract_task_group","text":"Quick Summary: Create the task group extract.yaml Create a python task to extract and load the data","title":"Step 1: Extract Task Group"},{"location":"project_examples/reddit_news_nlp/#task_details_load_data","text":"First, we need to define our extract group in our tasks folder. This group will only include the load_data task. This is quite a simple python task which will use the LoadData class from load_data.py which we will create later. Our load_data task will have two parameters : table : name of the table we plan to create in our database links : list of links to rss feeds tasks/extract.yaml tasks : load_data : type : python class : load_data.LoadData parameters : table : logs_reddit_feeds links : - https://www.reddit.com/r/USnews/new/.rss - https://www.reddit.com/r/UKnews/new/.rss - https://www.reddit.com/r/EUnews/new/.rss Note Parameters are not a requirement, however parameters make the code dynamic which is useful for reusability. The load_data task will have the following steps: Appending Reddit data to dataframe : loops through the links array, appends data from each link to a dataframe Updating database : loads dataframe into SQLite database using pandas.to_sql method","title":"Task Details (load_data)"},{"location":"project_examples/reddit_news_nlp/#loaddata_class","text":"Next, we will create our LoadData class. Our LoadData inherits properties from SAYN's PythonTask, in addition it will have 3 methods: fetch_reddit_data : fetches data from the Reddit RSS feeds setup : sets the order of steps to run run : defines what each step does during the run Attention fetch_reddit_data is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run.","title":"LoadData Class"},{"location":"project_examples/reddit_news_nlp/#utility_method_fetch_reddit_data","text":"The fetch_reddit_data function uses the feedparser.parse method to fetch the raw data from the rss feed link. It then converts the data into a pandas dataframe to make it easier to work with. The function also extracts the source of each article and adds it under the source column. python/load_data.py import pandas as pd import feedparser as f from sayn import PythonTask class LoadData ( PythonTask ): def fetch_reddit_data ( self , link ): \"\"\"Parse and label RSS Reddit data then return it in a pandas DataFrame\"\"\" # get data from supplied link raw_data = f . parse ( link ) # transform data to dataframe data = pd . DataFrame ( raw_data . entries ) # select columns of interest data = data . loc [:, [ \"id\" , \"link\" , \"updated\" , \"published\" , \"title\" ]] # get the source, only works for Reddit RSS feeds source_elements = link . split ( \"/\" ) data [ \"source\" ] = source_elements [ 4 ] + \"_\" + source_elements [ 5 ] return data def setup ( self ): self . set_run_steps ([ \"Appending Reddit data to dataframe\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Appending Reddit data to dataframe\" ): links = self . parameters [ \"links\" ] table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame () for link in links : temp_df = self . fetch_reddit_data ( link ) n_rows = len ( temp_df ) df = df . append ( temp_df ) self . info ( f \"Loading { n_rows } rows into destination: { table } ....\" ) with self . step ( \"Updating database\" ): if df is not None : df . to_sql ( table , self . default_db . engine , if_exists = \"append\" , index = False ) return self . success () Tip self.parameters[\"user_prefix\"] is set dynamically based on what you set it to in project.yaml, this can also be overwritten in settings.yaml","title":"Utility Method (fetch_reddit_data)"},{"location":"project_examples/reddit_news_nlp/#step_2_modelling_group","text":"Quick Summary: Create the SQL query dim_reddit_feeds.sql to filter out duplicates Create a modelling preset in project.yaml Create the task group modelling.yaml","title":"Step 2: Modelling Group"},{"location":"project_examples/reddit_news_nlp/#task_details_dim_reddit_feeds","text":"Currently our load_data task appends data to our database but it does not filter out any potential duplicates that we might encounter after multiple runs. This is where the modelling group comes in, we can define an AutoSQL task to filter out any duplicates. First, we need to create a sql query in our sql folder that will filter out any duplicates; we will call it dim_reddit_feeds.sql sql/dim_reddit_feeds.sql SELECT DISTINCT id , title , published , updated , link , source FROM {{ user_prefix }} logs_reddit_feeds Tip {{user_prefix}} is set dynamically. The default value is set in project.yaml . This can be overwritten using profiles in settings.yaml . Next, we will define a modelling preset in project.yaml . Presets enable you to create a task prototype which can be reused when defining tasks. Hence, the modelling preset will simplify the code in modelling.yaml while also allowing us to set dynamic file and table names. Attention Presets defined in project.yaml are project level presets, you can also define presets within individual task groups. project.yaml required_credentials : - warehouse default_db : warehouse presets : modelling : type : autosql materialisation : table file_name : \"{{ task.name }}.sql\" destination : table : \"{{ user_prefix }}{{ task.name }}\" parameters : user_prefix : Tip {{ task.name }} returns the name of task Now that we have the modelling preset , we can use it in the modelling group. Since we want dim_reddit_feeds to run after our load_data task, we will need to set the parents of the task to load_data . tasks/modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data","title":"Task Details (dim_reddit_feeds)"},{"location":"project_examples/reddit_news_nlp/#step_3_data_science_group","text":"Quick Summary: Create the task group data_science.yaml Create the python task wordcloud to generate wordclouds Create the python task nlp to generate text statistics Create the AutoSQL task dim_reddit_feeds_nlp_stats to calculate aggregate statistics grouped by source","title":"Step 3: Data Science Group"},{"location":"project_examples/reddit_news_nlp/#group_overview","text":"Now that we have our cleaned dataset, we can utilise python tasks to do some natural language processing on our text data. In particular, we will use two libraries for this analysis: nltk : for basic text statistics wordcloud : for generating wordcloud visualisations First, we need to create the data_science group in the tasks folder. There will be two tasks within this group: nlp : generates the text statistics wordcloud : generates the wordclouds Both tasks will use data from our dim_reddit_feeds table, therefore we will need to set their their table parameters to dim_reddit_feeds . Since both of these tasks are children of the dim_reddit_feeds task, we will also need to set their parents attributes to dim_reddit_feeds . The wordcloud task has a stopwords parameter, this parameter provides additional context related stopwords. tasks/data_science.yaml tasks : nlp : type : python class : nlp.LanguageProcessing parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds wordcloud : type : python class : wordcloud.RenderCloud parents : - dim_reddit_feeds parameters : table : dim_reddit_feeds stopwords : - Reddit","title":"Group Overview"},{"location":"project_examples/reddit_news_nlp/#task_details_wordcloud","text":"The wordcloud task will have the following steps: Grouping texts : aggregates article titles and groups them by source Generating clouds : generates a wordcloud for each source, as well as the full dataset","title":"Task Details (wordcloud)"},{"location":"project_examples/reddit_news_nlp/#rendercloud_class","text":"Next, we can define the class RenderCloud for the wordcloud task. RenderCloud has 3 methods: word_cloud : generates a wordcloud visualisation setup : sets the order of steps to run run : defines what each step does during the run Attention word_cloud is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/wordcloud.py import pandas as pd import numpy as np import matplotlib.pyplot as plt from sayn import PythonTask from PIL import Image from wordcloud import WordCloud , STOPWORDS , ImageColorGenerator class RenderCloud ( PythonTask ): def word_cloud ( self , name , text , stopwords , b_colour = \"white\" , c_colour = \"black\" , show = False ): \"\"\"Word cloud generating function\"\"\" # attempt to find a compatible mask try : mask = np . array ( Image . open ( f \"python/img/masks/ { name } _mask.png\" )) image_colours = ImageColorGenerator ( mask ) except : mask = None image_colours = None wordcloud = WordCloud ( stopwords = stopwords , max_words = 100 , mask = mask , background_color = b_colour , contour_width = 1 , contour_color = c_colour , color_func = image_colours , ) . generate ( text ) # store wordcloud image in \"python/img\" wordcloud . to_file ( f \"python/img/ { name } _wordcloud.png\" ) # declare show=True if you want to show wordclouds if show : plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( \"off\" ) plt . show () def setup ( self ): self . set_run_steps ([ \"Grouping texts\" , \"Generating clouds\" ]) return self . success () def run ( self ): with self . step ( \"Grouping texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) full_text = \" \" . join ( article for article in df . title ) sources = df . groupby ( \"source\" ) grouped_texts = sources . title . sum () with self . step ( \"Generating clouds\" ): stopwords = STOPWORDS . update ( self . parameters [ \"stopwords\" ]) self . info ( \"Generating reddit_wordcloud.png\" ) self . word_cloud ( \"reddit\" , full_text , stopwords ) # Source specific wordclouds for group , text in zip ( grouped_texts . keys (), grouped_texts ): self . info ( f \"Generating { group } _wordcloud.png\" ) self . word_cloud ( group , text , stopwords , b_colour = \"black\" , c_colour = \"white\" ) return self . success ()","title":"RenderCloud Class"},{"location":"project_examples/reddit_news_nlp/#task_details_nlp","text":"The nlp task will have the following steps: Processing texts : generates text statistics for each title Updating database : similar to LoadData step, has additional debugging information","title":"Task Details (nlp)"},{"location":"project_examples/reddit_news_nlp/#languageprocessing_class","text":"Moving on, we can define the class LanguageProcessing for the nlp task. LanguageProcessing has 3 methods: desc_text : provides counts of letters, words and sentences in an article setup : sets the order of steps to run run : defines what each step does during the run Attention desc_text is a utility method for this task, while setup and run are the usual SAYN methods. Please note that methods setup and run need to return either self.success() or self.fail() in order to run. python/nlp.py import pandas as pd from sayn import PythonTask from nltk import download from nltk.tokenize import word_tokenize , sent_tokenize download ( \"punkt\" ) class LanguageProcessing ( PythonTask ): def desc_text ( self , df , text_field , language ): \"\"\"Text stats generating function\"\"\" # counts the number of letters in text_field df [ text_field + \"_letters\" ] = df [ text_field ] . fillna ( \"\" ) . str . len () # counts the number of words in text_field df [ text_field + \"_words\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( word_tokenize ( x , language = language ))) ) # counts the number of sentences in text_field df [ text_field + \"_sentences\" ] = ( df [ text_field ] . fillna ( \"\" ) . apply ( lambda x : len ( sent_tokenize ( x , language = language ))) ) def setup ( self ): self . set_run_steps ([ \"Processing texts\" , \"Updating database\" ]) return self . success () def run ( self ): with self . step ( \"Processing texts\" ): table = self . parameters [ \"user_prefix\" ] + self . task_parameters [ \"table\" ] df = pd . DataFrame ( self . default_db . read_data ( f \"SELECT * FROM { table } \" )) self . info ( f \"Processing texts for title field\" ) self . desc_text ( df , \"title\" , \"english\" ) with self . step ( \"Updating database\" ): if df is not None : output = f \" { table } _ { self . name } \" n_rows = len ( df ) self . info ( f \"Loading { n_rows } rows into destination: { output } ....\" ) df . to_sql ( output , self . default_db . engine , if_exists = \"replace\" , index = False ) return self . success ()","title":"LanguageProcessing Class"},{"location":"project_examples/reddit_news_nlp/#task_details_dim_reddit_feeds_nlp_stats","text":"Now that we have individual article statistics, it would be a good idea to create an additional modelling task to find some aggregate statistics grouped by source. Let's create another SQL query called dim_reddit_feeds_nlp_stats in the sql folder. This query will give us the average, grouped by source, of the text statistics generated by the nlp task. sql/dim_reddit_feeds_nlp_stats.py SELECT source , AVG ( title_letters ) AS average_letters , AVG ( title_words ) AS average_words , AVG ( title_sentences ) AS average_sentences FROM {{ user_prefix }} dim_reddit_feeds_nlp GROUP BY 1 ORDER BY 1 Finally, we can add the dim_reddit_feeds_nlp_stats task to the the modelling group. Like the previous modelling task, we will create this task using the modelling preset in project.yaml ; setting the parents parameter to nlp . We want to materialise this query as a view; therefore, we will need to overwrite the materialisation parameter of the preset. modelling.yaml tasks : dim_reddit_feeds : preset : modelling parents : - load_data dim_reddit_feeds_nlp_stats : preset : modelling materialisation : view parents : - nlp","title":"Task Details (dim_reddit_feeds_nlp_stats)"},{"location":"project_examples/reddit_news_nlp/#step_4_run_the_project","text":"All that's left is to run the project in the command line. Change your directory to this project's folder and enter sayn run . Attention Please note that if you did not clone the git repo, you may have some issues with the wordcloud generation. We recommend you create a folder called img within the python folder, if you do not already have one.","title":"Step 4: Run the project"},{"location":"project_examples/simple_etl/","text":"SAYN Project Example: A Simple ETL \u00b6 Project Description \u00b6 Overview \u00b6 This is an example SAYN project which shows how to implement a simple ETL with the framework. You can find the GitHub repository here . This ETL extracts jokes from an API, translates them into Yodish (the language of Yoda, this is) with another API and then runs some SQL transformations on the extracted data. Both APIs are public and do not require an API key. However, they both have limited quotas (especially the Yodish translation API) so you should avoid re-running the extraction part of the project multiple times in a row (you can use the command sayn run -x tag:extract after the first sayn run ). Features Used \u00b6 Python task to extract data with APIs. Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily. Running The Project \u00b6 Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_simple_etl.git . Rename the settings_sample.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder. Running The Project With PostgreSQL \u00b6 If desired, you can also run the project using a PostgreSQL database. For this, you simply need to: Change the warehouse credential to use a PostgreSQL database connection. Install psycopg2 as a package.","title":"A Simple ETL"},{"location":"project_examples/simple_etl/#sayn_project_example_a_simple_etl","text":"","title":"SAYN Project Example: A Simple ETL"},{"location":"project_examples/simple_etl/#project_description","text":"","title":"Project Description"},{"location":"project_examples/simple_etl/#overview","text":"This is an example SAYN project which shows how to implement a simple ETL with the framework. You can find the GitHub repository here . This ETL extracts jokes from an API, translates them into Yodish (the language of Yoda, this is) with another API and then runs some SQL transformations on the extracted data. Both APIs are public and do not require an API key. However, they both have limited quotas (especially the Yodish translation API) so you should avoid re-running the extraction part of the project multiple times in a row (you can use the command sayn run -x tag:extract after the first sayn run ).","title":"Overview"},{"location":"project_examples/simple_etl/#features_used","text":"Python task to extract data with APIs. Autosql tasks to automate SQL transformations. Usage of parameters to make the code dynamic. Usage of presets to define tasks. By default, the project uses SQLite as a database. You can use DB Browser for SQLite to navigate the data easily.","title":"Features Used"},{"location":"project_examples/simple_etl/#running_the_project","text":"Clone the repository with the command git clone https://github.com/173TECH/sayn_project_example_simple_etl.git . Rename the settings_sample.yaml file to settings.yaml . Install the project dependencies by running the pip install -r requirements.txt command from the root of the project folder. Run all SAYN commands from the root of the project folder.","title":"Running The Project"},{"location":"project_examples/simple_etl/#running_the_project_with_postgresql","text":"If desired, you can also run the project using a PostgreSQL database. For this, you simply need to: Change the warehouse credential to use a PostgreSQL database connection. Install psycopg2 as a package.","title":"Running The Project With PostgreSQL"},{"location":"settings/project_yaml/","text":"Settings: project.yaml \u00b6 The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse schema_prefix : analytics parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' groups : group1 : type : sql file_name : \"group1/*.sql\" Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details. groups Defines groups that automatically generate tasks based on a list of files or a python module. See the task overview and python tasks for more details. prefix/suffix/override Settings to modify database object references","title":"project.yaml"},{"location":"settings/project_yaml/#settings_projectyaml","text":"The project.yaml defines the core components of the SAYN project. It is shared across all collaborators . project.yaml required_credentials : - warehouse default_db : warehouse schema_prefix : analytics parameters : user_prefix : '' schema_logs : analytics_logs schema_staging : analytics_staging schema_models : analytics_models presets : preset1 : type : sql file_name : '{{ task.name }}.sql' groups : group1 : type : sql file_name : \"group1/*.sql\" Property Description Default required_credentials The list of credentials used by the project. Credentials details are defined the settings.yaml file. Required default_db The credential used by default by sql and autosql tasks. Entry in required_credentials if only 1 defined parameters Project parameters used to make the tasks dynamic. They are overwritten by profile parameters in settings.yaml . See the Parameters section for more details. presets Defines preset task structures so task can inherit attributes from those presets directly. See the Presets section for more details. groups Defines groups that automatically generate tasks based on a list of files or a python module. See the task overview and python tasks for more details. prefix/suffix/override Settings to modify database object references","title":"Settings: project.yaml"},{"location":"settings/settings_yaml/","text":"Settings: settings.yaml \u00b6 The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production. Defining Credentials \u00b6 Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects. Using Environment Variables \u00b6 Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are either basic types (ie: strings), json or yaml encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh # JSON encoded credential export SAYN_CREDENTIAL_warehouse = '{\"type\": \"snowflake\", \"account\": ...}' # YAML encoded credential export SAYN_CREDENTIAL_backend = \" type: postgresql host: host.address.com user: ... \" # Project parameters as strings export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" # Project parameters allow complex types JSON or YAML encoded export SAYN_PARAMETER_dict_param = \" key1: value1 key2: value2 \" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence. Default run \u00b6 When a team member only works on a subset of the SAYN project (for example, a data analyst that works only with modelling tasks) it's useful to automatically filter the tasks that will be executed when we run sayn run . For this we can use the default_run configuration, which is set in the profile in your settings.yaml or through the environment variable SAYN_DEFAULT_RUN : settings.yaml profile: dev: default_run: -x group:extract .env.sh export SAYN_DEFAULT_RUN=\"-x group:extract\" So we just add the arguments we would give after sayn run or sayn compile . Only task selection and upstream prod are allowed ( -t/--tasks , -x/--exclude and -u/--upstream-prod ).","title":"settings.yaml"},{"location":"settings/settings_yaml/#settings_settingsyaml","text":"The settings.yaml defines local configuration like credentials. This file is unique to each SAYN user collaborating on the project and is automatically ignored by git. Warning settings.yaml should never be pushed to git as it contains credentials for databases and APIs used by the SAYN project. settings.yaml default_profile : dev profiles : dev : credentials : warehouse : snowflake-songoku parameters : table_prefix : songoku_ schema_logs : analytics_logs schema_staging : analytics_adhoc schema_models : analytics_adhoc prod : credentials : warehouse : snowflake-prod # no need for prod parameters as those are read from project.yaml credentials : snowflake-songoku : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] snowflake-prod : type : snowflake account : [ snowflake-account ] user : [ user-name ] password : '[password]' database : [ database ] schema : [ schema ] warehouse : [ warehouse ] role : [ role ] Property Description Default profiles A map of available profiles to the user. credentials and parameters are defined for each profile. Those parameters overwrite the parameters set in project.yaml . Required default_profile The profile used by default at execution time. Entry in required_credentials if only 1 defined credentials The list of credentials used in profiles to link required_credentials in project.yaml . Required This file enables the user to use two different profiles whenever desired: dev and prod . It is usually good practice to separate your environments in order to ensure that testing is never done directly on production.","title":"Settings: settings.yaml"},{"location":"settings/settings_yaml/#defining_credentials","text":"Credentials includes both databases (eg: your warehouse) as well as custom secrets used by python tasks. For a definition of a database connection see to the documentation for your database type For custom credentials, use the type: api and include values required: settings.yaml credentials : credential_name : type : api api_key : 'api_key' All credentials are accessible through self.connections['credential_name'] where credential_name is the name given in required_credentials. API credentials when accessed in python are defined as dictionary, whereas database connections are Database objects.","title":"Defining Credentials"},{"location":"settings/settings_yaml/#using_environment_variables","text":"Local settings can be set without the need of a settings.yaml file using environment variables instead. With environment variables we don't need to set profiles, only credentials and project parameters are defined. SAYN will interpret any environment variable names SAYN_CREDENTIAL_name or SAYN_PARAMETER_name . The values when using environment variables are either basic types (ie: strings), json or yaml encoded. Taking the settings.yaml example above for the dev profile, in environment variables: .env.sh # JSON encoded credential export SAYN_CREDENTIAL_warehouse = '{\"type\": \"snowflake\", \"account\": ...}' # YAML encoded credential export SAYN_CREDENTIAL_backend = \" type: postgresql host: host.address.com user: ... \" # Project parameters as strings export SAYN_PARAMETER_table_prefix = \"songoku_\" export SAYN_PARAMETER_schema_logs = \"analytics_logs\" export SAYN_PARAMETER_schema_staging = \"analytics_adhoc\" export SAYN_PARAMETER_schema_models = \"analytics_adhoc\" # Project parameters allow complex types JSON or YAML encoded export SAYN_PARAMETER_dict_param = \" key1: value1 key2: value2 \" When environement variables are defined and a settings.yaml file exists, the settings from both will be combined with the environment variables taking precedence.","title":"Using Environment Variables"},{"location":"settings/settings_yaml/#default_run","text":"When a team member only works on a subset of the SAYN project (for example, a data analyst that works only with modelling tasks) it's useful to automatically filter the tasks that will be executed when we run sayn run . For this we can use the default_run configuration, which is set in the profile in your settings.yaml or through the environment variable SAYN_DEFAULT_RUN : settings.yaml profile: dev: default_run: -x group:extract .env.sh export SAYN_DEFAULT_RUN=\"-x group:extract\" So we just add the arguments we would give after sayn run or sayn compile . Only task selection and upstream prod are allowed ( -t/--tasks , -x/--exclude and -u/--upstream-prod ).","title":"Default run"},{"location":"tasks/autosql/","text":"autosql Task \u00b6 About \u00b6 The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you. Defining autosql Tasks \u00b6 An autosql task group is defined as follows: project.yaml ... groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" ... An autosql task is defined by the following attributes: type : autosql . file_name : the path to a file within the sql folder of the project's root . When defining autosql groups in project.yaml this property needs to be a glob expression, for example group/*.sql . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. The final compiled value is affected by schema_prefix , schema_suffix and schema_override as specified in database objects . schema : the (optional) destination schema where the object will be created. The final compiled value is affected by schema_prefix , schema_suffix and schema_override as specified in database objects . table : is the name of the object that will be created. The final compiled value is affected by table_prefix , table_suffix and table_override as specified in database objects . db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info By default the task is executed in the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . Setting Dependencies With autosql \u00b6 With autosql tasks, you should use the src macro in your SELECT statements to implicitly create task dependencies. !!! example autosql query SELECT field1 , field2 FROM {{ src('my_table') }} l By using the {{ src('my_table') }} in your FROM clause, you are effectively telling SAYN that your task depends on the my_table table (or view). As a result, SAYN will look for the task that produces my_table and set it as a parent of this autosql task automatically. Tip When using the src macro, you can pass a structure formatted as schema.table such as {{ src('my_schema.my_table') }} . In this case, SAYN interprets the first element as the schema, the second element as the table or view. If you use schema_prefix and / or table_prefix in your project settings, SAYN will then prepend the schema_prefix to the schema value and table_prefix to the table value. For example, if your schema_prefix is set to analytics and table_prefix to up then {{ src('my_schema.my_table') }} will compile analytics_my_schema.up_my_table . Advanced Configuration \u00b6 If you need to amend the configuration (e.g. materialisation) of a specific autosql task within a group , you can overload the values specified in the YAML group definition. To do this, we simply call config from a Jinja tag within the sql file of the task: autosql with config {{ config(materialisation='view') }} SELECT ... The above code will override the value of materialisation setting defined in YAML to make this model a view. All other parameters described above in this page are also available to overload with config except db , file_name and name . Using autosql In incremental Mode \u00b6 autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2 Defining columns \u00b6 Autosql tasks accept a columns field in the task definition that affects the table creation by enforcing types and column order. Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages. CREATE TABLE DDLs \u00b6 SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. table_properties: database specific properties that affect table creation (indexes, cluster, sorting, etc.). post_hook: SQL statments executed right after the table/view creation. columns can define the following attributes: name: the column name. type: the column type. tests: list of keywords that constraint a specific column unique: enforces a unique constraint on the column. not_null: enforces a non null constraint on the column. allowed_values: list allowed values for the column. table_properties can define the following attributes (database specific): * indexes: * sorting: specify the sorting for the table * distribution_key: specify the type of distribution. * partitioning: specify the partitioning model for the table. * clustering: specify the clustering for the table. Attention Each supported database might have specific table_properties related to it; see the database-specific pages for further details and examples. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"AutoSQL"},{"location":"tasks/autosql/#autosql_task","text":"","title":"autosql Task"},{"location":"tasks/autosql/#about","text":"The autosql task lets you write a SELECT statement and SAYN then automates the data processing (i.e. table or view creation, incremental load, etc.) for you.","title":"About"},{"location":"tasks/autosql/#defining_autosql_tasks","text":"An autosql task group is defined as follows: project.yaml ... groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" ... An autosql task is defined by the following attributes: type : autosql . file_name : the path to a file within the sql folder of the project's root . When defining autosql groups in project.yaml this property needs to be a glob expression, for example group/*.sql . materialisation : this should be either table , view or incremental . table will create a table, view will create a view. incremental will create a table and will load the data incrementally based on a delete key (see more detail on incremental below). destination : this sets the details of the data processing. tmp_schema : the (optional) schema which will be used to store any necessary temporary object created in the process. The final compiled value is affected by schema_prefix , schema_suffix and schema_override as specified in database objects . schema : the (optional) destination schema where the object will be created. The final compiled value is affected by schema_prefix , schema_suffix and schema_override as specified in database objects . table : is the name of the object that will be created. The final compiled value is affected by table_prefix , table_suffix and table_override as specified in database objects . db : the (optional) destination database. delete_key : specifies the incremental process delete key. This is for incremental materialisation only. Info By default the task is executed in the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases .","title":"Defining autosql Tasks"},{"location":"tasks/autosql/#setting_dependencies_with_autosql","text":"With autosql tasks, you should use the src macro in your SELECT statements to implicitly create task dependencies. !!! example autosql query SELECT field1 , field2 FROM {{ src('my_table') }} l By using the {{ src('my_table') }} in your FROM clause, you are effectively telling SAYN that your task depends on the my_table table (or view). As a result, SAYN will look for the task that produces my_table and set it as a parent of this autosql task automatically. Tip When using the src macro, you can pass a structure formatted as schema.table such as {{ src('my_schema.my_table') }} . In this case, SAYN interprets the first element as the schema, the second element as the table or view. If you use schema_prefix and / or table_prefix in your project settings, SAYN will then prepend the schema_prefix to the schema value and table_prefix to the table value. For example, if your schema_prefix is set to analytics and table_prefix to up then {{ src('my_schema.my_table') }} will compile analytics_my_schema.up_my_table .","title":"Setting Dependencies With autosql"},{"location":"tasks/autosql/#advanced_configuration","text":"If you need to amend the configuration (e.g. materialisation) of a specific autosql task within a group , you can overload the values specified in the YAML group definition. To do this, we simply call config from a Jinja tag within the sql file of the task: autosql with config {{ config(materialisation='view') }} SELECT ... The above code will override the value of materialisation setting defined in YAML to make this model a view. All other parameters described above in this page are also available to overload with config except db , file_name and name .","title":"Advanced Configuration"},{"location":"tasks/autosql/#using_autosql_in_incremental_mode","text":"autosql tasks support loads incrementally, which is extremely useful for large data volumes when full refresh ( materialisation: table ) would be infeasible. We set an autosql task as incremental by: 1. Setting materialisation to incremental 2. Defining a delete_key autosql in incremental mode ... task_autosql_incremental : type : autosql file_name : task_autosql_incremental.sql materialisation : incremental destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql delete_key : dt ... When using incremental , SAYN will do the following in the background: Create a temporary table based on the incremental logic from the SAYN query. Delete from the final table those records for which the delete_key value is in the temporary table. Insert the contents of the temporary table into the final table. In order to make the SELECT statement incremental, SAYN provides the following arguments: full_load : a flag defaulting to False and controlled by the -f flag in the SAYN command. If -f is passed to the sayn command, the final table will be replaced with the temporary one in step 2 above, rather than performing a merge of the data. start_dt : a date defaulting to \"yesterday\" and controlled by the -s flag in the SAYN command. end_dt : a date defaulting to \"yesterday\" and controlled by the -e flag in the SAYN command. SQL using incremental arguments SELECT dt , field2 , COUNT ( 1 ) AS c FROM table WHERE dt BETWEEN {{ start_dt }} AND {{ end_dt }} GROUP BY 1 , 2","title":"Using autosql In incremental Mode"},{"location":"tasks/autosql/#defining_columns","text":"Autosql tasks accept a columns field in the task definition that affects the table creation by enforcing types and column order. Attention Each supported database might have specific DDL related to it. Below are the DDLs that SAYN supports across all databases. For DDLs related to specific databases see the database-specific pages.","title":"Defining columns"},{"location":"tasks/autosql/#create_table_ddls","text":"SAYN also lets you control the CREATE TABLE statement if you need more specification. This is done with: columns: the list of columns including their definitions. table_properties: database specific properties that affect table creation (indexes, cluster, sorting, etc.). post_hook: SQL statments executed right after the table/view creation. columns can define the following attributes: name: the column name. type: the column type. tests: list of keywords that constraint a specific column unique: enforces a unique constraint on the column. not_null: enforces a non null constraint on the column. allowed_values: list allowed values for the column. table_properties can define the following attributes (database specific): * indexes: * sorting: specify the sorting for the table * distribution_key: specify the type of distribution. * partitioning: specify the partitioning model for the table. * clustering: specify the clustering for the table. Attention Each supported database might have specific table_properties related to it; see the database-specific pages for further details and examples. Attention If the a primary key is defined in both the columns and indexes DDL entries, the primary key will be set as part of the CREATE TABLE statement only. autosql with columns ... task_autosql : type : autosql file_name : task_autosql.sql materialisation : table destination : tmp_schema : analytics_staging schema : analytics_models table : task_autosql ddl : columns : - name : x type : int primary : True - name : y type : varchar unique : True permissions : role_name : SELECT ...","title":"CREATE TABLE DDLs"},{"location":"tasks/copy/","text":"copy Task \u00b6 About \u00b6 The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Attention Copy tasks can only be defined in YAML groups in the tasks folder, not directly in project.yaml . Defining copy Tasks \u00b6 A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info By default the destination is the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . The tables specified in destination and source will be affected by prefixes, suffixes and overrides as described in database objects , meaning it only affects tables in the default_db (typically the destination in extraction tasks and the source in reverse ETL tasks). By default, tables will be copied in full every time SAYN runs replacing the table with the newly pulled data. This behaviour can be altered with the following: incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior or equal to the maximum found in the destination, or with a NULL value. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. append : a boolean flag indicating if data should be replaced in the destination. This means that in full load mode ( incremental_key not specified) records will be appended rather than the table being recreated every time; and in incremental mode records will not be removed, so delete_key shouldn't be specified. Additionally an extra column _sayn_load_ts will be added to the destination table to help with de-duplication. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at append : True In this other example, whenever the task runs it checks the latest value of updated_at and appends to the destination table every record in the source with an updated_at greater than or equal to the maximum value present in the destination. While the task is running, SAYN will get records from the source database and load into a temporary table, and will merge into the destination table once all records have been loaded. The frequency of loading into this table is determined by the value of max_batch_rows as defined in the credentials for the destination database, which defaults to 50000. However this behaviour can be changed with 2 properties: max_batch_rows : this allows you to overwrite the value specified in the credential for this task only. max_merge_rows : this value changes the behaviour so that instead of merging into the destination table once all rows have been loaded, instead SAYN will merge after this number of records have been loaded and then it will repeat the whole process. The advantage of using this parameter is that for copies that take a long time, an error (ie: loosing the connection with the source) wouldn't result in the process having to be started again from the beginning. Warning When using max_merge_rows SAYN will loop through the merge load and merge process until the number of records loaded is lower than the value of max_merge_rows . In order to avoid infinite loops, the process will also stop after a maximum of 100 iteration. To avoid issues, it should be set to a very large value (larger than max_batch_rows ). Data types and columns \u00b6 copy tasks accept a columns field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP An additional property dst_name in columns is also supported. Specifying this property will change the name of the column in the destination table. When using this property, delete_key and incremental_key need to reference this new name. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_ts delete_key : id columns : - id - name : updated_at dst_name : updated_ts In this example, the updated_at column at source will be called updated_ts on the target. Note the name in incremental_key uses the name on the target. Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Copy"},{"location":"tasks/copy/#copy_task","text":"","title":"copy Task"},{"location":"tasks/copy/#about","text":"The copy task copies tables from one database to another. It can be used to automatically ingest data from operational databases (e.g. PostgreSQL) to your analytics warehouse. Attention Copy tasks can only be defined in YAML groups in the tasks folder, not directly in project.yaml .","title":"About"},{"location":"tasks/copy/#defining_copy_tasks","text":"A copy task is defined as follows: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name copy tasks have the following parameters that need to be set: type : copy . source : the source details db : the source database. schema : the (optional) source schema. table : the name of the table top copy. destination : the destination details. tmp_schema : the (optional) staging schema used in the process of copying data. schema : the (optional) destination schema. table : the name of the table to store data into. db : the (optional) destination database. Info By default the destination is the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . The tables specified in destination and source will be affected by prefixes, suffixes and overrides as described in database objects , meaning it only affects tables in the default_db (typically the destination in extraction tasks and the source in reverse ETL tasks). By default, tables will be copied in full every time SAYN runs replacing the table with the newly pulled data. This behaviour can be altered with the following: incremental_key : the column to use to determine what data is new. The process will transfer any data in the source table with an incremental_key value superior or equal to the maximum found in the destination, or with a NULL value. delete_key : the column which will be used for deleting data in incremental loads. The process will delete any data in the destination table with a delete_key value found in the new dataset obtained before inserting. append : a boolean flag indicating if data should be replaced in the destination. This means that in full load mode ( incremental_key not specified) records will be appended rather than the table being recreated every time; and in incremental mode records will not be removed, so delete_key shouldn't be specified. Additionally an extra column _sayn_load_ts will be added to the destination table to help with de-duplication. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id In this example, we use updated_at which is a field updated every time a record changes (or is created) on a hypothetical backend database to select new records, and then we replace all records in the target based on the id s found in this new dataset. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at append : True In this other example, whenever the task runs it checks the latest value of updated_at and appends to the destination table every record in the source with an updated_at greater than or equal to the maximum value present in the destination. While the task is running, SAYN will get records from the source database and load into a temporary table, and will merge into the destination table once all records have been loaded. The frequency of loading into this table is determined by the value of max_batch_rows as defined in the credentials for the destination database, which defaults to 50000. However this behaviour can be changed with 2 properties: max_batch_rows : this allows you to overwrite the value specified in the credential for this task only. max_merge_rows : this value changes the behaviour so that instead of merging into the destination table once all rows have been loaded, instead SAYN will merge after this number of records have been loaded and then it will repeat the whole process. The advantage of using this parameter is that for copies that take a long time, an error (ie: loosing the connection with the source) wouldn't result in the process having to be started again from the beginning. Warning When using max_merge_rows SAYN will loop through the merge load and merge process until the number of records loaded is lower than the value of max_merge_rows . In order to avoid infinite loops, the process will also stop after a maximum of 100 iteration. To avoid issues, it should be set to a very large value (larger than max_batch_rows ).","title":"Defining copy Tasks"},{"location":"tasks/copy/#data_types_and_columns","text":"copy tasks accept a columns field in the task definition in the same way that autosql does. With this specification, we can override the default behaviour of copy when it comes to column types by enforcing specific column types in the final table: tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_at delete_key : id columns : - id - name : updated_at type : timestamp In this example we define 2 columns for task_copy : id and updated_at . This will make SAYN: 1. Copy only those 2 columns, disregarding any other columns present at source 2. Infer the type of id based on the type of that column at source 3. Enforce the destination table type for updated_at to be TIMESTAMP An additional property dst_name in columns is also supported. Specifying this property will change the name of the column in the destination table. When using this property, delete_key and incremental_key need to reference this new name. tasks/base.yaml task_copy : type : copy source : db : from_db schema : from_schema table : from_table destination : tmp_schema : staging_schema schema : schema table : table_name incremental_key : updated_ts delete_key : id columns : - id - name : updated_at dst_name : updated_ts In this example, the updated_at column at source will be called updated_ts on the target. Note the name in incremental_key uses the name on the target. Additionally, in the ddl property we can specify indexes and permissions like in autosql . Note that some databases support specific DDL other than these.","title":"Data types and columns"},{"location":"tasks/dummy/","text":"dummy Task \u00b6 About \u00b6 The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups. Defining dummy Tasks \u00b6 A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy Usage \u00b6 dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Dummy"},{"location":"tasks/dummy/#dummy_task","text":"","title":"dummy Task"},{"location":"tasks/dummy/#about","text":"The dummy is a task that does not do anything. It is mostly used as a handy connector between tasks when a large number of parents is common to several tasks. Using dummy as the parent of those reduces the length of the code and leads to cleaner task groups.","title":"About"},{"location":"tasks/dummy/#defining_dummy_tasks","text":"A dummy task has no additional properties other than the properties shared by all task types. Example task_dummy : type : dummy","title":"Defining dummy Tasks"},{"location":"tasks/dummy/#usage","text":"dummy tasks come in useful when you have multiple tasks that depend upon a long list of parents. Let's consider the following setup in your task group task_group.yaml : Example tasks : #definition of task_1, task_2, task_3, task_4 ... task_mlt_parents_1 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_2 : #task definition parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_3 : #task definition parents : - task_1 - task_2 - task_3 - task_4 You can avoid repeating the parents across those multiple tasks using a dummy task to create a connector. This is how it would look like with a dummy task. Example tasks : #some tasks dummy_task : type : dummy parents : - task_1 - task_2 - task_3 - task_4 task_mlt_parents_1 : #task definition parents : - dummy_task task_mlt_parents_2 : #task definition parents : - dummy_task task_mlt_parents_3 : #task definition parents : - dummy_task","title":"Usage"},{"location":"tasks/overview/","text":"Tasks \u00b6 About \u00b6 A SAYN project is split into units of execution called tasks. The order of execution of these tasks is given based on the dependencies between them which you specify when writing your tasks. SAYN then takes this information and generates a DAG (Direct Acyclic Graph) automatically. Info A Directed Acyclic Graph is a data structure which enables the conveniently modelling of tasks and dependencies: graph : a data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined based on the tables or views that tasks need to read. In SAYN, this is automated through the concept of sources and outputs . For more custom use, SAYN also supports the manual definition of relationship between tasks through parents . For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers! Task Types \u00b6 Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. sql : executes any SQL statement. There can be multiple statements within the SQL file. dummy : those tasks do not do anything. They can be used as connectors between tasks. Defining Tasks \u00b6 Tasks in SAYN are defined into groups which we describe in the project.yaml file in your project. Task groups define a set of tasks which share the same attributes. For example we can define a group formed of autosql tasks called core like this: project.yaml groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" The properties defined in the group tell SAYN how to generate tasks: type : this tells SAYN to create tasks of type autosql file_name : this property tells SAYN what files to use to generate tasks. The files for autosql tasks are stored under the sql folder, so this expression is telling us to create a task per file with the extension sql found in the sql/core folder materialisation : describes what database object to create in the database destination : defines where to create the database object, in this case we're just using the name, which will simply be the name of the task Attention You would always want the file_name property used in group definitions to be a glob expression so that it points at a list of files. Any other property defined in groups will be interpreted as described in the page for the task type in this documentation. When SAYN interprets this group, for every file found matching the glob expression in file_name a task will be generated and the name of that task will match the name of the file without the extension. For example if the sql/core folder in our project contains 2 files called table1.sql and table2.sql then 2 tasks will be created called table1 and table2 . To allow those 2 tasks to create different tables in the database we use Jinja expressions. In this case we just call the result table exactly the name of the task using \"{{ task.name }}\" . Tip When a SAYN project grows, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. This definition of groups in the project.yaml file is available for autosql , sql and python tasks. You can read more about this by heading to the corresponding pages. Task Attributes \u00b6 project.yaml groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" As you saw in the example above, task attributes can be defined in a dynamic way. This example shows how to use the task name to dynamically define a task. This will effectively tell the task to create the outputs of the core tasks into tables based on the task name, which is the name of the file without the .sql extension for autosql tasks. Tip You can also reference to {{ task.group }} dynamically. YAML based definition of tasks \u00b6 The model described above makes creating a SAYN project very easy, but there are situations where a more advanced model is required. For that we can define tasks in YAML files under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list sources A list of database tables or views this task uses. Optional list outputs A list of database tables or views this task produces Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list on_fail Defines the behaviour when the task fails . Optional one of: skip or no_skip Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it. Task failure behaviour \u00b6 When a task fails during an execution, all descendent tasks will be skipped as expected. However sometimes it can be useful to execute descending tasks even if a parent fails, for example when an API can frequently throw errors and we want to continue the execution just with as much data as it was possible to pull from it. In this case we make use of the on_fail task property to specify that we do not want to skip descending tasks. tasks/base.yaml tasks : could_fail_task : type : python class : could_fail.CouldFailTask on_fail : no_skip child_task : type : sql file_name : query_using_could_fail_data.sql parents : - failing_task In the above case, if could_fail_task fails, child_task will not be skipped.","title":"Overview"},{"location":"tasks/overview/#tasks","text":"","title":"Tasks"},{"location":"tasks/overview/#about","text":"A SAYN project is split into units of execution called tasks. The order of execution of these tasks is given based on the dependencies between them which you specify when writing your tasks. SAYN then takes this information and generates a DAG (Direct Acyclic Graph) automatically. Info A Directed Acyclic Graph is a data structure which enables the conveniently modelling of tasks and dependencies: graph : a data structure which consists of nodes connected by edges . directed : dependencies have a direction. If there is an edge (i.e. a dependency) between two tasks, one will run before the other. acyclic : there are no circular dependencies. If you process the whole graph, you will never encounter the same task twice. Dependencies between tasks are defined based on the tables or views that tasks need to read. In SAYN, this is automated through the concept of sources and outputs . For more custom use, SAYN also supports the manual definition of relationship between tasks through parents . For example, the SAYN tutorial defines the following DAG: Through tasks, SAYN provides a lot of automation under the hood, so make sure you explore the various task types SAYN offers!","title":"About"},{"location":"tasks/overview/#task_types","text":"Please see below the available SAYN task types: autosql : simply write a SELECT statement and SAYN automates the data processing (i.e. table or view creation, incremental load, etc.) for you. python : enables you to write a Python process. Can be used for a wide range of cases from data extraction to data science models - anything Python lets you do. copy : enables to automatically copy data from one database to another. sql : executes any SQL statement. There can be multiple statements within the SQL file. dummy : those tasks do not do anything. They can be used as connectors between tasks.","title":"Task Types"},{"location":"tasks/overview/#defining_tasks","text":"Tasks in SAYN are defined into groups which we describe in the project.yaml file in your project. Task groups define a set of tasks which share the same attributes. For example we can define a group formed of autosql tasks called core like this: project.yaml groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" The properties defined in the group tell SAYN how to generate tasks: type : this tells SAYN to create tasks of type autosql file_name : this property tells SAYN what files to use to generate tasks. The files for autosql tasks are stored under the sql folder, so this expression is telling us to create a task per file with the extension sql found in the sql/core folder materialisation : describes what database object to create in the database destination : defines where to create the database object, in this case we're just using the name, which will simply be the name of the task Attention You would always want the file_name property used in group definitions to be a glob expression so that it points at a list of files. Any other property defined in groups will be interpreted as described in the page for the task type in this documentation. When SAYN interprets this group, for every file found matching the glob expression in file_name a task will be generated and the name of that task will match the name of the file without the extension. For example if the sql/core folder in our project contains 2 files called table1.sql and table2.sql then 2 tasks will be created called table1 and table2 . To allow those 2 tasks to create different tables in the database we use Jinja expressions. In this case we just call the result table exactly the name of the task using \"{{ task.name }}\" . Tip When a SAYN project grows, it is good practice to start separating your tasks in multiple groups (e.g. extracts, core models, marketing models, finance models, data science, etc.) in order to organise processes. This definition of groups in the project.yaml file is available for autosql , sql and python tasks. You can read more about this by heading to the corresponding pages.","title":"Defining Tasks"},{"location":"tasks/overview/#task_attributes","text":"project.yaml groups: core: type: autosql file_name: \"core/*.sql\" materialisation: table destination: table: \"{{ task.name }}\" As you saw in the example above, task attributes can be defined in a dynamic way. This example shows how to use the task name to dynamically define a task. This will effectively tell the task to create the outputs of the core tasks into tables based on the task name, which is the name of the file without the .sql extension for autosql tasks. Tip You can also reference to {{ task.group }} dynamically.","title":"Task Attributes"},{"location":"tasks/overview/#yaml_based_definition_of_tasks","text":"The model described above makes creating a SAYN project very easy, but there are situations where a more advanced model is required. For that we can define tasks in YAML files under the tasks folder at the root level of your SAYN project. Each file in the tasks folder represents a task group and can be executed independently. By default, SAYN includes any file in the tasks folder ending with a .yaml extension when creating the DAG. Within each YAML file, tasks are defined in the tasks entry. tasks/base.yaml tasks : task_1 : # Task properties task_2 : # Task properties # ... All tasks share a number of common properties available: Property Description Required type The task type. Required one of: autosql , sql , python , copy , dummy preset A preset to inherit task properties from. See the presets section for more info. Optional name of preset parents A list of tasks this one depends on. All tasks in this list is ensured to run before the child task. Optional list sources A list of database tables or views this task uses. Optional list outputs A list of database tables or views this task produces Optional list tags A list of tags used in sayn run -t tag:tag_name . This allows for advanced task filtering when we don't want to run all tasks in the project. Optional list on_fail Defines the behaviour when the task fails . Optional one of: skip or no_skip Attention Different task types have different attributes. Make sure that you check each task type's specific documentation to understand how to define it.","title":"YAML based definition of tasks"},{"location":"tasks/overview/#task_failure_behaviour","text":"When a task fails during an execution, all descendent tasks will be skipped as expected. However sometimes it can be useful to execute descending tasks even if a parent fails, for example when an API can frequently throw errors and we want to continue the execution just with as much data as it was possible to pull from it. In this case we make use of the on_fail task property to specify that we do not want to skip descending tasks. tasks/base.yaml tasks : could_fail_task : type : python class : could_fail.CouldFailTask on_fail : no_skip child_task : type : sql file_name : query_using_could_fail_data.sql parents : - failing_task In the above case, if could_fail_task fails, child_task will not be skipped.","title":"Task failure behaviour"},{"location":"tasks/python/","text":"python Task \u00b6 About \u00b6 The python task allows you to run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. There are two models for specifying python tasks in SAYN: a simple way through using decorators and a more advanced way which is class based. Simple Definition of python Tasks \u00b6 You can define python tasks in SAYN very simply by using decorators. This will let you write a Python function and turn that function into a task. First, you need to add a group in project.yaml pointing to the .py file where the task code lives: project.yaml groups: decorator_tasks: type: python module: decorator_tasks parameters: param1: some_value Now all tasks defined in python/decorator_tasks.py will be added to the DAG. The module property expects a python path from the python folder in a similar way as you would import a module in python. For example, if our task definition exists in python/example_mod/decorator_tasks.py the value in module would be example_mod.decorator_tasks . python/decorator_tasks.py from sayn import task @task(outputs='logs.api_table', sources='logs.another_table') def example_task(context, warehouse, param1): src_table = context.src('logs.another_table') out_table = context.out('logs.api_table') warehouse.execute(f'CREATE OR REPLACE TABLE {out_table} AS SELECT * from {src_table}') The above example showcases the key elements to a python task: task : we import SAYN's task decorator which is used to turn functions into SAYN tasks added to the DAG. parameters to task : we can pass parameters sources , outputs and parents which are either lists of table names or a single table name. This allows SAYN define the task dependencies. function name: the name of the function ( example_task here) will be the name of the task. We can use this name with -t to execute this task only for example. function parameters: arguments to the function have special meaning and so the names need to be respected: context : is an object granting access to some functionality like project parameters, connections and other functions as seen further down. warehouse : connection names ( required_credentials in project.yaml ) will automatically provide the object of that connection. You can specify any number of connections here. param1: the rest of the function arguments are matched against task parameters, these are values defined in the parameter property in the group. Python decorators Decorators in python are used to modify the behaviour of a function. It can be a bit daunting to understand when we first encounter them but for the purpose of SAYN all you need to know is that @task turns a standard python function into a SAYN task which can assess useful properties via arguments. There are many resources online describing how decorators work, for example this . Given the code above, this task will: Depend on (execute after) the tasks that produce logs.another_table since we added the sources argument to the decorator. Be the parent of (execute before) any task reading from logs.api_table since we added the outputs argument to the decorator. Get the compiled value of logs.another_table and logs.api_table and keep in 2 variables. For details on database objects compilation make sure you check the database objects page . Execute a create table statement using the tables above on the database called warehouse in the project. Advanced python Task Definition With Classes \u00b6 The second model for defining python tasks is through classes. When using this model we get an opportunity to: do validation before the task is executed by overloading the setup method, which is useful as a way to alert early during the execution that something is incorrectly defined rather than waiting for the task to fail. define more complex dependencies than sources and outputs by overloading the config method. implement code for the compile stage allowing for more early stage indication of problems. A python task using classes is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python folder of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder. The class then needs to be defined as follows: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def config ( self ): self . src ( 'logs.source_table' ) self . out ( 'logs.output_table' ) def setup ( self ): # Do some validation of the parameters return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We set some dependencies by calling self.src and self.out . We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Python tasks can return self.success() or self.fail() to indicate the result of the execution, but it's not mandatory. If the code throws a python exception, the task will be considered as failed. Using the SAYN API \u00b6 When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. When using the decorator model, to access this functionality we need to include the context argument in the function, when using the class model the more standard self is used, and both give access to the same functionality. The list of available properties through self and context is: parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. default_db : provides access to the default_db database object specified in the project.yaml file. src : the src macro that translates database object names as described in database objects . Bear in mind that using this function also adds dependencies to the task, but only when called from the config method of a python task defined with the class model. out : the out macro that translates database object names as described in database objects . Bear in mind that using this function also creates dependencies between tasks, but only when called from the config method of a python task defined with the class model. Tip You can use self.default_db to easily perform some operations on the default database such as reading or loading data or if using decorators simply include an argument with the name of the connection. See the methods available on the Database API. Tip We all love pandas ! If you want to load a pandas dataframe you can use one of these options: with the pandas.DataFrame.to_sql method: df.to_sql(self.default_db.engine, 'table') . with the self.default_db.load_data method: self.default_db.load_data('table', df.to_dict('records')) . Logging For python Tasks With The SAYN API \u00b6 The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py context . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with context . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Python"},{"location":"tasks/python/#python_task","text":"","title":"python Task"},{"location":"tasks/python/#about","text":"The python task allows you to run python scripts. Therefore, those tasks can do anything Python can do. They are extremely useful for data extraction or data science models. There are two models for specifying python tasks in SAYN: a simple way through using decorators and a more advanced way which is class based.","title":"About"},{"location":"tasks/python/#simple_definition_of_python_tasks","text":"You can define python tasks in SAYN very simply by using decorators. This will let you write a Python function and turn that function into a task. First, you need to add a group in project.yaml pointing to the .py file where the task code lives: project.yaml groups: decorator_tasks: type: python module: decorator_tasks parameters: param1: some_value Now all tasks defined in python/decorator_tasks.py will be added to the DAG. The module property expects a python path from the python folder in a similar way as you would import a module in python. For example, if our task definition exists in python/example_mod/decorator_tasks.py the value in module would be example_mod.decorator_tasks . python/decorator_tasks.py from sayn import task @task(outputs='logs.api_table', sources='logs.another_table') def example_task(context, warehouse, param1): src_table = context.src('logs.another_table') out_table = context.out('logs.api_table') warehouse.execute(f'CREATE OR REPLACE TABLE {out_table} AS SELECT * from {src_table}') The above example showcases the key elements to a python task: task : we import SAYN's task decorator which is used to turn functions into SAYN tasks added to the DAG. parameters to task : we can pass parameters sources , outputs and parents which are either lists of table names or a single table name. This allows SAYN define the task dependencies. function name: the name of the function ( example_task here) will be the name of the task. We can use this name with -t to execute this task only for example. function parameters: arguments to the function have special meaning and so the names need to be respected: context : is an object granting access to some functionality like project parameters, connections and other functions as seen further down. warehouse : connection names ( required_credentials in project.yaml ) will automatically provide the object of that connection. You can specify any number of connections here. param1: the rest of the function arguments are matched against task parameters, these are values defined in the parameter property in the group. Python decorators Decorators in python are used to modify the behaviour of a function. It can be a bit daunting to understand when we first encounter them but for the purpose of SAYN all you need to know is that @task turns a standard python function into a SAYN task which can assess useful properties via arguments. There are many resources online describing how decorators work, for example this . Given the code above, this task will: Depend on (execute after) the tasks that produce logs.another_table since we added the sources argument to the decorator. Be the parent of (execute before) any task reading from logs.api_table since we added the outputs argument to the decorator. Get the compiled value of logs.another_table and logs.api_table and keep in 2 variables. For details on database objects compilation make sure you check the database objects page . Execute a create table statement using the tables above on the database called warehouse in the project.","title":"Simple Definition of python Tasks"},{"location":"tasks/python/#advanced_python_task_definition_with_classes","text":"The second model for defining python tasks is through classes. When using this model we get an opportunity to: do validation before the task is executed by overloading the setup method, which is useful as a way to alert early during the execution that something is incorrectly defined rather than waiting for the task to fail. define more complex dependencies than sources and outputs by overloading the config method. implement code for the compile stage allowing for more early stage indication of problems. A python task using classes is defined as follows: tasks/base.yaml task_python : type : python class : file_name.ClassName Where class is a python path to the Python class implementing the task. This code should be stored in the python folder of your project, which in itself is a python module that's dynamically loaded, so it needs an empty __init__.py file in the folder. The class then needs to be defined as follows: python/file_name.py from sayn import PythonTask class ClassName ( PythonTask ): def config ( self ): self . src ( 'logs.source_table' ) self . out ( 'logs.output_table' ) def setup ( self ): # Do some validation of the parameters return self . success () def run ( self ): # Do something useful return self . success () In this example: We create a new class inheriting from SAYN's PythonTask. We set some dependencies by calling self.src and self.out . We define a setup method to do some sanity checks. This method can be skipped, but it's useful to check the validity of project parameters or so some initial setup. We define the actual process to execute during sayn run with the run method. Both setup and run return the task status as successful return self.success() , however we can indicate a task failure to sayn with return self.fail() . Failing a python task forces child tasks to be skipped. Attention Python tasks can return self.success() or self.fail() to indicate the result of the execution, but it's not mandatory. If the code throws a python exception, the task will be considered as failed.","title":"Advanced python Task Definition With Classes"},{"location":"tasks/python/#using_the_sayn_api","text":"When defining our python task, you would want to access parts of the SAYN infrastructure like parameters and connections. When using the decorator model, to access this functionality we need to include the context argument in the function, when using the class model the more standard self is used, and both give access to the same functionality. The list of available properties through self and context is: parameters : accesses project and task parameters. For more details on parameters , see the Parameters section. run_arguments : provides access to the arguments passed to the sayn run command like the incremental values ( full_load , start_dt and end_dt ). connections : dictionary containing the databases and other custom API credentials. API connections appear as simple python dictionaries, while databases are SAYN's Database objects. default_db : provides access to the default_db database object specified in the project.yaml file. src : the src macro that translates database object names as described in database objects . Bear in mind that using this function also adds dependencies to the task, but only when called from the config method of a python task defined with the class model. out : the out macro that translates database object names as described in database objects . Bear in mind that using this function also creates dependencies between tasks, but only when called from the config method of a python task defined with the class model. Tip You can use self.default_db to easily perform some operations on the default database such as reading or loading data or if using decorators simply include an argument with the name of the connection. See the methods available on the Database API. Tip We all love pandas ! If you want to load a pandas dataframe you can use one of these options: with the pandas.DataFrame.to_sql method: df.to_sql(self.default_db.engine, 'table') . with the self.default_db.load_data method: self.default_db.load_data('table', df.to_dict('records')) .","title":"Using the SAYN API"},{"location":"tasks/python/#logging_for_python_tasks_with_the_sayn_api","text":"The unit of process within a task in SAYN is the step . Using steps is useful to indicate current progress of execution but also for debugging purposes. The tutorial is a good example of usage, as we define the load_data task as having 5 steps: python/load_data.py context . set_run_steps ( [ \"Generate Data\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) This code defines which steps form the task. Then we can define the start and end of that step with: python/load_data.py with context . step ( 'Generate Data' ): data_to_load = get_data ( tournament_battles ) Which will output the following on the screen: CLI output [ 1 /7 ] load_data ( started at 15 :25 ) : Step [ 1 /5 ] Generate Data The default cli presentation will show only the current step being executed, which in the case of the tutorial project goes very quickly. However we can persist these messages using the debug flag to the cli sayn run -d giving you this: CLI ouput [ 1 /7 ] load_data ( started at 15 :29 ) Run Steps: Generate Data, Load fighters, Load arenas, Load tournaments, Load battles \u2139 [ 1 /5 ] [ 15 :29 ] Executing Generate Data \u2714 [ 1 /5 ] [ 15 :29 ] Generate Data ( 19 .5ms ) \u2139 [ 2 /5 ] [ 15 :29 ] Executing Load fighters \u2714 [ 2 /5 ] [ 15 :29 ] Load fighters ( 16 .9ms ) \u2139 [ 3 /5 ] [ 15 :29 ] Executing Load arenas \u2714 [ 3 /5 ] [ 15 :29 ] Load arenas ( 12 .3ms ) \u2139 [ 4 /5 ] [ 15 :29 ] Executing Load tournaments \u2714 [ 4 /5 ] [ 15 :29 ] Load tournaments ( 10 .9ms ) \u2139 [ 5 /5 ] [ 15 :29 ] Executing Load battles \u2714 [ 5 /5 ] [ 15 :29 ] Load battles ( 210 .3ms ) \u2714 Took ( 273ms ) So you can see the time it takes to perform each step. Sometimes it's useful to output some extra text beyond steps. In those cases, the API provides some methods for a more adhoc logging model: self.debug(text) : debug log to console and file. Not printed unless -d is used. self.info(text) : info log to console and file. Not persisted to the screen if -d is not specified. self.warning(text) : warning log to console and file. Remains on the screen after the task finishes (look for yellow lines). self.error(text) : error log to console and file. Remains on the screen after the task finishes (look for red lines). Note self.error doesn't abort the execution of the task, nor it sets the final status to being failed. To indicate a python task has failed, use this construct: return self.fail(text) where text is an optional message string that will be showed on the screen. For more details on the SAYN API, check the API reference page .","title":"Logging For python Tasks With The SAYN API"},{"location":"tasks/sql/","text":"sql Task \u00b6 About \u00b6 The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql . Defining sql Tasks \u00b6 A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : the path to a file within the sql folder of the project's root . When defining sql groups in project.yaml this property needs to be a glob expression, for example group/*.sql . db : the (optional) destination database. Info By default the task is executed in the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . Tip The sql code can use the src and out macros to implicitly create task dependencies as decribed in database objects . Config macro \u00b6 Like autosql tasks, we can overload some values specified in the YAML. This is useful when we define groups in project.yaml and for a specific task we need to make a configuration change like the tags . To use this we simply call config from a Jinja tag within the sql file: autosql with config {{ config(tags=['creation_tasks']) }} CREATE TABLE ... The above code will override the value of tags setting defined in YAML so we can filter on a group of tasks when running SAYN. Other properties are available for overloading for advanced use cases: parents , outputs and sources .","title":"SQL"},{"location":"tasks/sql/#sql_task","text":"","title":"sql Task"},{"location":"tasks/sql/#about","text":"The sql task lets you execute a SQL script with one or many statements. This is useful for executing UPDATE statements for example, that wouldn't be covered by autosql .","title":"About"},{"location":"tasks/sql/#defining_sql_tasks","text":"A sql task is defined as follows: tasks/base.yaml task_sql : type : sql file_name : sql_task.sql A sql task is defined by the following attributes: file_name : the path to a file within the sql folder of the project's root . When defining sql groups in project.yaml this property needs to be a glob expression, for example group/*.sql . db : the (optional) destination database. Info By default the task is executed in the database defined by default_db in project.yaml . db can be specified to change this, in which case the connection specified needs to: Be a credential from the required_credentials list in project.yaml . Be defined in your settings.yaml . Be one of the supported databases . Tip The sql code can use the src and out macros to implicitly create task dependencies as decribed in database objects .","title":"Defining sql Tasks"},{"location":"tasks/sql/#config_macro","text":"Like autosql tasks, we can overload some values specified in the YAML. This is useful when we define groups in project.yaml and for a specific task we need to make a configuration change like the tags . To use this we simply call config from a Jinja tag within the sql file: autosql with config {{ config(tags=['creation_tasks']) }} CREATE TABLE ... The above code will override the value of tags setting defined in YAML so we can filter on a group of tasks when running SAYN. Other properties are available for overloading for advanced use cases: parents , outputs and sources .","title":"Config macro"},{"location":"tests/custom/","text":"custom tests \u00b6 About \u00b6 Custom tests are defined in their own task group tests . In the test definition you provide an SQL query that comprises the test and during execution that query will get executed. Info Defining normal tasks in the tests group will cause SAYN to fail. The SQL queries provided to the custom test, needs to live in a tests folder in the sql folder of the project. Defining Tests \u00b6 Defining custom tests is quite straight-forward. You only need to provide a file_name : tests.yaml tests: test_1: file_name: test.sql tasks: ... ... Writing custom test queries \u00b6 SAYN considers a test to be successful (meaning it passed) when the executing query returns empty (with no results). Thus, when writing custom test queries, the test needs to be expressed as a lack of results to show. As an example, we can look at how unique and not_null tests can be implemented with custom tests: SQL test query - unique SELECT t.column , COUNT(*) FROM table t GROUP BY t.columns HAVING COUNT(*) > 1 SQL test query - nullity SELECT t.column , COUNT(*) FROM table t WHERE t.column IS NULL GROUP BY t.column HAVING COUNT(*) > 1","title":"Custom"},{"location":"tests/custom/#custom_tests","text":"","title":"custom tests"},{"location":"tests/custom/#about","text":"Custom tests are defined in their own task group tests . In the test definition you provide an SQL query that comprises the test and during execution that query will get executed. Info Defining normal tasks in the tests group will cause SAYN to fail. The SQL queries provided to the custom test, needs to live in a tests folder in the sql folder of the project.","title":"About"},{"location":"tests/custom/#defining_tests","text":"Defining custom tests is quite straight-forward. You only need to provide a file_name : tests.yaml tests: test_1: file_name: test.sql tasks: ... ...","title":"Defining Tests"},{"location":"tests/custom/#writing_custom_test_queries","text":"SAYN considers a test to be successful (meaning it passed) when the executing query returns empty (with no results). Thus, when writing custom test queries, the test needs to be expressed as a lack of results to show. As an example, we can look at how unique and not_null tests can be implemented with custom tests: SQL test query - unique SELECT t.column , COUNT(*) FROM table t GROUP BY t.columns HAVING COUNT(*) > 1 SQL test query - nullity SELECT t.column , COUNT(*) FROM table t WHERE t.column IS NULL GROUP BY t.column HAVING COUNT(*) > 1","title":"Writing custom test queries"},{"location":"tests/overview/","text":"Data Tests \u00b6 About \u00b6 SAYN provides an extension to the functionality of the columns field in task definitions that enables the user to make use of predefined (standard) or custom tests for their table data fields. Standard tests are implemented for the autosql and copy task types, while custom tests are not task bound and can be applied to any table in the warehouse. Custom tests work like SAYN sql tasks with a specific SQL query structure that only execute during the SAYN test suite. Running sayn test through the CLI will execute the standard and custom tests for a given project. All CLI usage for tasks applies to tests as well, with the major difference being that tests don't make use of a DAG to determine the order of execution (so attempting to execute tests on parents of children of a task is not supported). Examples: sayn test -t test_name : run task_name only. sayn test -t test1 test2 : runs task1 and task2 only. sayn test -x test_name : run all tests except task_name . Test Types \u00b6 Please see below the available SAYN test types: unique : is applied on any number of columns of a given table and is responsible for validating uniqueness. This will also define constraints during the table creation where applicable. not_null : is applied on any number of columns of a given table and is responsible for validating nullity (or rather the lack of it). This will also define constraints during the table creation where applicable. allowed_values : is applied on any number of columns of a given table and is responsible for validating accepted values. This will also define constraints during the table creation where applicable. custom : is a specifically formated SQL query whose output is used to validate a successful or failed test. Defining Tests \u00b6 Tests are defined in a list format using the tests subfield for each entry in columns . For unique and not_null you need to include these keywords in the list, while for allowed_values we define another list that is populated by the allowed string values for that data field. For example, we can define tests to verify uniqueness and nullity for the id field and allowed values for the alias field for the following task in the core group: tasks.yaml task: type: autosql file_name: \"task.sql\" materialisation: table destination: table: \"{{ task.name }}\" columns: - name: id tests: - unique - not_null - name: alias tests: - allowed_values: - 'first' - 'second' - 'third' We can also define the tests inside task.sql by call config from a Jinja tag: tasks.sql {{ config(columns=[ {'name': 'id', 'tests':['unique', 'not_null']}, {'name':'alias', 'tests':['allowed_values':['first','second','third']}]) }} SELECT ... Custom tests are defined in their own task group called tests (defining tasks in an arbitrary test group will cause SAYN to fail). custom tests are provided with an SQL file that needs to exist in a tests folder in the sql project folder. For example, we can define a custom tests that executes the test query presented bellow: tests.yaml tests: test_1: file_name: test.sql tasks: ... ... SQL test query SELECT l.arena_id FROM dim_arenas as l WHERE l.arena_id IS NULL GROUP BY l.arena_id HAVING COUNT(*) > 0 You can read more about test types by heading to the corresponding pages.","title":"Overview"},{"location":"tests/overview/#data_tests","text":"","title":"Data Tests"},{"location":"tests/overview/#about","text":"SAYN provides an extension to the functionality of the columns field in task definitions that enables the user to make use of predefined (standard) or custom tests for their table data fields. Standard tests are implemented for the autosql and copy task types, while custom tests are not task bound and can be applied to any table in the warehouse. Custom tests work like SAYN sql tasks with a specific SQL query structure that only execute during the SAYN test suite. Running sayn test through the CLI will execute the standard and custom tests for a given project. All CLI usage for tasks applies to tests as well, with the major difference being that tests don't make use of a DAG to determine the order of execution (so attempting to execute tests on parents of children of a task is not supported). Examples: sayn test -t test_name : run task_name only. sayn test -t test1 test2 : runs task1 and task2 only. sayn test -x test_name : run all tests except task_name .","title":"About"},{"location":"tests/overview/#test_types","text":"Please see below the available SAYN test types: unique : is applied on any number of columns of a given table and is responsible for validating uniqueness. This will also define constraints during the table creation where applicable. not_null : is applied on any number of columns of a given table and is responsible for validating nullity (or rather the lack of it). This will also define constraints during the table creation where applicable. allowed_values : is applied on any number of columns of a given table and is responsible for validating accepted values. This will also define constraints during the table creation where applicable. custom : is a specifically formated SQL query whose output is used to validate a successful or failed test.","title":"Test Types"},{"location":"tests/overview/#defining_tests","text":"Tests are defined in a list format using the tests subfield for each entry in columns . For unique and not_null you need to include these keywords in the list, while for allowed_values we define another list that is populated by the allowed string values for that data field. For example, we can define tests to verify uniqueness and nullity for the id field and allowed values for the alias field for the following task in the core group: tasks.yaml task: type: autosql file_name: \"task.sql\" materialisation: table destination: table: \"{{ task.name }}\" columns: - name: id tests: - unique - not_null - name: alias tests: - allowed_values: - 'first' - 'second' - 'third' We can also define the tests inside task.sql by call config from a Jinja tag: tasks.sql {{ config(columns=[ {'name': 'id', 'tests':['unique', 'not_null']}, {'name':'alias', 'tests':['allowed_values':['first','second','third']}]) }} SELECT ... Custom tests are defined in their own task group called tests (defining tasks in an arbitrary test group will cause SAYN to fail). custom tests are provided with an SQL file that needs to exist in a tests folder in the sql project folder. For example, we can define a custom tests that executes the test query presented bellow: tests.yaml tests: test_1: file_name: test.sql tasks: ... ... SQL test query SELECT l.arena_id FROM dim_arenas as l WHERE l.arena_id IS NULL GROUP BY l.arena_id HAVING COUNT(*) > 0 You can read more about test types by heading to the corresponding pages.","title":"Defining Tests"},{"location":"tests/standard/","text":"standard tests \u00b6 About \u00b6 Standard tests correspond to tests on nullity, uniqueness and accepted values. They are supported for autosql and copy tasks. Defining standard Tests \u00b6 Standard tests are defined in a list format using the tests subfield for each entry in columns . The attributes that can be added to the list are as follows: unique : execute a uniqueness test on the column. not_null : execute a not nullity test on the column. allowed_values : execute a check on whether the column contains ONLY the allowed values (provided in a list). execute : will execute the test if True will skip if False . Info execute is a field that needs to exist in the same list level as the test name. In that case, instead of defining the test with the string of the test type, you will need to define the test type with the name attribute and bellow it define the execute attribute. An example of standard tests being defined for a task is: tasks.yaml task: type: autosql file_name: \"task.sql\" materialisation: table destination: table: \"{{ task.name }}\" columns: - name: id tests: - unique - name: not_null execute: True - name: alias tests: - name: allowed_values - 'first' - 'second' - 'third' execute: False We can also define the tests inside task.sql by call config from a Jinja tag: tasks.sql {{ config(columns=[ {'name': 'id', 'tests':['unique', {'name':'not_null', 'execute':True}]}, {'name':'alias', 'tests':[{'name':'allowed_values':['first','second','third'], execute: False }]}]) }} SELECT ...","title":"Standard"},{"location":"tests/standard/#standard_tests","text":"","title":"standard tests"},{"location":"tests/standard/#about","text":"Standard tests correspond to tests on nullity, uniqueness and accepted values. They are supported for autosql and copy tasks.","title":"About"},{"location":"tests/standard/#defining_standard_tests","text":"Standard tests are defined in a list format using the tests subfield for each entry in columns . The attributes that can be added to the list are as follows: unique : execute a uniqueness test on the column. not_null : execute a not nullity test on the column. allowed_values : execute a check on whether the column contains ONLY the allowed values (provided in a list). execute : will execute the test if True will skip if False . Info execute is a field that needs to exist in the same list level as the test name. In that case, instead of defining the test with the string of the test type, you will need to define the test type with the name attribute and bellow it define the execute attribute. An example of standard tests being defined for a task is: tasks.yaml task: type: autosql file_name: \"task.sql\" materialisation: table destination: table: \"{{ task.name }}\" columns: - name: id tests: - unique - name: not_null execute: True - name: alias tests: - name: allowed_values - 'first' - 'second' - 'third' execute: False We can also define the tests inside task.sql by call config from a Jinja tag: tasks.sql {{ config(columns=[ {'name': 'id', 'tests':['unique', {'name':'not_null', 'execute':True}]}, {'name':'alias', 'tests':[{'name':'allowed_values':['first','second','third'], execute: False }]}]) }} SELECT ...","title":"Defining standard Tests"},{"location":"tutorials/tutorial_part1/","text":"Tutorial Part 1: Getting Started With SAYN \u00b6 This tutorial navigates you through your first SAYN run and explains the core components of a SAYN project. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section . Your First SAYN Run \u00b6 To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: getting started commands sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You have made your first SAYN run! This executed several tasks that created SQL data models (several tables and one view) in the SQLite database dev.db . You can use DB Browser for SQLite in order to view the content of the database. These data models model battles from various tournaments and are similar to transformation processes you would run in-warehouse for analytics purposes. Now that you have made your first SAYN run, let's cover what happens in the background. Project Overview \u00b6 The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 logs \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project and the task groups. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution. Setting Up Your Project \u00b6 Now let's see how the tutorial project would be created from scratch. Step 1: Define The Project In project.yaml \u00b6 The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse groups : models : type : autosql file_name : \"*.sql\" materialisation : table destination : table : \"{{ task.name }}\" logs : type : python module : load_data The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped. groups : these define the core task groups of the project. The project only has one task group which defines the models task group. More details on what groups do in the next section of the tutorial. Step 2: Define Your Individual Settings \u00b6 Your individual settings are defined by the settings.yaml file which is stored at the root level of your directory. It contains: settings.yaml profiles : dev : credentials : warehouse : dev_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and your own credentials. default_profile : the profile used by default at execution time. This can be overriden if necessary via -p flag of the run command. credentials : here we define the credentials necessary to run the project. We simply define our warehouse credential which is currently a SQLite database. What Next? \u00b6 You now know the core components and have made your first SAYN run, congratulations! In the next section of the tutorial , we go through how to use SAYN for data modelling. Enjoy SAYN :)","title":"Tutorial (Part 1)"},{"location":"tutorials/tutorial_part1/#tutorial_part_1_getting_started_with_sayn","text":"This tutorial navigates you through your first SAYN run and explains the core components of a SAYN project. It uses the example project created by sayn init . It assumes SAYN is setup as described in the installation section .","title":"Tutorial Part 1: Getting Started With SAYN"},{"location":"tutorials/tutorial_part1/#your_first_sayn_run","text":"To get started, open a terminal, activate your virtual environment ( source sayn_venv/bin/activate ) and run the following: getting started commands sayn init sayn_tutorial cd sayn_tutorial sayn run This will create a new project with the contents of this tutorial and execute it. You have made your first SAYN run! This executed several tasks that created SQL data models (several tables and one view) in the SQLite database dev.db . You can use DB Browser for SQLite in order to view the content of the database. These data models model battles from various tournaments and are similar to transformation processes you would run in-warehouse for analytics purposes. Now that you have made your first SAYN run, let's cover what happens in the background.","title":"Your First SAYN Run"},{"location":"tutorials/tutorial_part1/#project_overview","text":"The sayn_tutorial folder has the following structure: tutorial \u251c\u2500\u2500 project.yaml \u251c\u2500\u2500 settings.yaml \u251c\u2500\u2500 python \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 load_data.py \u251c\u2500\u2500 sql \u2502 \u251c\u2500\u2500 dim_arenas.sql \u2502 \u251c\u2500\u2500 dim_fighters.sql \u2502 \u251c\u2500\u2500 dim_tournaments.sql \u2502 \u251c\u2500\u2500 f_battles.sql \u2502 \u251c\u2500\u2500 f_fighter_results.sql \u2502 \u2514\u2500\u2500 f_rankings.sql \u251c\u2500\u2500 compile \u251c\u2500\u2500 logs \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 readme.md \u2514\u2500\u2500 requirements.txt The main files are: project.yaml : defines the SAYN project and the task groups. It is shared across all collaborators . settings.yaml : defines the individual user's settings. It is unique for each collaborator and should never be pushed to git as it contains credentials. python : folder where scripts for python tasks are stored. sql : folder where SQL files for sql and autosql tasks are stored. logs : folder where SAYN logs are written. compile : folder where compiled SQL queries before execution.","title":"Project Overview"},{"location":"tutorials/tutorial_part1/#setting_up_your_project","text":"Now let's see how the tutorial project would be created from scratch.","title":"Setting Up Your Project"},{"location":"tutorials/tutorial_part1/#step_1_define_the_project_in_projectyaml","text":"The project.yaml file is at the root level of your directory and contains: project.yaml required_credentials : - warehouse default_db : warehouse groups : models : type : autosql file_name : \"*.sql\" materialisation : table destination : table : \"{{ task.name }}\" logs : type : python module : load_data The following is defined: required_credentials : the list of credentials used by the project. In this case we have a single credential called warehouse . The connection details will be defined in settings.yaml . default_db : the database used by sql and autosql tasks. Since we only have 1 credential, this field could be skipped. groups : these define the core task groups of the project. The project only has one task group which defines the models task group. More details on what groups do in the next section of the tutorial.","title":"Step 1: Define The Project In project.yaml"},{"location":"tutorials/tutorial_part1/#step_2_define_your_individual_settings","text":"Your individual settings are defined by the settings.yaml file which is stored at the root level of your directory. It contains: settings.yaml profiles : dev : credentials : warehouse : dev_db default_profile : dev credentials : dev_db : type : sqlite database : dev.db The following is defined: profiles : the definion of profiles for the project. A profile defines the connection between credentials in the project.yaml file and your own credentials. default_profile : the profile used by default at execution time. This can be overriden if necessary via -p flag of the run command. credentials : here we define the credentials necessary to run the project. We simply define our warehouse credential which is currently a SQLite database.","title":"Step 2: Define Your Individual Settings"},{"location":"tutorials/tutorial_part1/#what_next","text":"You now know the core components and have made your first SAYN run, congratulations! In the next section of the tutorial , we go through how to use SAYN for data modelling. Enjoy SAYN :)","title":"What Next?"},{"location":"tutorials/tutorial_part2/","text":"Tutorial Part 2: Data Modelling With SAYN \u00b6 In the first part of the tutorial , we executed our first SAYN run and went through the core components of a SAYN project. We will now see how to use SAYN for data modelling, a major process of analytics warehousing. SQL Tasks With SAYN \u00b6 SAYN can execute two main types of SQL tasks: * autosql : these tasks take a SELECT statement and create a table or view using it. All the processes are automated by SAYN in the background. * sql : these tasks take your SQL statement as is and execute it. These are not covered in this tutorial. Defining The Task Group \u00b6 In order to execute your tasks, you will need to define a task group. This is done in the project.yaml file, under the groups entry. This is the models group we have defined in our project: project.yaml ... groups : models : type : autosql file_name : \"{{ task.group }}/*.sql\" materialisation : table destination : table : \"{{ task.name }}\" This group effectively does the following: It creates a task group called models . This task group is defined to be autosql tasks. Each file with a .sql extension in the sql/models folder will be turned into an autosql task. Do not worry about the {{ task.group }} notation for now, this is simple Jinja templating to dynamically pick the models name of the group. All tasks from the group will model the output as tables. The tables will be named as per the task name. This task name is automatically generated from your file name, excluding the .sql extension. Writing Your Models \u00b6 A Simple Model \u00b6 As explained in the previous section, each file with a .sql extension in the sql/models folder will be turned into an autosql task following our models group definition. For example, the dim_arenas.sql file in the sql/models folder will be turned into a dim_arenas task. This is the SQL code of this file: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM logs_arenas l When executed, this task will create a table called dim_arenas using this SQL code. This is the dim_arenas you can find in the dev.db SQLite database at the root level of the project folder. You can execute this task only by running the command sayn run -t dim_arenas , where dim_arenas is our task name. A Model With Dependency \u00b6 Now, let's have a look at a model which depends on another model. The file f_battles.sql is a good example and actually depends on multiple other models. This is how the SQL query is written: sql/f_battles.sql SELECT t . tournament_name , t . tournament_name || '-' || CAST ( b . battle_id AS VARCHAR ) AS battle_id , a . arena_name , f1 . fighter_name AS fighter1_name , f2 . fighter_name AS fighter2_name , w . fighter_name AS winner_name FROM logs_battles b LEFT JOIN {{ src ( 'dim_tournaments' ) }} t ON b . tournament_id = t . tournament_id LEFT JOIN {{ src ( 'dim_arenas' ) }} a ON b . arena_id = a . arena_id LEFT JOIN {{ src ( 'dim_fighters' ) }} f1 ON b . fighter1_id = f1 . fighter_id LEFT JOIN {{ src ( 'dim_fighters' ) }} f2 ON b . fighter2_id = f2 . fighter_id LEFT JOIN {{ src ( 'dim_fighters' ) }} w ON b . winner_id = w . fighter_id As you can see, this query uses another Jinja templating notation, the src function which is core to using SAYN efficiently. You pass this function the name of a table, and SAYN will automatically build the dependencies between your tasks! For example, this f_battles task sources the table dim_tournaments (amongst others) with the src function. As a result, SAYN will look for the task that produces this dim_tournaments table (which is the dim_tournaments task) and set this task as a parent of the f_battles task. Therefore, dim_tournaments will always execute before f_battles . From the above code, you can see that many tasks will be set as parents of the f_battles task. Changing Your Model Materialisation \u00b6 You can easily amend the configuration of a single task when necessary with SAYN. For example, the task f_rankings , generated by f_rankings.sql , uses the following query: sql/f_rankings.sql {{ config ( materialisation = 'view' ) }} SELECT fr . fighter_name , CAST ( SUM ( fr . is_winner ) AS FLOAT ) / COUNT ( DISTINCT fr . battle_id ) AS win_rate FROM {{ src ( 'f_fighter_results' ) }} fr GROUP BY 1 ORDER BY 2 DESC As you can see, this task uses the config function, which will in this case overwrite the materialisation of the task's output to a view instead of a table. This config function can be really useful when you want to overwrite some attributes of specific tasks within your group. What Next? \u00b6 You now know how to build data models with SAYN. The next section of this tutorial will now go through how to use SAYN for Python processes. This will enable you to leverage SAYN for end-to-end ELT processes or data science tasks!","title":"Tutorial (Part 2)"},{"location":"tutorials/tutorial_part2/#tutorial_part_2_data_modelling_with_sayn","text":"In the first part of the tutorial , we executed our first SAYN run and went through the core components of a SAYN project. We will now see how to use SAYN for data modelling, a major process of analytics warehousing.","title":"Tutorial Part 2: Data Modelling With SAYN"},{"location":"tutorials/tutorial_part2/#sql_tasks_with_sayn","text":"SAYN can execute two main types of SQL tasks: * autosql : these tasks take a SELECT statement and create a table or view using it. All the processes are automated by SAYN in the background. * sql : these tasks take your SQL statement as is and execute it. These are not covered in this tutorial.","title":"SQL Tasks With SAYN"},{"location":"tutorials/tutorial_part2/#defining_the_task_group","text":"In order to execute your tasks, you will need to define a task group. This is done in the project.yaml file, under the groups entry. This is the models group we have defined in our project: project.yaml ... groups : models : type : autosql file_name : \"{{ task.group }}/*.sql\" materialisation : table destination : table : \"{{ task.name }}\" This group effectively does the following: It creates a task group called models . This task group is defined to be autosql tasks. Each file with a .sql extension in the sql/models folder will be turned into an autosql task. Do not worry about the {{ task.group }} notation for now, this is simple Jinja templating to dynamically pick the models name of the group. All tasks from the group will model the output as tables. The tables will be named as per the task name. This task name is automatically generated from your file name, excluding the .sql extension.","title":"Defining The Task Group"},{"location":"tutorials/tutorial_part2/#writing_your_models","text":"","title":"Writing Your Models"},{"location":"tutorials/tutorial_part2/#a_simple_model","text":"As explained in the previous section, each file with a .sql extension in the sql/models folder will be turned into an autosql task following our models group definition. For example, the dim_arenas.sql file in the sql/models folder will be turned into a dim_arenas task. This is the SQL code of this file: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM logs_arenas l When executed, this task will create a table called dim_arenas using this SQL code. This is the dim_arenas you can find in the dev.db SQLite database at the root level of the project folder. You can execute this task only by running the command sayn run -t dim_arenas , where dim_arenas is our task name.","title":"A Simple Model"},{"location":"tutorials/tutorial_part2/#a_model_with_dependency","text":"Now, let's have a look at a model which depends on another model. The file f_battles.sql is a good example and actually depends on multiple other models. This is how the SQL query is written: sql/f_battles.sql SELECT t . tournament_name , t . tournament_name || '-' || CAST ( b . battle_id AS VARCHAR ) AS battle_id , a . arena_name , f1 . fighter_name AS fighter1_name , f2 . fighter_name AS fighter2_name , w . fighter_name AS winner_name FROM logs_battles b LEFT JOIN {{ src ( 'dim_tournaments' ) }} t ON b . tournament_id = t . tournament_id LEFT JOIN {{ src ( 'dim_arenas' ) }} a ON b . arena_id = a . arena_id LEFT JOIN {{ src ( 'dim_fighters' ) }} f1 ON b . fighter1_id = f1 . fighter_id LEFT JOIN {{ src ( 'dim_fighters' ) }} f2 ON b . fighter2_id = f2 . fighter_id LEFT JOIN {{ src ( 'dim_fighters' ) }} w ON b . winner_id = w . fighter_id As you can see, this query uses another Jinja templating notation, the src function which is core to using SAYN efficiently. You pass this function the name of a table, and SAYN will automatically build the dependencies between your tasks! For example, this f_battles task sources the table dim_tournaments (amongst others) with the src function. As a result, SAYN will look for the task that produces this dim_tournaments table (which is the dim_tournaments task) and set this task as a parent of the f_battles task. Therefore, dim_tournaments will always execute before f_battles . From the above code, you can see that many tasks will be set as parents of the f_battles task.","title":"A Model With Dependency"},{"location":"tutorials/tutorial_part2/#changing_your_model_materialisation","text":"You can easily amend the configuration of a single task when necessary with SAYN. For example, the task f_rankings , generated by f_rankings.sql , uses the following query: sql/f_rankings.sql {{ config ( materialisation = 'view' ) }} SELECT fr . fighter_name , CAST ( SUM ( fr . is_winner ) AS FLOAT ) / COUNT ( DISTINCT fr . battle_id ) AS win_rate FROM {{ src ( 'f_fighter_results' ) }} fr GROUP BY 1 ORDER BY 2 DESC As you can see, this task uses the config function, which will in this case overwrite the materialisation of the task's output to a view instead of a table. This config function can be really useful when you want to overwrite some attributes of specific tasks within your group.","title":"Changing Your Model Materialisation"},{"location":"tutorials/tutorial_part2/#what_next","text":"You now know how to build data models with SAYN. The next section of this tutorial will now go through how to use SAYN for Python processes. This will enable you to leverage SAYN for end-to-end ELT processes or data science tasks!","title":"What Next?"},{"location":"tutorials/tutorial_part3/","text":"Tutorial Part 3: Using Python With SAYN \u00b6 The previous section of this tutorial showed you how to use SAYN for data modelling purposes. We will now show you how to use Python with SAYN. This will therefore enable you to write end-to-end ELT processes and data science tasks with SAYN. Let's dive in! Adding Your Python Task Group \u00b6 As we did for the autosql tasks, we will need to add a task group for our Python tasks. To do so, add the following to your project.yaml : project.yaml ... groups : models : ... say_hello : type : python module : say_hello This will do the following: Create a task group called say_hello . All tasks in this group will be of type python . All functions using the task decorator in the file python/say_hello.py will be transformed into Python tasks. This file should already exist in your python folder and defines one task: say_hello . All python tasks should be stored in the python folder where an __init__.py file must exist. Writing Your Python Tasks \u00b6 A Simple Python Task \u00b6 Our tutorial project has two python tasks. It starts with a simple Python task that interacts with the task's context. This is the say_hello task in the python/say_hello.py file, defined as follows: python/say_hello.py from sayn import task @task def say_hello ( context ): context . info ( 'Hello!' ) Here are the core concepts to know for running Python tasks with SAYN: You should import the task decorator from sayn which you can then use to define your tasks. There is a more advanced way to define python tasks with classes using SAYN which you can find in the documentation related to python tasks. Use the @task decorator and then simply define your function. The task name will be the name of the function, in this case say_hello . We pass the context to our function, which can then be used in our code to access task related information and control the logger. In our case, we log Hello! as information. Creating Data Logs \u00b6 The second task in the python/load_data.py module actually does something more interesting. It creates some random logs, which is the data you initially had in the logs table of dev.db . First, let's add this task to project.yaml by adding a task group: project.yaml ... groups : models : ... logs : type : python module : load_data Let's look at whole the code from the python/load_data.py file: python/load_data.py import random from uuid import uuid4 from sayn import task @task () def say_hello ( context ): context . info ( 'Hello!' ) @task ( outputs = [ 'logs_arenas' , 'logs_tournaments' , 'logs_battles' , 'logs_fighters' ] ) def load_data ( context , warehouse ): fighters = [ \"Son Goku\" , \"John\" , \"Lucy\" , \"Dr. x\" , \"Carlos\" , \"Dr. y?\" ] arenas = [ \"Earth Canyon\" , \"World Edge\" , \"Namek\" , \"Volcanic Crater\" , \"Underwater\" ] tournaments = [ \"World Championships\" , \"Tenka-ichi Budokai\" , \"King of the Mountain\" ] context . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) with context . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( tournaments ) ] with context . step ( \"Generate Battles\" ): battles = list () for tournament in tournaments : tournament_id = tournament [ \"tournament_id\" ] # Randomly select a number of battles to generate for each tournament n_battles = random . choice ([ 10 , 20 , 30 ]) for _ in range ( n_battles ): battle_id = str ( uuid4 ()) # Randomly choose fighters and arena fighter1_id = random . choice ( fighters )[ \"fighter_id\" ] fighter2_id = random . choice ( [ f for f in fighters if f [ \"fighter_id\" ] != fighter1_id ] )[ \"fighter_id\" ] arena_id = random . choice ( arenas )[ \"arena_id\" ] # Pick a winner winner_id = ( fighter1_id if random . uniform ( 0 , 1 ) <= 0.5 else fighter2_id ) battles . append ( { \"event_id\" : str ( uuid4 ()), \"tournament_id\" : tournament_id , \"battle_id\" : battle_id , \"arena_id\" : arena_id , \"fighter1_id\" : fighter1_id , \"fighter2_id\" : fighter2_id , \"winner_id\" : winner_id , } ) data_to_load = { \"fighters\" : fighters , \"arenas\" : arenas , \"tournaments\" : tournaments , \"battles\" : battles , } # Load logs for log_type , log_data in data_to_load . items (): with context . step ( f \"Load { log_type } \" ): warehouse . load_data ( f \"logs_ { log_type } \" , log_data , replace = True ) The second task defined in this module is the load_data one. It uses some further features: The load_data task produces outputs which are defined in the decorator. This will enable you to refer to these outputs with the src function in autosql tasks and automatically set dependencies to the load_data task. warehouse is also passed as a parameter to the function. This enables you to easily access the warehouse connection in your task. You can notably see that at the end of the script with the call to warehouse.load_data . Setting Dependencies With load_data \u00b6 As mentioned, we now have a python task which produces some logs which we want our autosql tasks to use for the data modelling process. As a result, we should ensure that the load_data task is always executed first. Because our load_data task produces outputs , we can refer to these with the src function in autosql tasks and automatically create dependencies. For example, the SQL query of the dim_arenas task should be changed from: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM logs_arenas l To: sql/dim_arenas.sql amended SELECT l . arena_id , l . arena_name FROM {{ src ( 'logs_arenas' ) }} l This will now mention that the dim_arenas task sources the logs_arenas table which is an output of the load_data task. SAYN will automatically make load_data a parent of the dim_arenas task and therefore always execute it before. You can do the same for all the other logs tables used in the other autosql tasks. What Next? \u00b6 This is it for our tutorial. You should now have a good understanding of the true power of SAYN! Our documentation has more extensive details about all the SAYN core concepts: Tasks Parameters Presets Databases Data Tests Enjoy SAYN and happy ELT-ing! :)","title":"Tutorial (Part 3)"},{"location":"tutorials/tutorial_part3/#tutorial_part_3_using_python_with_sayn","text":"The previous section of this tutorial showed you how to use SAYN for data modelling purposes. We will now show you how to use Python with SAYN. This will therefore enable you to write end-to-end ELT processes and data science tasks with SAYN. Let's dive in!","title":"Tutorial Part 3: Using Python With SAYN"},{"location":"tutorials/tutorial_part3/#adding_your_python_task_group","text":"As we did for the autosql tasks, we will need to add a task group for our Python tasks. To do so, add the following to your project.yaml : project.yaml ... groups : models : ... say_hello : type : python module : say_hello This will do the following: Create a task group called say_hello . All tasks in this group will be of type python . All functions using the task decorator in the file python/say_hello.py will be transformed into Python tasks. This file should already exist in your python folder and defines one task: say_hello . All python tasks should be stored in the python folder where an __init__.py file must exist.","title":"Adding Your Python Task Group"},{"location":"tutorials/tutorial_part3/#writing_your_python_tasks","text":"","title":"Writing Your Python Tasks"},{"location":"tutorials/tutorial_part3/#a_simple_python_task","text":"Our tutorial project has two python tasks. It starts with a simple Python task that interacts with the task's context. This is the say_hello task in the python/say_hello.py file, defined as follows: python/say_hello.py from sayn import task @task def say_hello ( context ): context . info ( 'Hello!' ) Here are the core concepts to know for running Python tasks with SAYN: You should import the task decorator from sayn which you can then use to define your tasks. There is a more advanced way to define python tasks with classes using SAYN which you can find in the documentation related to python tasks. Use the @task decorator and then simply define your function. The task name will be the name of the function, in this case say_hello . We pass the context to our function, which can then be used in our code to access task related information and control the logger. In our case, we log Hello! as information.","title":"A Simple Python Task"},{"location":"tutorials/tutorial_part3/#creating_data_logs","text":"The second task in the python/load_data.py module actually does something more interesting. It creates some random logs, which is the data you initially had in the logs table of dev.db . First, let's add this task to project.yaml by adding a task group: project.yaml ... groups : models : ... logs : type : python module : load_data Let's look at whole the code from the python/load_data.py file: python/load_data.py import random from uuid import uuid4 from sayn import task @task () def say_hello ( context ): context . info ( 'Hello!' ) @task ( outputs = [ 'logs_arenas' , 'logs_tournaments' , 'logs_battles' , 'logs_fighters' ] ) def load_data ( context , warehouse ): fighters = [ \"Son Goku\" , \"John\" , \"Lucy\" , \"Dr. x\" , \"Carlos\" , \"Dr. y?\" ] arenas = [ \"Earth Canyon\" , \"World Edge\" , \"Namek\" , \"Volcanic Crater\" , \"Underwater\" ] tournaments = [ \"World Championships\" , \"Tenka-ichi Budokai\" , \"King of the Mountain\" ] context . set_run_steps ( [ \"Generate Dimensions\" , \"Generate Battles\" , \"Load fighters\" , \"Load arenas\" , \"Load tournaments\" , \"Load battles\" , ] ) with context . step ( \"Generate Dimensions\" ): # Add ids to the dimensions fighters = [ { \"fighter_id\" : str ( uuid4 ()), \"fighter_name\" : val } for id , val in enumerate ( fighters ) ] arenas = [ { \"arena_id\" : str ( uuid4 ()), \"arena_name\" : val } for id , val in enumerate ( arenas ) ] tournaments = [ { \"tournament_id\" : str ( uuid4 ()), \"tournament_name\" : val } for id , val in enumerate ( tournaments ) ] with context . step ( \"Generate Battles\" ): battles = list () for tournament in tournaments : tournament_id = tournament [ \"tournament_id\" ] # Randomly select a number of battles to generate for each tournament n_battles = random . choice ([ 10 , 20 , 30 ]) for _ in range ( n_battles ): battle_id = str ( uuid4 ()) # Randomly choose fighters and arena fighter1_id = random . choice ( fighters )[ \"fighter_id\" ] fighter2_id = random . choice ( [ f for f in fighters if f [ \"fighter_id\" ] != fighter1_id ] )[ \"fighter_id\" ] arena_id = random . choice ( arenas )[ \"arena_id\" ] # Pick a winner winner_id = ( fighter1_id if random . uniform ( 0 , 1 ) <= 0.5 else fighter2_id ) battles . append ( { \"event_id\" : str ( uuid4 ()), \"tournament_id\" : tournament_id , \"battle_id\" : battle_id , \"arena_id\" : arena_id , \"fighter1_id\" : fighter1_id , \"fighter2_id\" : fighter2_id , \"winner_id\" : winner_id , } ) data_to_load = { \"fighters\" : fighters , \"arenas\" : arenas , \"tournaments\" : tournaments , \"battles\" : battles , } # Load logs for log_type , log_data in data_to_load . items (): with context . step ( f \"Load { log_type } \" ): warehouse . load_data ( f \"logs_ { log_type } \" , log_data , replace = True ) The second task defined in this module is the load_data one. It uses some further features: The load_data task produces outputs which are defined in the decorator. This will enable you to refer to these outputs with the src function in autosql tasks and automatically set dependencies to the load_data task. warehouse is also passed as a parameter to the function. This enables you to easily access the warehouse connection in your task. You can notably see that at the end of the script with the call to warehouse.load_data .","title":"Creating Data Logs"},{"location":"tutorials/tutorial_part3/#setting_dependencies_with_load_data","text":"As mentioned, we now have a python task which produces some logs which we want our autosql tasks to use for the data modelling process. As a result, we should ensure that the load_data task is always executed first. Because our load_data task produces outputs , we can refer to these with the src function in autosql tasks and automatically create dependencies. For example, the SQL query of the dim_arenas task should be changed from: sql/dim_arenas.sql SELECT l . arena_id , l . arena_name FROM logs_arenas l To: sql/dim_arenas.sql amended SELECT l . arena_id , l . arena_name FROM {{ src ( 'logs_arenas' ) }} l This will now mention that the dim_arenas task sources the logs_arenas table which is an output of the load_data task. SAYN will automatically make load_data a parent of the dim_arenas task and therefore always execute it before. You can do the same for all the other logs tables used in the other autosql tasks.","title":"Setting Dependencies With load_data"},{"location":"tutorials/tutorial_part3/#what_next","text":"This is it for our tutorial. You should now have a good understanding of the true power of SAYN! Our documentation has more extensive details about all the SAYN core concepts: Tasks Parameters Presets Databases Data Tests Enjoy SAYN and happy ELT-ing! :)","title":"What Next?"}]}